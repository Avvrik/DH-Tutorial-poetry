{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import text collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texts_amateur.txt\", 'r', encoding = \"utf-8-sig\") as f:\n",
    "    lay = f.read()\n",
    "with open(\"texts_children.txt\", 'r', encoding = \"utf-8-sig\") as f:\n",
    "    children = f.read()\n",
    "with open(\"texts_professional.txt\", 'r', encoding = \"utf-8-sig\") as f:\n",
    "    prof = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean from punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "children_clean = re.sub(\"[.,\\\"?!:;()\\[\\]{}\\-–—]\", \"\", children)\n",
    "lay_clean = re.sub(\"[.,\\\"?!:;()\\[\\]{}\\-–—]\", \"\", lay)\n",
    "prof_clean = re.sub(\"[.,\\\"?!:;()\\[\\]{}\\-–—]\", \"\", prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(poem):\n",
    "    tokens = word_tokenize(poem)\n",
    "    tokens_tags = nltk.pos_tag(tokens)\n",
    "    matching_tags = {'NN':'n', 'VB':'v', 'JJ':'a', 'RB':'r'}\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lemma = []\n",
    "    for (token, tag) in tokens_tags:\n",
    "\n",
    "        if not(token[0].isalpha()):\n",
    "            continue\n",
    "        token = token.lower()\n",
    "        if tag[:2] in matching_tags:\n",
    "            token = lemmatizer.lemmatize(token, pos=matching_tags[tag[:2]])\n",
    "            tokens_lemma.append(token)\n",
    "        \n",
    "    return ' '.join(tokens_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_child = lemmatizer(children)\n",
    "with open('lemmas_children.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(lemmas_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_lay = lemmatizer(lay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemmas_lay.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(lemmas_lay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_prof = lemmatizer(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemmas_prof.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(lemmas_prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_children = lemmas_child.split()\n",
    "tokens_lay = lemmas_lay.split()\n",
    "tokens_prof = lemmas_prof.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_words = {elem for elem in tokens_lay if len(elem) < 3} | {elem for elem in tokens_prof if len(elem) < 3} | {elem for elem in tokens_children if len(elem) < 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinies = ['a',\n",
    " 'ah',\n",
    " 'am',\n",
    " 'an',\n",
    " 'as',\n",
    " 'at',\n",
    " 'be',\n",
    " 'do',\n",
    " 'go',\n",
    " 'ha',\n",
    " 'he',\n",
    " 'hi',\n",
    " 'i',\n",
    " 'in',\n",
    " 'is',\n",
    " 'it',\n",
    " 'me',\n",
    " 'my',\n",
    " 'no',\n",
    " 'of',\n",
    " 'oh',\n",
    " 'ok',\n",
    " 'or',\n",
    " 'so',\n",
    " 'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tinies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arr in [tokens_lay, tokens_prof, tokens_children]:\n",
    "    for elem in arr:\n",
    "        if len(elem) < 3 and elem not in tinies:\n",
    "            arr.remove(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_child = \" \".join(tokens_children)\n",
    "lemmas_lay = \" \".join(tokens_lay)\n",
    "lemmas_prof = \" \".join(tokens_prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jingle',\n",
       " 'saddest',\n",
       " 'reluctance',\n",
       " 'guess',\n",
       " 'chan',\n",
       " 'monday',\n",
       " 'late',\n",
       " 'upper',\n",
       " 'uniform',\n",
       " 'wall',\n",
       " 'granny',\n",
       " 'explore',\n",
       " 'craaash',\n",
       " 'together…',\n",
       " 'storage',\n",
       " 'wareers',\n",
       " 'formula',\n",
       " 'tell',\n",
       " 'crater',\n",
       " 'tint']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_child = set(tokens_children)\n",
    "list(unique_child)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['profound',\n",
       " 'quill',\n",
       " 'variety',\n",
       " 'unanswered',\n",
       " 'cuz',\n",
       " 'immenently',\n",
       " 'wrinkle',\n",
       " 'revelation',\n",
       " 'willingly',\n",
       " 'guess',\n",
       " 'decipher',\n",
       " 'late',\n",
       " 'retain',\n",
       " 'uniform',\n",
       " 'idiocy',\n",
       " 'wall',\n",
       " 'emptiness',\n",
       " 'regale',\n",
       " 'wart',\n",
       " 'medical']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_lay = set(tokens_lay)\n",
    "list(unique_lay)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sky-floor',\n",
       " 'wherefore',\n",
       " 'oust',\n",
       " 'monday',\n",
       " 'emptiness',\n",
       " 'maremma',\n",
       " 'flautist',\n",
       " 'february',\n",
       " 'moondust',\n",
       " 'you.',\n",
       " 'coronado',\n",
       " 'incetown',\n",
       " 'infamy',\n",
       " 'crane',\n",
       " 'concede',\n",
       " 'plainly',\n",
       " 'mounting',\n",
       " 'blizzard',\n",
       " 'quaintly',\n",
       " 'wing']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_prof = set(tokens_prof)\n",
    "list(unique_prof)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('be', 3325), ('have', 593), ('go', 445), ('love', 411), ('do', 380), ('not', 314), ('friend', 282), ('get', 282), ('so', 280), ('day', 267), ('see', 256), ('make', 241), ('know', 215), ('say', 202), ('don', 196), ('come', 193), ('look', 180), ('time', 173), ('play', 169), ('just', 162)]\n",
      "\n",
      "[('be', 2051), ('do', 667), ('have', 478), ('so', 432), (\"n't\", 419), ('know', 417), ('love', 395), ('just', 373), ('not', 365), ('never', 266), ('see', 261), ('go', 257), ('feel', 235), ('i', 225), ('say', 220), ('now', 210), ('day', 201), ('want', 198), ('make', 197), ('heart', 195)]\n",
      "\n",
      "[('be', 1820), ('have', 488), ('not', 328), ('do', 252), ('go', 192), ('say', 190), ('come', 184), ('now', 168), ('make', 162), ('know', 161), ('love', 151), ('so', 151), ('night', 147), ('eye', 126), ('then', 125), ('see', 122), ('old', 118), ('time', 114), ('man', 110), ('still', 110)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freq_children = Counter(tokens_children)\n",
    "frequency_children = freq_children.most_common(20)\n",
    "print(frequency_children)\n",
    "print()\n",
    "freq_lay = Counter(tokens_lay)\n",
    "frequency_lay = freq_lay.most_common(20)\n",
    "print(frequency_lay)\n",
    "print()\n",
    "freq_prof = Counter(tokens_prof)\n",
    "frequency_prof = freq_prof.most_common(20)\n",
    "print(frequency_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Lay_frequency.tsv', 'w') as f:\n",
    "    f.write('Word\\tFrequency\\n')\n",
    "    for word,fr in frequency_lay:\n",
    "            f.write(word + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Children_frequency.tsv', 'w') as f:\n",
    "    f.write('Word\\tFrequency\\n')\n",
    "    for word,fr in frequency_children:\n",
    "            f.write(word + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Prof_frequency.tsv', 'w') as f:\n",
    "    f.write('Word\\tFrequency\\n')\n",
    "    for word,fr in frequency_prof:\n",
    "            f.write(word + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most frequent bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams_children = []\n",
    "    \n",
    "for i in range(len(tokens_children)-n+1):\n",
    "    bigrams_children.append(tokens_children[i:i+n])\n",
    "    \n",
    "ngrams_children = [' '.join(tokens_children[i:i+n]) for i in range(len(tokens_children)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams_lay = []\n",
    "    \n",
    "for i in range(len(tokens_lay)-n+1):\n",
    "    bigrams_lay.append(tokens_lay[i:i+n])\n",
    "    \n",
    "ngrams_lay = [' '.join(tokens_lay[i:i+n]) for i in range(len(tokens_lay)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams_prof = []\n",
    "    \n",
    "for i in range(len(tokens_prof)-n+1):\n",
    "    bigrams_prof.append(tokens_prof[i:i+n])\n",
    "    \n",
    "ngrams_prof = [' '.join(tokens_prof[i:i+n]) for i in range(len(tokens_prof)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('friend be', 94),\n",
       " ('be best', 83),\n",
       " ('be not', 73),\n",
       " ('do not', 58),\n",
       " ('be be', 56),\n",
       " ('be so', 56),\n",
       " ('be very', 55),\n",
       " ('best friend', 48),\n",
       " ('be great', 44),\n",
       " ('be fun', 42),\n",
       " ('be as', 40),\n",
       " ('love be', 40),\n",
       " ('want be', 39),\n",
       " ('be good', 38),\n",
       " ('be go', 36)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq = Counter(ngrams_children)\n",
    "freq_bigram_children = bigram_freq.most_common(15)\n",
    "freq_bigram_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Bigram_frequency_child.tsv', 'w') as f:\n",
    "    f.write('Bigram\\tFrequency\\n')\n",
    "    for bigr,fr in freq_bigram_children:\n",
    "            f.write(bigr + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"do n't\", 252),\n",
       " (\"n't know\", 88),\n",
       " ('be so', 65),\n",
       " ('so much', 57),\n",
       " ('be not', 56),\n",
       " ('love be', 50),\n",
       " ('know be', 47),\n",
       " ('life be', 44),\n",
       " ('be just', 42),\n",
       " ('be be', 42),\n",
       " ('have be', 38),\n",
       " ('know do', 36),\n",
       " ('be there', 35),\n",
       " (\"be n't\", 33),\n",
       " ('heart be', 33)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq_lay = Counter(ngrams_lay)\n",
    "freq_bigram_lay = bigram_freq_lay.most_common(15)\n",
    "freq_bigram_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Bigram_frequency_lay.tsv', 'w') as f:\n",
    "    f.write('Bigram\\tFrequency\\n')\n",
    "    for bigr,fr in freq_bigram_lay:\n",
    "            f.write(bigr + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be not', 59),\n",
       " (\"do n't\", 53),\n",
       " ('have be', 50),\n",
       " ('do not', 43),\n",
       " ('love be', 23),\n",
       " ('be be', 22),\n",
       " ('say be', 21),\n",
       " ('be go', 19),\n",
       " ('life be', 19),\n",
       " ('not be', 18),\n",
       " ('be so', 18),\n",
       " ('be make', 17),\n",
       " ('here be', 17),\n",
       " ('now be', 16),\n",
       " ('too much', 16)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq_prof = Counter(ngrams_prof)\n",
    "freq_bigram_prof = bigram_freq_prof.most_common(15)\n",
    "freq_bigram_prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Bigram_frequency_prof.tsv', 'w') as f:\n",
    "    f.write('Bigram\\tFrequency\\n')\n",
    "    for bigr,fr in freq_bigram_prof:\n",
    "            f.write(bigr + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS (part of speech) - tagging. \n",
    "Tag the words in each corpus, count POS tags to compare the corpora with regard to part of speech frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove stopwords using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Avvrik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_children_clean = [word for word in tokens_children if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lay_clean = [word for word in tokens_lay if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_prof_clean = [word for word in tokens_prof if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tag the words with NLTK and count the tags, then calculate percentage of each part of speech in the text collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 16507, 'JJ': 7420, 'VBP': 2679, 'RB': 2187, 'VB': 1456, 'NNS': 618, 'VBD': 597, 'IN': 478, 'VBG': 441, 'VBN': 245, 'JJS': 177, 'VBZ': 114, 'CD': 90, 'JJR': 80, 'NNP': 40, 'MD': 36, 'RP': 35, 'RBS': 29, 'RBR': 29, 'DT': 26, 'WP': 25, 'FW': 23, 'UH': 18, 'CC': 11, 'WRB': 9, 'PRP': 5, 'WDT': 2, 'WP$': 2, 'PRP$': 2})\n",
      "33381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CC': 0.0003295287738533897,\n",
       " 'CD': 0.0026961445133459154,\n",
       " 'DT': 0.0007788861927443756,\n",
       " 'FW': 0.0006890147089661784,\n",
       " 'IN': 0.01431952308199275,\n",
       " 'JJ': 0.22228213654474102,\n",
       " 'JJR': 0.002396572900751925,\n",
       " 'JJS': 0.0053024175429136336,\n",
       " 'MD': 0.0010784578053383662,\n",
       " 'NN': 0.4945028609089003,\n",
       " 'NNP': 0.0011982864503759624,\n",
       " 'NNS': 0.01851352565830862,\n",
       " 'PRP': 0.0001497858062969953,\n",
       " 'PRP$': 5.9914322518798116e-05,\n",
       " 'RB': 0.06551631167430574,\n",
       " 'RBR': 0.0008687576765225727,\n",
       " 'RBS': 0.0008687576765225727,\n",
       " 'RP': 0.001048500644078967,\n",
       " 'UH': 0.0005392289026691831,\n",
       " 'VB': 0.04361762679368503,\n",
       " 'VBD': 0.017884425271861237,\n",
       " 'VBG': 0.013211108115394984,\n",
       " 'VBN': 0.0073395045085527695,\n",
       " 'VBP': 0.08025523501393007,\n",
       " 'VBZ': 0.003415116383571493,\n",
       " 'WDT': 5.9914322518798116e-05,\n",
       " 'WP': 0.0007489290314849765,\n",
       " 'WP$': 5.9914322518798116e-05,\n",
       " 'WRB': 0.00026961445133459155}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_children = nltk.pos_tag(tokens_children_clean)\n",
    "    \n",
    "from collections import Counter\n",
    "counts_children = Counter(tag for word,tag in pos_tags_children)\n",
    "print(counts_children)\n",
    "total = sum(counts_children.values())\n",
    "print(total)\n",
    "dict((word, float(n)/total) for word,n in counts_children.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 12073, 'JJ': 5709, 'RB': 2745, 'VBP': 2630, 'VB': 1972, 'VBD': 558, 'NNS': 391, 'VBG': 361, 'IN': 303, 'VBN': 263, 'CD': 116, 'VBZ': 90, 'JJR': 72, 'JJS': 54, 'RP': 40, 'RBR': 38, 'MD': 32, 'DT': 23, 'FW': 22, 'NNP': 14, 'RBS': 8, 'CC': 8, 'WP': 6, 'PRP': 6, 'WDT': 4, 'UH': 2, 'WRB': 2, 'WP$': 2})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CC': 0.0002904443799012489,\n",
       " 'CD': 0.004211443508568109,\n",
       " 'DT': 0.0008350275922160906,\n",
       " 'FW': 0.0007987220447284345,\n",
       " 'IN': 0.011000580888759803,\n",
       " 'JJ': 0.20726837060702874,\n",
       " 'JJR': 0.00261399941911124,\n",
       " 'JJS': 0.0019604995643334303,\n",
       " 'MD': 0.0011617775196049957,\n",
       " 'NN': 0.43831687481847226,\n",
       " 'NNP': 0.0005082776648271856,\n",
       " 'NNS': 0.014195469067673541,\n",
       " 'PRP': 0.00021783328492593667,\n",
       " 'RB': 0.09965872785361603,\n",
       " 'RBR': 0.0013796108045309324,\n",
       " 'RBS': 0.0002904443799012489,\n",
       " 'RP': 0.0014522218995062445,\n",
       " 'UH': 7.261109497531223e-05,\n",
       " 'VB': 0.07159453964565786,\n",
       " 'VBD': 0.020258495498112112,\n",
       " 'VBG': 0.013106302643043856,\n",
       " 'VBN': 0.009548358989253557,\n",
       " 'VBP': 0.09548358989253558,\n",
       " 'VBZ': 0.0032674992738890504,\n",
       " 'WDT': 0.00014522218995062446,\n",
       " 'WP': 0.00021783328492593667,\n",
       " 'WP$': 7.261109497531223e-05,\n",
       " 'WRB': 7.261109497531223e-05}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_lay = nltk.pos_tag(tokens_lay_clean)\n",
    "    \n",
    "from collections import Counter\n",
    "counts_lay = Counter(tag for word,tag in pos_tags_lay)\n",
    "print(counts_lay)\n",
    "total = sum(counts_lay.values())\n",
    "dict((word, float(n)/total) for word,n in counts_lay.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 17796, 'JJ': 8160, 'VBP': 2205, 'RB': 2031, 'VB': 1064, 'VBD': 979, 'NNS': 601, 'VBG': 500, 'IN': 434, 'VBN': 348, 'VBZ': 169, 'JJR': 97, 'CD': 69, 'JJS': 67, 'RBR': 58, 'MD': 48, 'NNP': 30, 'RP': 29, 'FW': 26, 'WP': 18, 'CC': 18, 'WP$': 12, 'DT': 11, 'PRP': 6, 'RBS': 5, 'WDT': 4, 'UH': 3, 'WRB': 3, 'PDT': 2, 'PRP$': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CC': 0.0005173305742369374,\n",
       " 'CD': 0.0019831005345749267,\n",
       " 'DT': 0.000316146462033684,\n",
       " 'FW': 0.0007472552738977985,\n",
       " 'IN': 0.012473414956601713,\n",
       " 'JJ': 0.23452319365407828,\n",
       " 'JJR': 0.0027878369833879407,\n",
       " 'JJS': 0.0019256193596597115,\n",
       " 'MD': 0.0013795481979651664,\n",
       " 'NN': 0.5114674943955855,\n",
       " 'NNP': 0.000862217623728229,\n",
       " 'NNS': 0.01727309306202219,\n",
       " 'PDT': 5.748117491521527e-05,\n",
       " 'PRP': 0.0001724435247456458,\n",
       " 'PRP$': 2.8740587457607634e-05,\n",
       " 'RB': 0.058372133126401105,\n",
       " 'RBR': 0.0016669540725412428,\n",
       " 'RBS': 0.00014370293728803817,\n",
       " 'RP': 0.0008334770362706214,\n",
       " 'UH': 8.62217623728229e-05,\n",
       " 'VB': 0.030579985054894523,\n",
       " 'VBD': 0.028137035120997873,\n",
       " 'VBG': 0.014370293728803817,\n",
       " 'VBN': 0.010001724435247456,\n",
       " 'VBP': 0.06337299534402484,\n",
       " 'VBZ': 0.00485715928033569,\n",
       " 'WDT': 0.00011496234983043054,\n",
       " 'WP': 0.0005173305742369374,\n",
       " 'WP$': 0.0003448870494912916,\n",
       " 'WRB': 8.62217623728229e-05}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_prof = nltk.pos_tag(tokens_prof_clean)\n",
    "    \n",
    "from collections import Counter\n",
    "counts_prof = Counter(tag for word,tag in pos_tags_prof)\n",
    "print(counts_prof)\n",
    "total = sum(counts_prof.values())\n",
    "dict((word, float(n)/total) for word,n in counts_prof.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf of each corpus with regard to other two corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_texts = [lemmas_child, lemmas_lay, lemmas_prof]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 12683)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(min_df=1, max_df=3, stop_words = 'english')\n",
    "tf = count.fit_transform(three_texts).toarray()\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 12683)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df=1, max_df=3, stop_words = 'english')\n",
    "tfidf = vect.fit_transform(three_texts).toarray()\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'friend': 4376,\n",
       " 'hang': 5005,\n",
       " 'shop': 9789,\n",
       " 'ice': 5476,\n",
       " 'cream': 2456,\n",
       " 'oreo': 7646,\n",
       " 'money': 7065,\n",
       " 'happy': 5019,\n",
       " 'mother': 7126,\n",
       " 'day': 2715,\n",
       " 'mum': 7200,\n",
       " 'love': 6531,\n",
       " 'miss': 7005,\n",
       " 'new': 7351,\n",
       " 'zealand': 12657,\n",
       " 'great': 4811,\n",
       " 'really': 8804,\n",
       " 'good': 4709,\n",
       " 'place': 8154,\n",
       " 'country': 2393,\n",
       " 'house': 5388,\n",
       " 'lot': 6519,\n",
       " 'poisonous': 8254,\n",
       " 'spider': 10327,\n",
       " 'want': 12189,\n",
       " 'bite': 1028,\n",
       " 'rat': 8769,\n",
       " 'amaze': 311,\n",
       " 'easy': 3403,\n",
       " 'chase': 1770,\n",
       " 'bee': 883,\n",
       " 'aspire': 565,\n",
       " 'believe': 919,\n",
       " 'confidence': 2218,\n",
       " 'dream': 3257,\n",
       " 'enjoy': 3596,\n",
       " 'family': 3890,\n",
       " 'greatness': 4813,\n",
       " 'inspire': 5727,\n",
       " 'joy': 5965,\n",
       " 'kindness': 6064,\n",
       " 'mystery': 7244,\n",
       " 'precious': 8396,\n",
       " 'question': 8659,\n",
       " 'rely': 8947,\n",
       " 'special': 10298,\n",
       " 'try': 11571,\n",
       " 'unstoppable': 11888,\n",
       " 'vacate': 11967,\n",
       " 'worship': 12538,\n",
       " 'ray': 8782,\n",
       " 'yolo': 12630,\n",
       " 'zone': 12676,\n",
       " 'drink': 3282,\n",
       " 'sucker': 10721,\n",
       " 'enviromental': 3635,\n",
       " 'dislike': 3090,\n",
       " 'pointless': 8250,\n",
       " 'plastic': 8173,\n",
       " 'ocean': 7551,\n",
       " 'destroyer': 2928,\n",
       " 'hi': 5220,\n",
       " 'piece': 8091,\n",
       " 'paper': 7811,\n",
       " 'write': 12573,\n",
       " 'draw': 3249,\n",
       " 'turn': 11606,\n",
       " 'aeroplane': 167,\n",
       " 'old': 7583,\n",
       " 'play': 8181,\n",
       " 'dress': 3272,\n",
       " 'teddy': 11053,\n",
       " 'magic': 6629,\n",
       " 'fairy': 3869,\n",
       " 'like': 6371,\n",
       " 'unicorn': 11805,\n",
       " 'rainbow': 8720,\n",
       " 'discourage': 3058,\n",
       " 'small': 10069,\n",
       " 'leave': 6267,\n",
       " 'fingerprint': 4050,\n",
       " 'furniture': 4447,\n",
       " 'wall': 12171,\n",
       " 'chair': 1734,\n",
       " 'sit': 9926,\n",
       " 'rest': 9035,\n",
       " 'compare': 2145,\n",
       " 'couch': 2379,\n",
       " 'best': 959,\n",
       " 'beep': 887,\n",
       " 'sheep': 9738,\n",
       " 'dad': 2647,\n",
       " 'nice': 7363,\n",
       " 'helpful': 5183,\n",
       " 'help': 5181,\n",
       " 'hurt': 5445,\n",
       " 'listen': 6424,\n",
       " 'little': 6436,\n",
       " 'bit': 1026,\n",
       " 'cheeky': 1790,\n",
       " 'let': 6312,\n",
       " 'silly': 9882,\n",
       " 'chill': 1838,\n",
       " 'eat': 3404,\n",
       " 'healthy': 5109,\n",
       " 'stuff': 10677,\n",
       " 'dads': 2649,\n",
       " 'sister': 9925,\n",
       " 'annoy': 388,\n",
       " 'bossy': 1219,\n",
       " 'attitude': 620,\n",
       " 'naughty': 7291,\n",
       " 'hurtful': 5446,\n",
       " 'argumentative': 504,\n",
       " 'school': 9494,\n",
       " 'death': 2738,\n",
       " 'end': 3565,\n",
       " 'road': 9183,\n",
       " 'life': 6346,\n",
       " 'traveler': 11477,\n",
       " 'soul': 10246,\n",
       " 'guide': 4902,\n",
       " 'solid': 10192,\n",
       " 'mountain': 7143,\n",
       " 'emerge': 3528,\n",
       " 'view': 12062,\n",
       " 'way': 12257,\n",
       " 'black': 1039,\n",
       " 'white': 12362,\n",
       " 'picture': 8088,\n",
       " 'forever': 4265,\n",
       " 'real': 8798,\n",
       " 'secret': 9589,\n",
       " 'giggle': 4600,\n",
       " 'chat': 1776,\n",
       " 'feel': 3969,\n",
       " 'sad': 9347,\n",
       " 'trust': 11567,\n",
       " 'grow': 4874,\n",
       " 'cat': 1661,\n",
       " 'wander': 12185,\n",
       " 'forest': 4261,\n",
       " 'aware': 673,\n",
       " 'tree': 11492,\n",
       " 'climb': 1983,\n",
       " 'high': 5230,\n",
       " 'marshy': 6732,\n",
       " 'swamp': 10852,\n",
       " 'stuck': 10672,\n",
       " 'come': 2112,\n",
       " 'stop': 10589,\n",
       " 'saw': 9444,\n",
       " 'strolling': 10656,\n",
       " 'past': 7878,\n",
       " 'run': 9310,\n",
       " 'paw': 7913,\n",
       " 'furry': 4452,\n",
       " 'coat': 2042,\n",
       " 'mark': 6717,\n",
       " 'site': 9928,\n",
       " 'people': 7975,\n",
       " 'send': 9626,\n",
       " 'poem': 8237,\n",
       " 'advertising': 158,\n",
       " 'comment': 2131,\n",
       " 'mention': 6858,\n",
       " 'sorry': 10240,\n",
       " 'meow': 6860,\n",
       " 'strum': 10666,\n",
       " 'guitar': 4910,\n",
       " 'dog': 3150,\n",
       " 'woof': 12507,\n",
       " 'beat': 854,\n",
       " 'drum': 3311,\n",
       " 'bird': 1016,\n",
       " 'song': 10211,\n",
       " 'tin': 11297,\n",
       " 'whistle': 12360,\n",
       " 'tune': 11594,\n",
       " 'pig': 8101,\n",
       " 'grunt': 4891,\n",
       " 'toot': 11376,\n",
       " 'kazoo': 6027,\n",
       " 'flute': 4193,\n",
       " 'melody': 6832,\n",
       " 'graceful': 4744,\n",
       " 'swan': 10855,\n",
       " 'shake': 9697,\n",
       " 'maraca': 6700,\n",
       " 'rattle': 8772,\n",
       " 'snake': 10102,\n",
       " 'jive': 5932,\n",
       " 'fairyland': 3870,\n",
       " 'clock': 1993,\n",
       " 'strike': 10641,\n",
       " 'lunch': 6569,\n",
       " 'time': 11291,\n",
       " 'meant': 6803,\n",
       " 'dust': 3353,\n",
       " 'twinkle': 11640,\n",
       " 'shone': 9785,\n",
       " 'golden': 4699,\n",
       " 'twas': 11622,\n",
       " 'gold': 4698,\n",
       " 'break': 1287,\n",
       " 'plate': 8175,\n",
       " 'blame': 1050,\n",
       " 'make': 6651,\n",
       " 'late': 6210,\n",
       " 'snap': 10105,\n",
       " 'key': 6042,\n",
       " 'blue': 1127,\n",
       " 'mean': 6800,\n",
       " 'flu': 4180,\n",
       " 'look': 6496,\n",
       " 'right': 9149,\n",
       " 'everyday': 3715,\n",
       " 'rrrrreeeeaaalllyyyy': 9276,\n",
       " 'kind': 6059,\n",
       " 'shout': 9805,\n",
       " 'annoying': 390,\n",
       " 'work': 12517,\n",
       " 'aaaaahhhhhh': 3,\n",
       " 'wolf': 12487,\n",
       " 'den': 2863,\n",
       " 'provide': 8542,\n",
       " 'shelter': 9744,\n",
       " 'warmth': 12207,\n",
       " 'cub': 2566,\n",
       " 'soon': 10219,\n",
       " 'bear': 847,\n",
       " 'chicken': 1824,\n",
       " 'fun': 4433,\n",
       " 'fee': 3964,\n",
       " 'ciennah': 1902,\n",
       " 'age': 198,\n",
       " 'home': 5304,\n",
       " 'maybe': 6783,\n",
       " 'thing': 11175,\n",
       " 'fall': 3879,\n",
       " 'pick': 8084,\n",
       " 'able': 24,\n",
       " 'remember': 8953,\n",
       " 'say': 9447,\n",
       " 'tiger': 11270,\n",
       " 'loud': 6522,\n",
       " 'roar': 9188,\n",
       " 'fierce': 4017,\n",
       " 'proud': 8538,\n",
       " 'cloud': 2013,\n",
       " 'float': 4151,\n",
       " 'sky': 9974,\n",
       " 'steam': 10523,\n",
       " 'rise': 9171,\n",
       " 'cap': 1576,\n",
       " 'sea': 9563,\n",
       " 'shower': 9811,\n",
       " 'water': 12238,\n",
       " 'tap': 10996,\n",
       " 'drain': 3239,\n",
       " 'rain': 8719,\n",
       " 'collecting': 2081,\n",
       " 'pan': 7791,\n",
       " 'filter': 4039,\n",
       " 'soggy': 10173,\n",
       " 'sand': 9393,\n",
       " 'jealousy': 5890,\n",
       " 'green': 4820,\n",
       " 'taste': 11015,\n",
       " 'porridge': 8316,\n",
       " 'smell': 10077,\n",
       " 'jungle': 5990,\n",
       " 'berry': 951,\n",
       " 'pull': 8579,\n",
       " 'hair': 4962,\n",
       " 'arrive': 530,\n",
       " 'think': 11177,\n",
       " 'add': 109,\n",
       " 'emoticon': 3534,\n",
       " 'word': 12511,\n",
       " 'tell': 11071,\n",
       " 'share': 9715,\n",
       " 'message': 6888,\n",
       " 'thanks': 11135,\n",
       " 'roger': 9211,\n",
       " 'stevens': 10542,\n",
       " 'body': 1157,\n",
       " 'start': 10491,\n",
       " 'walk': 12168,\n",
       " 'torch': 11389,\n",
       " 'worry': 12535,\n",
       " 'ghost': 4588,\n",
       " 'walking': 12170,\n",
       " 'door': 3185,\n",
       " 'oh': 7574,\n",
       " 'guess': 4897,\n",
       " 'win': 12414,\n",
       " 'haunt': 5076,\n",
       " 'blood': 1106,\n",
       " 'drip': 3285,\n",
       " 'bye': 1483,\n",
       " 'orange': 7633,\n",
       " 'darker': 2692,\n",
       " 'true': 11558,\n",
       " 'yellow': 12619,\n",
       " 'light': 6361,\n",
       " 'shade': 9689,\n",
       " 'color': 2096,\n",
       " 'daylight': 2717,\n",
       " 'sun': 10761,\n",
       " 'act': 96,\n",
       " 'story': 10597,\n",
       " 'costume': 2374,\n",
       " 'heel': 5156,\n",
       " 'rose': 9243,\n",
       " 'red': 8861,\n",
       " 'violet': 12081,\n",
       " 'crayon': 2449,\n",
       " 'spike': 10331,\n",
       " 'prick': 8439,\n",
       " 'partner': 7861,\n",
       " 'different': 2991,\n",
       " 'size': 9935,\n",
       " 'big': 994,\n",
       " 'live': 6438,\n",
       " 'zoo': 12677,\n",
       " 'pet': 8040,\n",
       " 'monster': 7072,\n",
       " 'room': 9233,\n",
       " 'alive': 269,\n",
       " 'stand': 10472,\n",
       " 'tall': 10978,\n",
       " 'wind': 12415,\n",
       " 'branch': 1275,\n",
       " 'spooky': 10378,\n",
       " 'curled': 2603,\n",
       " 'sharp': 9720,\n",
       " 'creature': 2468,\n",
       " 'crawl': 2446,\n",
       " 'long': 6488,\n",
       " 'leg': 6279,\n",
       " 'beetle': 892,\n",
       " 'shiny': 9766,\n",
       " 'shell': 9743,\n",
       " 'dark': 2688,\n",
       " 'murky': 7209,\n",
       " 'ready': 8797,\n",
       " 'stormy': 10596,\n",
       " 'glint': 4647,\n",
       " 'thunder': 11245,\n",
       " 'lightning': 6368,\n",
       " 'blast': 1062,\n",
       " 'soak': 10150,\n",
       " 'wet': 12320,\n",
       " 'slowly': 10049,\n",
       " 'rot': 9248,\n",
       " 'away': 675,\n",
       " 'cow': 2412,\n",
       " 'shin': 9762,\n",
       " 'degree': 2823,\n",
       " 'barbecue': 770,\n",
       " 'lit': 6427,\n",
       " 'watch': 12233,\n",
       " 'glass': 4632,\n",
       " 'hat': 5060,\n",
       " 'sing': 9906,\n",
       " 'flower': 4171,\n",
       " 'blossom': 1117,\n",
       " 'sunday': 10765,\n",
       " 'toy': 11428,\n",
       " 'jacob': 5861,\n",
       " 'rooster': 9238,\n",
       " 'crow': 2524,\n",
       " 'owl': 7734,\n",
       " 'hoot': 5340,\n",
       " 'pigeon': 8102,\n",
       " 'coo': 2320,\n",
       " 'dove': 3215,\n",
       " 'robin': 9196,\n",
       " 'chirp': 1849,\n",
       " 'squark': 10421,\n",
       " 'duck': 3319,\n",
       " 'quack': 8636,\n",
       " 'swallow': 10850,\n",
       " 'talk': 10975,\n",
       " 'wood': 12502,\n",
       " 'dance': 2669,\n",
       " 'whirl': 12350,\n",
       " 'blow': 1120,\n",
       " 'leaf': 6247,\n",
       " 'ground': 4869,\n",
       " 'step': 10538,\n",
       " 'hear': 5112,\n",
       " 'crunch': 2548,\n",
       " 'beneath': 940,\n",
       " 'foot': 4231,\n",
       " 'fly': 4196,\n",
       " 'head': 5094,\n",
       " 'rabbit': 8688,\n",
       " 'hop': 5344,\n",
       " 'middle': 6921,\n",
       " 'listening': 6425,\n",
       " 'nature': 7288,\n",
       " 'sound': 10250,\n",
       " 'peace': 7920,\n",
       " 'bed': 868,\n",
       " 'sol': 10179,\n",
       " 'shoe': 9783,\n",
       " 'stab': 10442,\n",
       " 'grain': 4758,\n",
       " 'population': 8310,\n",
       " 'china': 1846,\n",
       " 'computer': 2177,\n",
       " 'cool': 2330,\n",
       " 'wake': 12164,\n",
       " 'morning': 7103,\n",
       " 'stomach': 10577,\n",
       " 'ache': 78,\n",
       " 'goodness': 4711,\n",
       " 'sake': 9368,\n",
       " 'night': 7374,\n",
       " 'chilli': 1839,\n",
       " 'kick': 6049,\n",
       " 'spicy': 10326,\n",
       " 'deserve': 2906,\n",
       " 'ipad': 5816,\n",
       " 'phone': 8069,\n",
       " 'loan': 6458,\n",
       " 'mom': 7056,\n",
       " 'uncle': 11712,\n",
       " 'dill': 3003,\n",
       " 'instead': 5735,\n",
       " 'electronic': 3479,\n",
       " 'eye': 3818,\n",
       " 'pad': 7749,\n",
       " 'cut': 2622,\n",
       " 'india': 5627,\n",
       " 'chutney': 1898,\n",
       " 'food': 4224,\n",
       " 'seafood': 9567,\n",
       " 'pakistan': 7770,\n",
       " 'culture': 2582,\n",
       " 'relatives': 8930,\n",
       " 'vulture': 12140,\n",
       " 'holiday': 5295,\n",
       " 'friends': 4378,\n",
       " 'title': 11322,\n",
       " 'choose': 1866,\n",
       " 'fight': 4026,\n",
       " 'borrow': 1215,\n",
       " 'tomorrow': 11359,\n",
       " 'trade': 11437,\n",
       " 'privacy': 8463,\n",
       " 'invade': 5800,\n",
       " 'hug': 5407,\n",
       " 'smile': 10081,\n",
       " 'tear': 11044,\n",
       " 'shed': 9736,\n",
       " 'spread': 10397,\n",
       " 'know': 6106,\n",
       " 'care': 1607,\n",
       " 'laugh': 6219,\n",
       " 'wish': 12458,\n",
       " 'answer': 396,\n",
       " 'fade': 3854,\n",
       " 'promise': 8500,\n",
       " 'lose': 6514,\n",
       " 'heart': 5118,\n",
       " 'apart': 423,\n",
       " 'use': 11949,\n",
       " 'important': 5574,\n",
       " 'respect': 9025,\n",
       " 'lovingly': 6540,\n",
       " 'cute': 2623,\n",
       " 'birthday': 1023,\n",
       " 'dragon': 3237,\n",
       " 'hill': 5241,\n",
       " 'abc': 12,\n",
       " 'short': 9795,\n",
       " 'def': 2795,\n",
       " 'hij': 5239,\n",
       " 'hike': 5240,\n",
       " 'finally': 4043,\n",
       " 'lolly': 6481,\n",
       " 'sweet': 10872,\n",
       " 'relax': 8931,\n",
       " 'flow': 4170,\n",
       " 'happen': 5012,\n",
       " 'forward': 4303,\n",
       " 'minecraft': 6963,\n",
       " 'brainy': 1271,\n",
       " 'fast': 3924,\n",
       " 'unfortunately': 11788,\n",
       " 'bad': 715,\n",
       " 'paradise': 7819,\n",
       " 'split': 10365,\n",
       " 'yum': 12647,\n",
       " 'ok': 7577,\n",
       " 'quiet': 8671,\n",
       " 'sight': 9863,\n",
       " 'wave': 12248,\n",
       " 'wonderful': 12493,\n",
       " 'spirit': 10347,\n",
       " 'build': 1406,\n",
       " 'shelf': 9742,\n",
       " 'creation': 2463,\n",
       " 'craft': 2431,\n",
       " 'mind': 6960,\n",
       " 'tasty': 11019,\n",
       " 'temptation': 11081,\n",
       " 'careful': 1611,\n",
       " 'wise': 12456,\n",
       " 'fortnite': 4297,\n",
       " 'fortnight': 4295,\n",
       " 'pleasure': 8195,\n",
       " 'mate': 6762,\n",
       " 'melissa': 6826,\n",
       " 'drop': 3299,\n",
       " 'hide': 5226,\n",
       " 'seek': 9601,\n",
       " 'tidy': 11263,\n",
       " 'messy': 6894,\n",
       " 'bully': 1418,\n",
       " 'sigh': 9861,\n",
       " 'norman': 7443,\n",
       " 'quite': 8680,\n",
       " 'brilliant': 1335,\n",
       " 'idea': 5484,\n",
       " 'scale': 9453,\n",
       " 'purple': 8611,\n",
       " 'shine': 9763,\n",
       " 'amazing': 312,\n",
       " 'yeah': 12609,\n",
       " 'yes': 12621,\n",
       " 'breeze': 1304,\n",
       " 'sway': 10863,\n",
       " 'hooting': 5343,\n",
       " 'beak': 842,\n",
       " 'wing': 12433,\n",
       " 'gleam': 4636,\n",
       " 'moon': 7081,\n",
       " 'silhouette': 9876,\n",
       " 'swoop': 10911,\n",
       " 'search': 9573,\n",
       " 'prey': 8436,\n",
       " 'fro': 4392,\n",
       " 'spot': 10386,\n",
       " 'goes': 4693,\n",
       " 'read': 8791,\n",
       " 'far': 3905,\n",
       " 'clearly': 1969,\n",
       " 'sleep': 10004,\n",
       " 'early': 3381,\n",
       " 'happily': 5017,\n",
       " 'clean': 1961,\n",
       " 'properly': 8513,\n",
       " 'strange': 10608,\n",
       " 'fear': 3950,\n",
       " 'butterfly': 1473,\n",
       " 'mouse': 7154,\n",
       " 'die': 2982,\n",
       " 'man': 6666,\n",
       " 'cousin': 2405,\n",
       " 'happiness': 5018,\n",
       " 'cuddle': 2569,\n",
       " 'await': 665,\n",
       " 'sugar': 10737,\n",
       " 'moist': 7047,\n",
       " 'yummy': 12648,\n",
       " 'mixture': 7023,\n",
       " 'inside': 5716,\n",
       " 'melt': 6834,\n",
       " 'mouth': 7162,\n",
       " 'elsie': 3502,\n",
       " 'paint': 7763,\n",
       " 'hate': 5067,\n",
       " 'thistle': 11187,\n",
       " 'educate': 3433,\n",
       " 'poetry': 8243,\n",
       " 'footpath': 4235,\n",
       " 'world': 12526,\n",
       " 'chocolate': 1858,\n",
       " 'rhyme': 9107,\n",
       " 'goodbye': 4710,\n",
       " 'mix': 7019,\n",
       " 'colour': 2101,\n",
       " 'orchard': 7638,\n",
       " 'juice': 5979,\n",
       " 'seed': 9600,\n",
       " 'cure': 2594,\n",
       " 'gramps': 4763,\n",
       " 'grig': 4839,\n",
       " 'field': 4014,\n",
       " 'clover': 2021,\n",
       " 'piggy': 8103,\n",
       " 'need': 7318,\n",
       " 'wait': 12160,\n",
       " 'tonight': 11368,\n",
       " 'superbowl': 10781,\n",
       " 'cheer': 1791,\n",
       " 'lovely': 6535,\n",
       " 'friday': 4373,\n",
       " 'butter': 1472,\n",
       " 'toast': 11329,\n",
       " 'face': 3841,\n",
       " 'sure': 10806,\n",
       " 'monday': 7063,\n",
       " 'lobby': 6460,\n",
       " 'swim': 10891,\n",
       " 'pine': 8118,\n",
       " 'delicious': 2835,\n",
       " 'marshmallows': 6731,\n",
       " 'beautiful': 861,\n",
       " 'star': 10478,\n",
       " 'haiku': 4959,\n",
       " 'sunset': 10776,\n",
       " 'summer': 10755,\n",
       " 'flame': 4095,\n",
       " 'burn': 1444,\n",
       " 'bonfire': 1185,\n",
       " 'glow': 4667,\n",
       " 'bright': 1326,\n",
       " 'sweetness': 10881,\n",
       " 'freshly': 4368,\n",
       " 'grass': 4788,\n",
       " 'sadness': 9353,\n",
       " 'emotion': 3535,\n",
       " 'indigo': 5636,\n",
       " 'carrot': 1633,\n",
       " 'parrot': 7846,\n",
       " 'ride': 9133,\n",
       " 'chariot': 1758,\n",
       " 'tow': 11419,\n",
       " 'rotten': 9252,\n",
       " 'thrown': 11236,\n",
       " 'left': 6277,\n",
       " 'outside': 7694,\n",
       " 'peck': 7935,\n",
       " 'boy': 1250,\n",
       " 'pitch': 8140,\n",
       " 'beauty': 863,\n",
       " 'earth': 3386,\n",
       " 'ha': 4942,\n",
       " 'teach': 11036,\n",
       " 'english': 3586,\n",
       " 'spanish': 10275,\n",
       " 'hindi': 5246,\n",
       " 'portuguese': 8324,\n",
       " 'splash': 10355,\n",
       " 'tsunami': 11576,\n",
       " 'storm': 10595,\n",
       " 'soil': 10175,\n",
       " 'form': 4284,\n",
       " 'race': 8691,\n",
       " 'bunch': 1427,\n",
       " 'rude': 9288,\n",
       " 'sport': 10383,\n",
       " 'pump': 8588,\n",
       " 'forget': 4271,\n",
       " 'utility': 11959,\n",
       " 'pole': 8259,\n",
       " 'west': 12316,\n",
       " 'hot': 5380,\n",
       " 'cover': 2408,\n",
       " 'filbert': 4031,\n",
       " 'fox': 4315,\n",
       " 'gon': 4703,\n",
       " 'master': 6753,\n",
       " 'table': 10937,\n",
       " 'howl': 5397,\n",
       " 'vicious': 12051,\n",
       " 'moonlight': 7085,\n",
       " 'game': 4496,\n",
       " 'catch': 1665,\n",
       " 'cave': 1687,\n",
       " 'stay': 10513,\n",
       " 'slobber': 10039,\n",
       " 'drool': 3294,\n",
       " 'teeth': 11062,\n",
       " 'mace': 6607,\n",
       " 'bruise': 1375,\n",
       " 'especially': 3681,\n",
       " 'bat': 814,\n",
       " 'log': 6474,\n",
       " 'air': 229,\n",
       " 'conditioner': 2206,\n",
       " 'toilet': 11343,\n",
       " 'google': 4714,\n",
       " 'classroom': 1955,\n",
       " 'prefect': 8405,\n",
       " 'honest': 5320,\n",
       " 'fair': 3865,\n",
       " 'video': 12058,\n",
       " 'youtube': 12643,\n",
       " 'youtuber': 12644,\n",
       " 'parent': 7834,\n",
       " 'ask': 559,\n",
       " 'permission': 8004,\n",
       " 'post': 8338,\n",
       " 'excitedly': 3744,\n",
       " 'role': 9216,\n",
       " 'lo': 6454,\n",
       " 'lov': 6529,\n",
       " 'fort': 4291,\n",
       " 'fortn': 4293,\n",
       " 'fortni': 4294,\n",
       " 'fortnit': 4296,\n",
       " 'pub': 8567,\n",
       " 'pubg': 8568,\n",
       " 'curtain': 2611,\n",
       " 'brewing': 1311,\n",
       " 'chime': 1842,\n",
       " 'harry': 5047,\n",
       " 'prefer': 8406,\n",
       " 'perry': 8013,\n",
       " 'fell': 3978,\n",
       " 'hole': 5292,\n",
       " 'yeet': 12616,\n",
       " 'breakfast': 1289,\n",
       " 'realize': 8803,\n",
       " 'meat': 6807,\n",
       " 'fine': 4047,\n",
       " 'mirror': 6988,\n",
       " 'brush': 1380,\n",
       " 'pack': 7744,\n",
       " 'bag': 719,\n",
       " 'math': 6766,\n",
       " 'test': 11120,\n",
       " 'gag': 4473,\n",
       " 'img': 5531,\n",
       " 'alt': 299,\n",
       " 'class': 1952,\n",
       " 'wp': 12552,\n",
       " 'smiley': 10082,\n",
       " 'src': 10437,\n",
       " 'http': 5401,\n",
       " 'style': 10689,\n",
       " 'height': 5163,\n",
       " 'max': 6782,\n",
       " 'yell': 12617,\n",
       " 'stair': 10456,\n",
       " 'bob': 1153,\n",
       " 'mule': 7193,\n",
       " 'later': 6212,\n",
       " 'wonder': 12492,\n",
       " 'pie': 8090,\n",
       " 'yay': 12606,\n",
       " 'river': 9178,\n",
       " 'beast': 851,\n",
       " 'shiver': 9775,\n",
       " 'usa': 11947,\n",
       " 'veggie': 12012,\n",
       " 'teacher': 11037,\n",
       " 'book': 1194,\n",
       " 'brother': 1367,\n",
       " 'hershey': 5213,\n",
       " 'bar': 765,\n",
       " 'rush': 9320,\n",
       " 'minute': 6982,\n",
       " 'energy': 3579,\n",
       " 'car': 1594,\n",
       " 'homework': 5313,\n",
       " 'jump': 5983,\n",
       " 'alarm': 241,\n",
       " 'set': 9666,\n",
       " 'cold': 2073,\n",
       " 'dread': 3255,\n",
       " 'downstairs': 3222,\n",
       " 'munch': 7203,\n",
       " 'dare': 2685,\n",
       " 'hungry': 5433,\n",
       " 'mommy': 7060,\n",
       " 'daddy': 2648,\n",
       " 'ate': 597,\n",
       " 'bus': 1461,\n",
       " 'leap': 6254,\n",
       " 'motto': 7135,\n",
       " 'lotto': 6521,\n",
       " 'inspirational': 5726,\n",
       " 'feature': 3961,\n",
       " 'learning': 6260,\n",
       " 'pride': 8442,\n",
       " 'powerful': 8372,\n",
       " 'stride': 10638,\n",
       " 'jewel': 5918,\n",
       " 'balloon': 743,\n",
       " 'sibling': 9840,\n",
       " 'treat': 11489,\n",
       " 'mister': 7014,\n",
       " 'perfect': 7986,\n",
       " 'pumpkin': 8589,\n",
       " 'baby': 697,\n",
       " 'fussy': 4460,\n",
       " 'feeling': 3970,\n",
       " 'sunshine': 10777,\n",
       " 'fence': 3984,\n",
       " 'intense': 5754,\n",
       " 'poor': 8295,\n",
       " 'straight': 10602,\n",
       " 'woooed': 12510,\n",
       " 'gymnastics': 4940,\n",
       " 'simply': 9898,\n",
       " 'flip': 4147,\n",
       " 'cartwheel': 1642,\n",
       " 'lead': 6244,\n",
       " 'deep': 2788,\n",
       " 'crystal': 2565,\n",
       " 'glimmer': 4644,\n",
       " 'coral': 2343,\n",
       " 'waterfall': 12239,\n",
       " 'rainy': 8726,\n",
       " 'umbrella': 11677,\n",
       " 'eating': 3406,\n",
       " 'mozzarella': 7178,\n",
       " 'ramble': 8734,\n",
       " 'hard': 5022,\n",
       " 'lawn': 6233,\n",
       " 'topic': 11383,\n",
       " 'quantity': 8647,\n",
       " 'wide': 12384,\n",
       " 'bike': 999,\n",
       " 'rid': 9129,\n",
       " 'riding': 9141,\n",
       " 'backwards': 711,\n",
       " 'dinner': 3015,\n",
       " 'tower': 11422,\n",
       " 'terror': 11115,\n",
       " 'restaurant': 9036,\n",
       " 'enjoying': 3598,\n",
       " 'transparent': 11468,\n",
       " 'wear': 12266,\n",
       " 'sleeve': 10011,\n",
       " 'guffaw': 4900,\n",
       " 'laughter': 6220,\n",
       " 'prance': 8381,\n",
       " 'free': 4349,\n",
       " 'groove': 4861,\n",
       " 'stomp': 10579,\n",
       " 'romp': 9226,\n",
       " 'welcome': 12304,\n",
       " 'bin': 1011,\n",
       " 'solve': 10201,\n",
       " 'maze': 6787,\n",
       " 'bring': 1341,\n",
       " 'nigeria': 7371,\n",
       " 'mystical': 7246,\n",
       " 'sandy': 9398,\n",
       " 'floor': 4158,\n",
       " 'algae': 260,\n",
       " 'widow': 12387,\n",
       " 'sung': 10768,\n",
       " 'today': 11335,\n",
       " 'follow': 4219,\n",
       " 'gracefully': 4745,\n",
       " 'flowingly': 4178,\n",
       " 'harmonise': 5040,\n",
       " 'africa': 189,\n",
       " 'huge': 5408,\n",
       " 'continent': 2298,\n",
       " 'honey': 5323,\n",
       " 'hold': 5288,\n",
       " 'tricky': 11518,\n",
       " 'north': 7445,\n",
       " 'south': 10261,\n",
       " 'america': 328,\n",
       " 'europe': 3701,\n",
       " 'kangaroo': 6013,\n",
       " 'stralia': 10606,\n",
       " 'australia': 642,\n",
       " 'asia': 557,\n",
       " 'antarctica': 399,\n",
       " 'ring': 9156,\n",
       " 'ear': 3378,\n",
       " 'jolt': 5950,\n",
       " 'electricity': 3476,\n",
       " 'hurtle': 5448,\n",
       " 'lay': 6235,\n",
       " 'scared': 9468,\n",
       " 'noise': 7418,\n",
       " 'bang': 754,\n",
       " 'swing': 10898,\n",
       " 'window': 12423,\n",
       " 'crackle': 2428,\n",
       " 'angrily': 372,\n",
       " 'sparkle': 10281,\n",
       " 'flicker': 4136,\n",
       " 'existence': 3765,\n",
       " 'disappear': 3041,\n",
       " 'second': 9588,\n",
       " 'hypnotize': 5466,\n",
       " 'mesmerize': 6886,\n",
       " 'steel': 10526,\n",
       " 'stamen': 10466,\n",
       " 'unwavering': 11911,\n",
       " 'reveal': 9066,\n",
       " 'gaze': 4541,\n",
       " 'steely': 10527,\n",
       " 'expression': 3801,\n",
       " 'icy': 5481,\n",
       " 'pierce': 8095,\n",
       " 'anybody': 417,\n",
       " 'terrify': 11112,\n",
       " 'blaze': 1064,\n",
       " 'passion': 7874,\n",
       " 'infinite': 5656,\n",
       " 'fuel': 4424,\n",
       " 'ablaze': 23,\n",
       " 'soften': 10167,\n",
       " 'mask': 6743,\n",
       " 'perplex': 8011,\n",
       " 'year': 12610,\n",
       " 'warm': 12202,\n",
       " 'armor': 516,\n",
       " 'reflection': 8885,\n",
       " 'smooth': 10092,\n",
       " 'scent': 9482,\n",
       " 'jar': 5880,\n",
       " 'travel': 11476,\n",
       " 'brighten': 1327,\n",
       " 'rad': 8699,\n",
       " 'super': 10779,\n",
       " 'okay': 7578,\n",
       " 'morocco': 7104,\n",
       " 'niger': 7370,\n",
       " 'chad': 1731,\n",
       " 'swaziland': 10866,\n",
       " 'band': 750,\n",
       " 'brand': 1276,\n",
       " 'legs': 6288,\n",
       " 'shard': 9714,\n",
       " 'tiny': 11309,\n",
       " 'flap': 4098,\n",
       " 'plant': 8169,\n",
       " 'asleep': 562,\n",
       " 'brown': 1371,\n",
       " 'dig': 2995,\n",
       " 'speckle': 10302,\n",
       " 'nose': 7448,\n",
       " 'lick': 6337,\n",
       " 'pollution': 8269,\n",
       " 'awful': 679,\n",
       " 'crummy': 2545,\n",
       " 'killing': 6058,\n",
       " 'execute': 3751,\n",
       " 'saudi': 9431,\n",
       " 'arabia': 477,\n",
       " 'korea': 6121,\n",
       " 'greenland': 4822,\n",
       " 'healing': 5106,\n",
       " 'save': 9438,\n",
       " 'acceptable': 58,\n",
       " 'fresh': 4365,\n",
       " 'animal': 377,\n",
       " 'human': 5415,\n",
       " 'factory': 3851,\n",
       " 'victory': 12057,\n",
       " 'hello': 5177,\n",
       " 'habitat': 4947,\n",
       " 'dry': 3317,\n",
       " 'heat': 5132,\n",
       " 'burnt': 1449,\n",
       " 'buzz': 1480,\n",
       " 'brings': 1344,\n",
       " 'sunny': 10773,\n",
       " 'seat': 9582,\n",
       " 'roller': 9218,\n",
       " 'coaster': 2041,\n",
       " 'zoom': 12679,\n",
       " 'slope': 10042,\n",
       " 'whoosh': 12375,\n",
       " 'steep': 10528,\n",
       " 'bump': 1423,\n",
       " 'corner': 2349,\n",
       " 'fling': 4144,\n",
       " 'force': 4243,\n",
       " 'scrap': 9526,\n",
       " 'spring': 10401,\n",
       " 'sudden': 10725,\n",
       " 'faster': 3926,\n",
       " 'round': 9260,\n",
       " 'flash': 4102,\n",
       " 'scream': 9536,\n",
       " 'push': 8623,\n",
       " 'thriller': 11223,\n",
       " 'gaping': 4507,\n",
       " 'memory': 6842,\n",
       " 'painful': 7759,\n",
       " 'joyous': 5967,\n",
       " 'procrastinate': 8477,\n",
       " 'heartbeat': 5121,\n",
       " 'creamy': 2457,\n",
       " 'coloured': 2102,\n",
       " 'throat': 11225,\n",
       " 'milk': 6940,\n",
       " 'silk': 9877,\n",
       " 'bark': 782,\n",
       " 'hangar': 5006,\n",
       " 'airport': 232,\n",
       " 'pancake': 7792,\n",
       " 'favourite': 3944,\n",
       " ...}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['95',\n",
       " 'aaaaaaaaaaaaaccccccccccoooooooo',\n",
       " 'aaaaaaaaaaahhhhhhhhhhhhhhhh',\n",
       " 'aaaaahhhhhh',\n",
       " 'aarhus',\n",
       " 'abacus',\n",
       " 'abandon',\n",
       " 'abated',\n",
       " 'abattoir',\n",
       " 'abbey',\n",
       " 'abbie',\n",
       " 'abbot',\n",
       " 'abc',\n",
       " 'abdicate',\n",
       " 'abdomen',\n",
       " 'abduct',\n",
       " 'abe',\n",
       " 'abed',\n",
       " 'abel',\n",
       " 'aberdeen',\n",
       " 'abide',\n",
       " 'ability',\n",
       " 'abject',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'abominable',\n",
       " 'abort',\n",
       " 'abound',\n",
       " 'abracadabra',\n",
       " 'abraham',\n",
       " 'abroad',\n",
       " 'abrumptly',\n",
       " 'abrupt',\n",
       " 'absconded',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutes',\n",
       " 'absolution',\n",
       " 'absolves',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'abstract',\n",
       " 'abstracted',\n",
       " 'absurd',\n",
       " 'absurdities',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abusive',\n",
       " 'abyss',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'acclaim',\n",
       " 'accompaniment',\n",
       " 'accomplish',\n",
       " 'accomplishment',\n",
       " 'accord',\n",
       " 'accost',\n",
       " 'account',\n",
       " 'accounting',\n",
       " 'accrue',\n",
       " 'accumulate',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accusation',\n",
       " 'accuser',\n",
       " 'ace',\n",
       " 'ach',\n",
       " 'achaia',\n",
       " 'ache',\n",
       " 'achelous',\n",
       " 'achenor',\n",
       " 'acheron',\n",
       " 'achieve',\n",
       " 'achievement',\n",
       " 'achilles',\n",
       " 'aching',\n",
       " 'acid',\n",
       " 'acknowledge',\n",
       " 'acknowledging',\n",
       " 'acorn',\n",
       " 'acorned',\n",
       " 'acquire',\n",
       " 'acquires',\n",
       " 'acre',\n",
       " 'acrid',\n",
       " 'acrobat',\n",
       " 'act',\n",
       " 'actian',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acutally',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'adder',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addictive',\n",
       " 'addition',\n",
       " 'addle',\n",
       " 'address',\n",
       " 'adelade',\n",
       " 'adelaide',\n",
       " 'adele',\n",
       " 'adept',\n",
       " 'adequate',\n",
       " 'adieu',\n",
       " 'adjective',\n",
       " 'adjunct',\n",
       " 'adjust',\n",
       " 'adjusted',\n",
       " 'adjustment',\n",
       " 'admirable',\n",
       " 'admire',\n",
       " 'admirer',\n",
       " 'admit',\n",
       " 'ado',\n",
       " 'adolescence',\n",
       " 'adonis',\n",
       " 'adopt',\n",
       " 'adorable',\n",
       " 'adoration',\n",
       " 'adore',\n",
       " 'adorn',\n",
       " 'adornment',\n",
       " 'adoze',\n",
       " 'adrastus',\n",
       " 'adrenaline',\n",
       " 'adrift',\n",
       " 'adroit',\n",
       " 'adult',\n",
       " 'adulterer',\n",
       " 'adulterous',\n",
       " 'adultery',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'advent',\n",
       " 'adventist',\n",
       " 'adventure',\n",
       " 'adventurer',\n",
       " 'adventurous',\n",
       " 'adversity',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'aegis',\n",
       " 'aemilia',\n",
       " 'aeneas',\n",
       " 'aeonium',\n",
       " 'aerie',\n",
       " 'aero',\n",
       " 'aeroplane',\n",
       " 'aeros',\n",
       " 'aeschylus',\n",
       " 'aesop',\n",
       " 'aether',\n",
       " 'aetna',\n",
       " 'afar',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affectation',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affliction',\n",
       " 'afford',\n",
       " 'afield',\n",
       " 'aflame',\n",
       " 'afloat',\n",
       " 'aflutter',\n",
       " 'afold',\n",
       " 'afoot',\n",
       " 'aforetime',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'aft',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'aftershave',\n",
       " 'afterward',\n",
       " 'agains',\n",
       " 'agape',\n",
       " 'agathas',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agee',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'aggregate',\n",
       " 'aggression',\n",
       " 'agh',\n",
       " 'agile',\n",
       " 'agility',\n",
       " 'agitate',\n",
       " 'ago',\n",
       " 'agon',\n",
       " 'agonizing',\n",
       " 'agony',\n",
       " 'agree',\n",
       " 'agrees',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhhhhhhhhhhhhhhhhhhhhhhh',\n",
       " 'aid',\n",
       " 'aide',\n",
       " 'aiken',\n",
       " 'ail',\n",
       " 'ails',\n",
       " 'aim',\n",
       " 'aimless',\n",
       " 'aimlessly',\n",
       " 'ain',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airless',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airstations',\n",
       " 'airy',\n",
       " 'aisle',\n",
       " 'ajar',\n",
       " 'akashic',\n",
       " 'ala',\n",
       " 'alabaster',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alba',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'albion',\n",
       " 'albir',\n",
       " 'album',\n",
       " 'alchemize',\n",
       " 'alcoflunce',\n",
       " 'alcohol',\n",
       " 'alder',\n",
       " 'ale',\n",
       " 'aleph',\n",
       " 'alert',\n",
       " 'alesha',\n",
       " 'alex',\n",
       " 'alexis',\n",
       " 'alfie',\n",
       " 'algae',\n",
       " 'algebra',\n",
       " 'alice',\n",
       " 'alien',\n",
       " 'aliens',\n",
       " 'alight',\n",
       " 'alignment',\n",
       " 'alike',\n",
       " 'alistair',\n",
       " 'alive',\n",
       " 'alivei',\n",
       " 'allegedly',\n",
       " 'allen',\n",
       " 'alley',\n",
       " 'allied',\n",
       " 'alligator',\n",
       " 'allight',\n",
       " 'alliteration',\n",
       " 'allllllllllllllllllllllllllllllllll',\n",
       " 'allness',\n",
       " 'allot',\n",
       " 'allotments',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allure',\n",
       " 'allurement',\n",
       " 'alluvial',\n",
       " 'ally',\n",
       " 'alma',\n",
       " 'almighty',\n",
       " 'almond',\n",
       " 'aloft',\n",
       " 'alonethis',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'alp',\n",
       " 'alphabet',\n",
       " 'alright',\n",
       " 'alt',\n",
       " 'altar',\n",
       " 'alter',\n",
       " 'alternately',\n",
       " 'alters',\n",
       " 'altitude',\n",
       " 'altogether',\n",
       " 'alway',\n",
       " 'amanda',\n",
       " 'amaryllis',\n",
       " 'amateur',\n",
       " 'amateurs',\n",
       " 'amaze',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'ambassador',\n",
       " 'amber',\n",
       " 'ambered',\n",
       " 'ambience',\n",
       " 'ambient',\n",
       " 'ambition',\n",
       " 'ambitious',\n",
       " 'amble',\n",
       " 'ambrosial',\n",
       " 'ambulance',\n",
       " 'ambush',\n",
       " 'ambushed',\n",
       " 'amen',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amethyst',\n",
       " 'amiable',\n",
       " 'amiably',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'amiss',\n",
       " 'ammoth',\n",
       " 'amor',\n",
       " 'amp',\n",
       " 'amphetamine',\n",
       " 'ample',\n",
       " 'amputate',\n",
       " 'amuse',\n",
       " 'ana',\n",
       " 'anacca',\n",
       " 'anahorish',\n",
       " 'analize',\n",
       " 'analyst',\n",
       " 'analytically',\n",
       " 'ancestral',\n",
       " 'anchor',\n",
       " 'anchored',\n",
       " 'ancient',\n",
       " 'andra',\n",
       " 'andrew',\n",
       " 'andromeda',\n",
       " 'andy',\n",
       " 'aneas',\n",
       " 'anemone',\n",
       " 'anesthetic',\n",
       " 'anew',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'angelic',\n",
       " 'angelico',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angkor',\n",
       " 'angle',\n",
       " 'angler',\n",
       " 'anglo',\n",
       " 'angrily',\n",
       " 'angry',\n",
       " 'angst',\n",
       " 'angstrom',\n",
       " 'anguish',\n",
       " 'animal',\n",
       " 'anjou',\n",
       " 'ankle',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'annalist',\n",
       " 'annals',\n",
       " 'annihilation',\n",
       " 'anniversary',\n",
       " 'annodomini',\n",
       " 'announce',\n",
       " 'annoy',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'anoints',\n",
       " 'anonymity',\n",
       " 'anonymous',\n",
       " 'anorexic',\n",
       " 'anothers',\n",
       " 'answer',\n",
       " 'answering',\n",
       " 'ant',\n",
       " 'antarctica',\n",
       " 'antenna',\n",
       " 'anther',\n",
       " 'anthropology',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'anticipating',\n",
       " 'antidote',\n",
       " 'antimachus',\n",
       " 'antipode',\n",
       " 'antiquated',\n",
       " 'antique',\n",
       " 'antler',\n",
       " 'antlered',\n",
       " 'antony',\n",
       " 'anvil',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyte',\n",
       " 'anytiime',\n",
       " 'anytime',\n",
       " 'apan',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apathetic',\n",
       " 'ape',\n",
       " 'aphid',\n",
       " 'aphrodite',\n",
       " 'apollo',\n",
       " 'apologetic',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'apostrophe',\n",
       " 'appal',\n",
       " 'appall',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'apparition',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appears',\n",
       " 'appendicitis',\n",
       " 'apperhension',\n",
       " 'appertif',\n",
       " 'appetite',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'applesauce',\n",
       " 'appletree',\n",
       " 'appletrees',\n",
       " 'apply',\n",
       " 'appointed',\n",
       " 'appointment',\n",
       " 'appraise',\n",
       " 'appreciate',\n",
       " 'apprehension',\n",
       " 'apprentice',\n",
       " 'apprenticeship',\n",
       " 'appries',\n",
       " 'approach',\n",
       " 'approbation',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'april',\n",
       " 'apron',\n",
       " 'aqeducts',\n",
       " 'aqua',\n",
       " 'aquarium',\n",
       " 'aquarius',\n",
       " 'aquatic',\n",
       " 'aquinas',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabesque',\n",
       " 'arabia',\n",
       " 'arabian',\n",
       " 'arabic',\n",
       " 'aran',\n",
       " 'arbor',\n",
       " 'arc',\n",
       " 'arcadian',\n",
       " 'arcane',\n",
       " 'arch',\n",
       " 'archeaologist',\n",
       " 'archer',\n",
       " 'archerfield',\n",
       " 'archery',\n",
       " 'archetype',\n",
       " 'arctic',\n",
       " 'ardestan',\n",
       " 'ardor',\n",
       " 'ardour',\n",
       " 'arduously',\n",
       " 'area',\n",
       " 'aren',\n",
       " 'arena',\n",
       " 'aretha',\n",
       " 'arghhhhhh',\n",
       " 'argo',\n",
       " 'argue',\n",
       " 'argument',\n",
       " 'argumentative',\n",
       " 'arianna',\n",
       " 'ariel',\n",
       " 'aries',\n",
       " 'arise',\n",
       " 'arissa',\n",
       " 'aristocrat',\n",
       " 'aristotle',\n",
       " 'ark',\n",
       " 'arm',\n",
       " 'armaan',\n",
       " 'armchair',\n",
       " 'armor',\n",
       " 'armour',\n",
       " 'armoured',\n",
       " 'army',\n",
       " 'aroma',\n",
       " 'aromatic',\n",
       " 'arose',\n",
       " 'arouse',\n",
       " 'arousing',\n",
       " 'arraign',\n",
       " 'array',\n",
       " 'arrest',\n",
       " 'arrgh',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrogance',\n",
       " 'arrow',\n",
       " 'arrows',\n",
       " 'art',\n",
       " 'artery',\n",
       " 'artful',\n",
       " 'artifice',\n",
       " 'artificial',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artpiece',\n",
       " 'artwork',\n",
       " 'ascend',\n",
       " 'ascends',\n",
       " 'ascent',\n",
       " 'ascraeus',\n",
       " 'ascription',\n",
       " 'ash',\n",
       " 'ashamed',\n",
       " 'ashen',\n",
       " 'ashore',\n",
       " 'ashplants',\n",
       " 'ashtray',\n",
       " 'ashur',\n",
       " 'asia',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'askew',\n",
       " 'aslant',\n",
       " 'asleep',\n",
       " 'asp',\n",
       " 'aspect',\n",
       " 'aspire',\n",
       " 'assail',\n",
       " 'assailant',\n",
       " 'assailing',\n",
       " 'assassinate',\n",
       " 'assault',\n",
       " 'assemble',\n",
       " 'assembled',\n",
       " 'assert',\n",
       " 'asses',\n",
       " 'asshole',\n",
       " 'assiduous',\n",
       " 'assimilate',\n",
       " 'assistant',\n",
       " 'associate',\n",
       " 'assume',\n",
       " 'assumes',\n",
       " 'assurance',\n",
       " 'assure',\n",
       " 'assuredly',\n",
       " 'asterisk',\n",
       " 'asteroid',\n",
       " 'astonish',\n",
       " 'astonishment',\n",
       " 'astound',\n",
       " 'astounded',\n",
       " 'astray',\n",
       " 'astrology',\n",
       " 'astronaut',\n",
       " 'astronomy',\n",
       " 'asway',\n",
       " 'atalic',\n",
       " 'ate',\n",
       " 'ated',\n",
       " 'atheist',\n",
       " 'athlete',\n",
       " 'athletic',\n",
       " 'atlantic',\n",
       " 'atlas',\n",
       " 'atmosphere',\n",
       " 'atmospheric',\n",
       " 'atom',\n",
       " 'atop',\n",
       " 'atrocity',\n",
       " 'atropos',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attacker',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attendance',\n",
       " 'attendant',\n",
       " 'attention',\n",
       " 'attentive',\n",
       " 'attic',\n",
       " 'attitude',\n",
       " 'attract',\n",
       " 'attractive',\n",
       " 'attrition',\n",
       " 'au',\n",
       " 'auburn',\n",
       " 'auction',\n",
       " 'audi',\n",
       " 'audible',\n",
       " 'audience',\n",
       " 'audition',\n",
       " 'aughty',\n",
       " 'august',\n",
       " 'augustus',\n",
       " 'aum',\n",
       " 'aunt',\n",
       " 'auntie',\n",
       " 'aura',\n",
       " 'aureate',\n",
       " 'aureole',\n",
       " 'auspicious',\n",
       " 'austere',\n",
       " 'australia',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'autism',\n",
       " 'auto',\n",
       " 'automatically',\n",
       " 'automobile',\n",
       " 'autonomy',\n",
       " 'autopsy',\n",
       " 'autumn',\n",
       " 'avail',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'avernus',\n",
       " 'averse',\n",
       " 'avert',\n",
       " 'averted',\n",
       " 'avicii',\n",
       " 'avid',\n",
       " 'avila',\n",
       " 'avocado',\n",
       " 'avoid',\n",
       " 'avon',\n",
       " 'await',\n",
       " 'awaited',\n",
       " 'awake',\n",
       " 'awaken',\n",
       " 'awakeness',\n",
       " 'awakening',\n",
       " 'awakens',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'awaysay',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awkwardly',\n",
       " 'awooooo',\n",
       " 'awry',\n",
       " 'axe',\n",
       " 'ay',\n",
       " 'aye',\n",
       " 'azure',\n",
       " 'baaaaaaa',\n",
       " 'baaing',\n",
       " 'baba',\n",
       " 'babble',\n",
       " 'babbling',\n",
       " 'babe',\n",
       " 'babel',\n",
       " 'baby',\n",
       " 'babylon',\n",
       " 'babylonian',\n",
       " 'bacchanal',\n",
       " 'bacchus',\n",
       " 'bach',\n",
       " 'backache',\n",
       " 'backcloth',\n",
       " 'backed',\n",
       " 'background',\n",
       " 'backs',\n",
       " 'backstreet',\n",
       " 'backstroke',\n",
       " 'backward',\n",
       " 'backwards',\n",
       " 'backwash',\n",
       " 'backyard',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'bade',\n",
       " 'badger',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'baggage',\n",
       " 'bagpipe',\n",
       " 'bagpiper',\n",
       " 'bahamas',\n",
       " 'bail',\n",
       " 'bait',\n",
       " 'baize',\n",
       " 'bake',\n",
       " 'bakeboard',\n",
       " 'baker',\n",
       " 'bakery',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'balancing',\n",
       " 'balascio',\n",
       " 'bald',\n",
       " 'bale',\n",
       " 'balham',\n",
       " 'ball',\n",
       " 'ballad',\n",
       " 'ballerina',\n",
       " 'ballet',\n",
       " 'ballgame',\n",
       " 'balloon',\n",
       " 'ballooner',\n",
       " 'ballyshannon',\n",
       " 'balmyard',\n",
       " 'bamboo',\n",
       " 'ban',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bandage',\n",
       " 'bandstand',\n",
       " 'bane',\n",
       " 'bang',\n",
       " 'bangkok',\n",
       " 'banish',\n",
       " 'banister',\n",
       " 'bank',\n",
       " 'banked',\n",
       " 'bann',\n",
       " 'banner',\n",
       " 'banquet',\n",
       " 'banshee',\n",
       " 'banter',\n",
       " 'bar',\n",
       " 'barack',\n",
       " 'barb',\n",
       " 'barbarism',\n",
       " 'barbarous',\n",
       " 'barbecue',\n",
       " 'barbershop',\n",
       " 'barbitos',\n",
       " 'bard',\n",
       " 'bardic',\n",
       " 'bardot',\n",
       " 'bare',\n",
       " 'bared',\n",
       " 'barefoot',\n",
       " 'bareheaded',\n",
       " 'barely',\n",
       " 'barf',\n",
       " 'bark',\n",
       " 'barker',\n",
       " 'barley',\n",
       " 'barlow',\n",
       " 'barn',\n",
       " 'barnacle',\n",
       " 'barnyard',\n",
       " 'barometer',\n",
       " 'baron',\n",
       " 'barr',\n",
       " 'barrage',\n",
       " 'barrel',\n",
       " 'barreled',\n",
       " 'barren',\n",
       " 'barrier',\n",
       " 'barrow',\n",
       " 'barter',\n",
       " 'basalt',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'baseless',\n",
       " 'basement',\n",
       " 'bash',\n",
       " 'basher',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'basket',\n",
       " 'basketball',\n",
       " 'bass',\n",
       " 'bastard',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'bate',\n",
       " 'bath',\n",
       " 'bather',\n",
       " 'bathing',\n",
       " 'bathroom',\n",
       " 'bathysphere',\n",
       " 'batman',\n",
       " 'baton',\n",
       " 'batten',\n",
       " 'batter',\n",
       " 'battering',\n",
       " 'battery',\n",
       " 'battle',\n",
       " 'battlefield',\n",
       " 'battlement',\n",
       " 'baugh',\n",
       " 'bay',\n",
       " 'bazaar',\n",
       " 'bbqs',\n",
       " 'beach',\n",
       " 'beached',\n",
       " 'beacon',\n",
       " 'bead',\n",
       " 'beaded',\n",
       " 'beady',\n",
       " 'beagle',\n",
       " 'beak',\n",
       " 'beaker',\n",
       " 'beam',\n",
       " 'beaming',\n",
       " 'bean',\n",
       " 'bear',\n",
       " 'beard',\n",
       " 'bearded',\n",
       " 'bearing',\n",
       " 'beast',\n",
       " 'beastly',\n",
       " 'beasty',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beater',\n",
       " 'beatific',\n",
       " 'beatification',\n",
       " 'beating',\n",
       " 'beatles',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'becalm',\n",
       " 'becareful',\n",
       " 'beckett',\n",
       " 'beckon',\n",
       " 'bed',\n",
       " 'bedding',\n",
       " 'beddy',\n",
       " 'bede',\n",
       " 'bedfellow',\n",
       " 'bedl',\n",
       " 'bedlam',\n",
       " 'bedmate',\n",
       " 'bedraggle',\n",
       " 'bedreggled',\n",
       " 'bedroom',\n",
       " 'bedsheets',\n",
       " 'bedside',\n",
       " 'bedspread',\n",
       " 'bedtime',\n",
       " 'bee',\n",
       " 'beef',\n",
       " 'beefcake',\n",
       " 'beelzebub',\n",
       " 'beep',\n",
       " 'beer',\n",
       " 'beerhouse',\n",
       " 'beery',\n",
       " 'beethoven',\n",
       " 'beetle',\n",
       " 'befall',\n",
       " 'befor',\n",
       " 'befriend',\n",
       " 'beg',\n",
       " 'beggar',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'begining',\n",
       " 'beginn',\n",
       " 'beginning',\n",
       " 'begs',\n",
       " 'beguile',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behaviour',\n",
       " 'beheaded',\n",
       " 'behold',\n",
       " 'beholder',\n",
       " 'beiber',\n",
       " 'beige',\n",
       " 'belaud',\n",
       " 'belchin',\n",
       " 'beleafs',\n",
       " 'beleave',\n",
       " 'beleive',\n",
       " 'belief',\n",
       " 'believe',\n",
       " 'believing',\n",
       " 'bell',\n",
       " 'bellerophon',\n",
       " 'bellied',\n",
       " 'bellmetal',\n",
       " 'bellow',\n",
       " 'bellowing',\n",
       " 'belly',\n",
       " 'bellyache',\n",
       " 'belong',\n",
       " 'beloved',\n",
       " 'belovéd',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'benares',\n",
       " 'bench',\n",
       " 'benches',\n",
       " 'bend',\n",
       " 'bending',\n",
       " 'bendy',\n",
       " 'beneath',\n",
       " 'benediction',\n",
       " 'benefit',\n",
       " 'benevolence',\n",
       " 'benign',\n",
       " 'bennett',\n",
       " 'bent',\n",
       " 'bentless',\n",
       " 'bequeath',\n",
       " 'berate',\n",
       " 'bereft',\n",
       " 'berry',\n",
       " 'berth',\n",
       " 'bertold',\n",
       " 'beseech',\n",
       " 'beset',\n",
       " 'besmirch',\n",
       " 'besotted',\n",
       " 'bessie',\n",
       " 'best',\n",
       " 'bestfriend',\n",
       " 'bestow',\n",
       " 'bet',\n",
       " 'beth',\n",
       " 'bethany',\n",
       " 'bethel',\n",
       " 'bethlehem',\n",
       " 'betray',\n",
       " 'betrayal',\n",
       " 'better',\n",
       " 'bettie',\n",
       " 'betting',\n",
       " 'betty',\n",
       " 'betweenie',\n",
       " 'bevvy',\n",
       " 'beware',\n",
       " 'bewilder',\n",
       " 'bewildered',\n",
       " 'bewitch',\n",
       " 'bewitched',\n",
       " 'bex',\n",
       " 'bfas',\n",
       " 'bffs',\n",
       " 'bfg',\n",
       " 'bhasin',\n",
       " 'bias',\n",
       " 'bib',\n",
       " 'bicker',\n",
       " 'bicky',\n",
       " 'bid',\n",
       " 'bidding',\n",
       " 'bide',\n",
       " 'bier',\n",
       " 'bifocals',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggie',\n",
       " 'biggig',\n",
       " 'biggy',\n",
       " 'bike',\n",
       " ...]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = vect.get_feature_names()\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rows represent corpora, columns represent feature words. Therefore, to find tf-idf of each corpus, we need to extract it from the rows one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12683,)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_children = tfidf[0]\n",
    "row_children.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_indeces_child = np.argsort(row_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indeces_child = list(sort_indeces_child)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue',\n",
       " 'sun',\n",
       " 'fun',\n",
       " 'cat',\n",
       " 'night',\n",
       " 'want',\n",
       " 'best',\n",
       " 'sky',\n",
       " 'like',\n",
       " 'play',\n",
       " 'mum',\n",
       " 'time',\n",
       " 'look',\n",
       " 'come',\n",
       " 'say',\n",
       " 'know',\n",
       " 'make',\n",
       " 'day',\n",
       " 'friend',\n",
       " 'love']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_child = []\n",
    "for ind in top_indeces_child:\n",
    "    top_words_child.append(words[ind])\n",
    "top_words_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('children_tf_idf.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(str(top_words_child))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wish',\n",
       " 'try',\n",
       " 'come',\n",
       " 'tell',\n",
       " 'leave',\n",
       " 'look',\n",
       " 'thing',\n",
       " 'eye',\n",
       " 'way',\n",
       " 'time',\n",
       " 'think',\n",
       " 'life',\n",
       " 'heart',\n",
       " 'want',\n",
       " 'make',\n",
       " 'day',\n",
       " 'say',\n",
       " 'feel',\n",
       " 'love',\n",
       " 'know']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_lay = tfidf[1]\n",
    "row_lay.shape\n",
    "sort_indeces_lay = np.argsort(row_lay) \n",
    "\n",
    "top_indeces_lay = list(sort_indeces_lay)[-20:]\n",
    "  \n",
    "top_words_lay = []\n",
    "for ind in top_indeces_lay:\n",
    "    top_words_lay.append(words[ind])\n",
    "top_words_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lay_tf_idf.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(str(top_words_lay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['face',\n",
       " 'life',\n",
       " 'think',\n",
       " 'leave',\n",
       " 'white',\n",
       " 'look',\n",
       " 'hand',\n",
       " 'day',\n",
       " 'light',\n",
       " 'man',\n",
       " 'time',\n",
       " 'old',\n",
       " 'eye',\n",
       " 'just',\n",
       " 'love',\n",
       " 'night',\n",
       " 'make',\n",
       " 'know',\n",
       " 'come',\n",
       " 'say']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_prof = tfidf[2]\n",
    "row_prof.shape\n",
    "sort_indeces_prof = np.argsort(row_prof) \n",
    "\n",
    "top_indeces_prof = list(sort_indeces_prof)[-20:]\n",
    "  \n",
    "top_words_prof = []\n",
    "for ind in top_indeces_prof:\n",
    "    top_words_prof.append(words[ind])\n",
    "top_words_prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prof_tf_idf.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(str(top_words_prof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split the strings into poems\n",
    "2. Vectorize them and find term frequency for each text collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "poems_children = [lemmas_child[i:i+n] for i in range(0, len(lemmas_child), n)]\n",
    "poems_lay = [lemmas_lay[i:i+n] for i in range(0, len(lemmas_lay), n)]\n",
    "poems_prof = [lemmas_prof[i:i+n] for i in range(0, len(lemmas_prof), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 5033)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_child = CountVectorizer(stop_words = 'english')\n",
    "tf_child = count_child.fit_transform(poems_children).toarray()\n",
    "tf_child.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 1056)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_lay = CountVectorizer(min_df=2, max_df=3, stop_words = 'english')\n",
    "tf_lay = count_lay.fit_transform(poems_lay).toarray()\n",
    "tf_lay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 1935)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_prof = CountVectorizer(min_df=2, max_df=3, stop_words = 'english')\n",
    "tf_prof = count_prof.fit_transform(poems_prof).toarray()\n",
    "tf_prof.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avvrik\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "night, like, play, day, know, love, friend\n",
      "topic 1\n",
      "sky, know, make, friend, come, love, day\n",
      "topic 2\n",
      "say, come, know, make, day, friend, love\n",
      "topic 3\n",
      "luck, fast, day, lover, say, love, bad\n",
      "topic 4\n",
      "know, come, day, make, say, friend, love\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "lda_child = LatentDirichletAllocation(n_components=5)\n",
    "lda_child.fit(tf_child)\n",
    "\n",
    "topic_words_child = lda_child.components_\n",
    "topic_words_child.shape\n",
    "count_words_child = count_child.get_feature_names()\n",
    "\n",
    "for topic_ind, topic in enumerate(topic_words_child):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_child = list(topic.argsort())[-7:]\n",
    "    lda_top_words_child = []\n",
    "    for ind in top_indeces_child:\n",
    "        lda_top_words_child.append(count_words_child[ind])\n",
    "    print(', '.join(lda_top_words_child))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avvrik\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "completely, bean, poetess, dye, princess, eternal, prince\n",
      "topic 1\n",
      "okay, painful, warmth, patient, funny, horse, illusion\n",
      "topic 2\n",
      "exactly, blanket, christmas, crystal, fuck, shoe, sneeze\n",
      "topic 3\n",
      "queen, moss, hunt, imagine, discover, feather, garden\n",
      "topic 4\n",
      "misconstrue, pursue, sex, wishing, guitar, spider, depression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "lda_lay = LatentDirichletAllocation(n_components=5)\n",
    "lda_lay.fit(tf_lay)\n",
    "\n",
    "topic_words_lay = lda_lay.components_\n",
    "topic_words_lay.shape\n",
    "count_words_lay = count_lay.get_feature_names()\n",
    "\n",
    "for topic_ind, topic in enumerate(topic_words_lay):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_lay = list(topic.argsort())[-7:]\n",
    "    lda_top_words_lay = []\n",
    "    for ind in top_indeces_lay:\n",
    "        lda_top_words_lay.append(count_words_lay[ind])\n",
    "    print(', '.join(lda_top_words_lay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avvrik\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "mountainside, daisy, realize, perch, giant, unsaid, valley\n",
      "topic 1\n",
      "princess, goal, tidy, thread, miner, childhood, odor\n",
      "topic 2\n",
      "ceiling, aunt, venus, halt, radio, frog, create\n",
      "topic 3\n",
      "coldly, sabbath, warmth, fiesole, scum, nibble, harlem\n",
      "topic 4\n",
      "taught, brave, jim, tomb, nonsense, negro, ram\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "lda_prof = LatentDirichletAllocation(n_components=5)\n",
    "lda_prof.fit(tf_prof)\n",
    "\n",
    "topic_words_prof = lda_prof.components_\n",
    "topic_words_prof.shape\n",
    "count_words_prof = count_prof.get_feature_names()\n",
    "\n",
    "for topic_ind, topic in enumerate(topic_words_prof):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_prof = list(topic.argsort())[-7:]\n",
    "    lda_top_words_prof = []\n",
    "    for ind in top_indeces_prof:\n",
    "        lda_top_words_prof.append(count_words_prof[ind])\n",
    "    print(', '.join(lda_top_words_prof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling with NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "like, play, make, know, day, say\n",
      "topic 1\n",
      "sound, taste, smell, look, summer, beach\n",
      "topic 2\n",
      "tree, blue, love, wish, day, sky\n",
      "topic 3\n",
      "like, play, dad, best, friend, love\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_child = NMF(n_components=4)\n",
    "nmf_child.fit(tf_child)\n",
    "nmf_topic_word_child = nmf_child.components_\n",
    "nmf_topic_word_child.shape\n",
    "for topic_ind, topic in enumerate(nmf_topic_word_child):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_child = list(topic.argsort())[-6:]\n",
    "    top_words_child = []\n",
    "    for ind in top_indeces_child:\n",
    "        top_words_child.append(count_words_child[ind])\n",
    "    print(', '.join(top_words_child))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "drag, hunter, sanity, yo, woo, didnt\n",
      "topic 1\n",
      "roof, beam, farewell, shudder, proof, professional\n",
      "topic 2\n",
      "shes, harp, fighting, daisy, girlfriend, sober\n",
      "topic 3\n",
      "slave, distraught, splendor, sharpen, drone, mr\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_lay = NMF(n_components=4)\n",
    "nmf_lay.fit(tf_lay)\n",
    "nmf_topic_word_lay = nmf_lay.components_\n",
    "nmf_topic_word_lay.shape\n",
    "for topic_ind, topic in enumerate(nmf_topic_word_lay):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_lay = list(topic.argsort())[-6:]\n",
    "    top_words_lay = []\n",
    "    for ind in top_indeces_lay:\n",
    "        top_words_lay.append(count_words_lay[ind])\n",
    "    print(', '.join(top_words_lay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "bough, halt, perch, scum, harlem, nibble\n",
      "topic 1\n",
      "tier, coldly, dragon, aye, warmth, fiesole\n",
      "topic 2\n",
      "nude, princess, thread, miner, childhood, odor\n",
      "topic 3\n",
      "upward, industry, vale, daisy, mountainside, valley\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_prof = NMF(n_components=4)\n",
    "nmf_prof.fit(tf_prof)\n",
    "nmf_topic_word_prof = nmf_prof.components_\n",
    "nmf_topic_word_prof.shape\n",
    "for topic_ind, topic in enumerate(nmf_topic_word_prof):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_prof = list(topic.argsort())[-6:]\n",
    "    top_words_prof = []\n",
    "    for ind in top_indeces_prof:\n",
    "        top_words_prof.append(count_words_prof[ind])\n",
    "    print(', '.join(top_words_prof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
