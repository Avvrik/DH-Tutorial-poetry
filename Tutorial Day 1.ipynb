{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texts_amateur.txt\", 'r', encoding = \"utf-8-sig\") as f:\n",
    "    lay = f.read()\n",
    "with open(\"texts_children.txt\", 'r', encoding = \"utf-8-sig\") as f:\n",
    "    children = f.read()\n",
    "with open(\"texts_professional.txt\", 'r', encoding = \"utf-8-sig\") as f:\n",
    "    prof = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "children_clean = re.sub(\"[.,\\\"?!:;()\\[\\]{}\\-–—]\", \"\", children)\n",
    "lay_clean = re.sub(\"[.,\\\"?!:;()\\[\\]{}\\-–—]\", \"\", lay)\n",
    "prof_clean = re.sub(\"[.,\\\"?!:;()\\[\\]{}\\-–—]\", \"\", prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(poem):\n",
    "    tokens = word_tokenize(poem)\n",
    "    tokens_tags = nltk.pos_tag(tokens)\n",
    "    matching_tags = {'NN':'n', 'VB':'v', 'JJ':'a', 'RB':'r'}\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lemma = []\n",
    "    for (token, tag) in tokens_tags:\n",
    "\n",
    "        if not(token[0].isalpha()):\n",
    "            continue\n",
    "        token = token.lower()\n",
    "        if tag[:2] in matching_tags:\n",
    "            token = lemmatizer.lemmatize(token, pos=matching_tags[tag[:2]])\n",
    "            tokens_lemma.append(token)\n",
    "        \n",
    "    return ' '.join(tokens_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_child = lemmatizer(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_lay = lemmatizer(lay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_prof = lemmatizer(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_children = lemmas_child.split()\n",
    "tokens_lay = lemmas_lay.split()\n",
    "tokens_prof = lemmas_prof.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['me',\n",
       " 'friend',\n",
       " 'hang',\n",
       " 'together',\n",
       " 'go',\n",
       " 'shop',\n",
       " 'get',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'oreo',\n",
       " 'money',\n",
       " 'happy',\n",
       " 'mother',\n",
       " 'day',\n",
       " 'mum',\n",
       " 'love',\n",
       " 'so',\n",
       " 'much',\n",
       " 'miss',\n",
       " 'new',\n",
       " 'zealand',\n",
       " 'be',\n",
       " 'great',\n",
       " 'be',\n",
       " 'really',\n",
       " 'good',\n",
       " 'place',\n",
       " 'be',\n",
       " 're',\n",
       " 'get',\n",
       " 'country',\n",
       " 've',\n",
       " 'get',\n",
       " 'find',\n",
       " 'house',\n",
       " 'get',\n",
       " 'lot',\n",
       " 'poisonous',\n",
       " 'spider',\n",
       " 'want',\n",
       " 'get',\n",
       " 'bite',\n",
       " 'rat',\n",
       " 'be',\n",
       " 'amaze',\n",
       " 't',\n",
       " 'see',\n",
       " 'be',\n",
       " 'as',\n",
       " 'easy',\n",
       " 'see',\n",
       " 'be',\n",
       " 'be',\n",
       " 'love',\n",
       " 'chase',\n",
       " 'bee',\n",
       " 'rat',\n",
       " 'be',\n",
       " 'amaze',\n",
       " 't',\n",
       " 'see',\n",
       " 'rat',\n",
       " 'be',\n",
       " 'as',\n",
       " 'easy',\n",
       " 'a',\n",
       " 'be',\n",
       " 'aspire',\n",
       " 'b',\n",
       " 'be',\n",
       " 'believe',\n",
       " 'c',\n",
       " 'be',\n",
       " 'confidence',\n",
       " 'd',\n",
       " 'be',\n",
       " 'dream',\n",
       " 'e',\n",
       " 'be',\n",
       " 'enjoy',\n",
       " 'f',\n",
       " 'be',\n",
       " 'family',\n",
       " 'g',\n",
       " 'be',\n",
       " 'greatness',\n",
       " 'h',\n",
       " 'be',\n",
       " 'happy',\n",
       " 'be',\n",
       " 'inspire',\n",
       " 'j',\n",
       " 'be',\n",
       " 'joy',\n",
       " 'k',\n",
       " 'be',\n",
       " 'kindness',\n",
       " 'l',\n",
       " 'be',\n",
       " 'love',\n",
       " 'm',\n",
       " 'be',\n",
       " 'mystery',\n",
       " 'n',\n",
       " 'be',\n",
       " 'never',\n",
       " 'o',\n",
       " 'be',\n",
       " 'p',\n",
       " 'be',\n",
       " 'precious',\n",
       " 'q',\n",
       " 'be',\n",
       " 'question',\n",
       " 'r',\n",
       " 'be',\n",
       " 'rely',\n",
       " 's',\n",
       " 'be',\n",
       " 'special',\n",
       " 't',\n",
       " 'be',\n",
       " 'try',\n",
       " 'u',\n",
       " 'be',\n",
       " 'unstoppable',\n",
       " 'v',\n",
       " 'be',\n",
       " 'vacate',\n",
       " 'w',\n",
       " 'be',\n",
       " 'worship',\n",
       " 'x',\n",
       " 'be',\n",
       " 'x-ray',\n",
       " 'y',\n",
       " 'be',\n",
       " 'yolo',\n",
       " 'z',\n",
       " 'be',\n",
       " 'zone',\n",
       " 'drink-sucker',\n",
       " 'enviromental-dislike',\n",
       " 'pointless-plastic',\n",
       " 'ocean-destroyer',\n",
       " 'hi',\n",
       " 'm',\n",
       " 'piece',\n",
       " 'paper',\n",
       " 'write',\n",
       " 'draw',\n",
       " 'turn',\n",
       " 'aeroplane',\n",
       " 'be',\n",
       " 'not',\n",
       " 'too',\n",
       " 'old',\n",
       " 'play',\n",
       " 'dress',\n",
       " 'be',\n",
       " 'not',\n",
       " 'too',\n",
       " 'old',\n",
       " 'have',\n",
       " 'teddy',\n",
       " 'be',\n",
       " 'not',\n",
       " 'too',\n",
       " 'old',\n",
       " 'believe',\n",
       " 'magic',\n",
       " 'be',\n",
       " 'not',\n",
       " 'too',\n",
       " 'old',\n",
       " 'play',\n",
       " 'fairy',\n",
       " 'be',\n",
       " 'not',\n",
       " 'too',\n",
       " 'old',\n",
       " 'like',\n",
       " 'unicorn',\n",
       " 'be',\n",
       " 'not',\n",
       " 'too',\n",
       " 'old',\n",
       " 'chase',\n",
       " 'rainbow',\n",
       " 'be',\n",
       " 'not',\n",
       " 'too',\n",
       " 'old',\n",
       " 'sometimes',\n",
       " 'get',\n",
       " 'discourage',\n",
       " 'm',\n",
       " 'so',\n",
       " 'small',\n",
       " 'always',\n",
       " 'leave',\n",
       " 'fingerprint',\n",
       " 'furniture',\n",
       " 'wall',\n",
       " 'hi',\n",
       " 'be',\n",
       " 'chair',\n",
       " 'sit',\n",
       " 'down',\n",
       " 'rest',\n",
       " 'compare',\n",
       " 'couch',\n",
       " 'be',\n",
       " 'best',\n",
       " 'beep',\n",
       " 'sheep',\n",
       " 'dad',\n",
       " 'be',\n",
       " 'nice',\n",
       " 'sometimes',\n",
       " 'dad',\n",
       " 'be',\n",
       " 'helpful',\n",
       " 'dad',\n",
       " 'help',\n",
       " 're',\n",
       " 'hurt',\n",
       " 'dad',\n",
       " 'don',\n",
       " 't',\n",
       " 'listen',\n",
       " 'dad',\n",
       " 'be',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'cheeky',\n",
       " 'dad',\n",
       " 'don',\n",
       " 't',\n",
       " 'let',\n",
       " 'do',\n",
       " 'anything',\n",
       " 'dad',\n",
       " 'be',\n",
       " 'silly',\n",
       " 'dad',\n",
       " 'just',\n",
       " 'chill',\n",
       " 'dad',\n",
       " 'just',\n",
       " 'let',\n",
       " 'eat',\n",
       " 'healthy',\n",
       " 'stuff',\n",
       " 'ever',\n",
       " 'dad',\n",
       " 'dads',\n",
       " 'sometimes',\n",
       " 'play',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'annoy',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'bossy',\n",
       " 'sister',\n",
       " 'have',\n",
       " 'lot',\n",
       " 'attitude',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'very',\n",
       " 'naughty',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'very',\n",
       " 'hurtful',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'very',\n",
       " 'argumentative',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'nice',\n",
       " 'school',\n",
       " 'death',\n",
       " 'be',\n",
       " 'not',\n",
       " 'end',\n",
       " 'death',\n",
       " 'never',\n",
       " 'be',\n",
       " 'end',\n",
       " 'death',\n",
       " 'be',\n",
       " 'road',\n",
       " 'life',\n",
       " 'be',\n",
       " 'traveler',\n",
       " 'soul',\n",
       " 'be',\n",
       " 'guide',\n",
       " 'solid',\n",
       " 'ice',\n",
       " 'mountain',\n",
       " 'emerge',\n",
       " 'go',\n",
       " 'mountain',\n",
       " 'view',\n",
       " 'way',\n",
       " 'be',\n",
       " 'black',\n",
       " 'white',\n",
       " 'picture',\n",
       " 'friend',\n",
       " 'be',\n",
       " 'forever',\n",
       " 'or',\n",
       " 're',\n",
       " 'not',\n",
       " 'real',\n",
       " 'friend',\n",
       " 'keep',\n",
       " 'secret',\n",
       " 'end',\n",
       " 'giggle',\n",
       " 'chat',\n",
       " 'feel',\n",
       " 'sad',\n",
       " 'cry',\n",
       " 'trust',\n",
       " 'rely',\n",
       " 'grow',\n",
       " 'old',\n",
       " 'together',\n",
       " 'friend',\n",
       " 'life',\n",
       " 'once',\n",
       " 'be',\n",
       " 'cat',\n",
       " 'wander',\n",
       " 'day',\n",
       " 'forest',\n",
       " 'go',\n",
       " 'not',\n",
       " 'aware',\n",
       " 'find',\n",
       " 'day',\n",
       " 'tree',\n",
       " 'climb',\n",
       " 'up',\n",
       " 'high',\n",
       " 'marshy',\n",
       " 'swamp',\n",
       " 'try',\n",
       " 'not',\n",
       " 'get',\n",
       " 'stuck',\n",
       " 'come',\n",
       " 'stop',\n",
       " 'saw',\n",
       " 'unicorn',\n",
       " 'strolling',\n",
       " 'past',\n",
       " 'run',\n",
       " 'paw',\n",
       " 'furry',\n",
       " 'coat',\n",
       " 'unicorn',\n",
       " 'unicorn',\n",
       " 'have',\n",
       " 'trust',\n",
       " 'unicorn',\n",
       " 'have',\n",
       " 'trust',\n",
       " 'cat',\n",
       " 'come',\n",
       " 'wander',\n",
       " 'day',\n",
       " 'wander',\n",
       " 'be',\n",
       " 'into',\n",
       " 'forest',\n",
       " 'day',\n",
       " 'cat',\n",
       " 'unicorn',\n",
       " 'be',\n",
       " 'still',\n",
       " 'friend',\n",
       " 'friend',\n",
       " 'be',\n",
       " 'day',\n",
       " 'hi',\n",
       " 'mark',\n",
       " 'be',\n",
       " 'site',\n",
       " 'people',\n",
       " 'send',\n",
       " 'poem',\n",
       " 'not']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_children[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_words = {elem for elem in tokens_lay if len(elem) < 3} | {elem for elem in tokens_prof if len(elem) < 3} | {elem for elem in tokens_children if len(elem) < 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinies = ['a',\n",
    " 'ah',\n",
    " 'am',\n",
    " 'an',\n",
    " 'as',\n",
    " 'at',\n",
    " 'be',\n",
    " 'do',\n",
    " 'go',\n",
    " 'ha',\n",
    " 'he',\n",
    " 'hi',\n",
    " 'i',\n",
    " 'in',\n",
    " 'is',\n",
    " 'it',\n",
    " 'me',\n",
    " 'my',\n",
    " 'no',\n",
    " 'of',\n",
    " 'oh',\n",
    " 'ok',\n",
    " 'or',\n",
    " 'so',\n",
    " 'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tinies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arr in [tokens_lay, tokens_prof, tokens_children]:\n",
    "    for elem in arr:\n",
    "        if len(elem) < 3 and elem not in tinies:\n",
    "            arr.remove(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'cheeky',\n",
       " 'dad',\n",
       " 'don',\n",
       " 'let',\n",
       " 'do',\n",
       " 'anything',\n",
       " 'dad',\n",
       " 'be',\n",
       " 'silly',\n",
       " 'dad',\n",
       " 'just',\n",
       " 'chill',\n",
       " 'dad',\n",
       " 'just',\n",
       " 'let',\n",
       " 'eat',\n",
       " 'healthy',\n",
       " 'stuff',\n",
       " 'ever',\n",
       " 'dad',\n",
       " 'dads',\n",
       " 'sometimes',\n",
       " 'play',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'annoy',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'bossy',\n",
       " 'sister',\n",
       " 'have',\n",
       " 'lot',\n",
       " 'attitude',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'very',\n",
       " 'naughty',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'very',\n",
       " 'hurtful',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'very',\n",
       " 'argumentative',\n",
       " 'sister',\n",
       " 'be',\n",
       " 'nice',\n",
       " 'school',\n",
       " 'death',\n",
       " 'be',\n",
       " 'not',\n",
       " 'end',\n",
       " 'death',\n",
       " 'never',\n",
       " 'be',\n",
       " 'end',\n",
       " 'death',\n",
       " 'be',\n",
       " 'road',\n",
       " 'life',\n",
       " 'be',\n",
       " 'traveler',\n",
       " 'soul',\n",
       " 'be',\n",
       " 'guide',\n",
       " 'solid',\n",
       " 'ice',\n",
       " 'mountain',\n",
       " 'emerge',\n",
       " 'go',\n",
       " 'mountain',\n",
       " 'view',\n",
       " 'way',\n",
       " 'be',\n",
       " 'black',\n",
       " 'white',\n",
       " 'picture',\n",
       " 'friend',\n",
       " 'be',\n",
       " 'forever',\n",
       " 'or',\n",
       " 'not',\n",
       " 'real',\n",
       " 'friend',\n",
       " 'keep',\n",
       " 'secret',\n",
       " 'end',\n",
       " 'giggle',\n",
       " 'chat',\n",
       " 'feel',\n",
       " 'sad',\n",
       " 'cry',\n",
       " 'trust',\n",
       " 'rely',\n",
       " 'grow',\n",
       " 'old',\n",
       " 'together',\n",
       " 'friend',\n",
       " 'life',\n",
       " 'once',\n",
       " 'be',\n",
       " 'cat',\n",
       " 'wander',\n",
       " 'day',\n",
       " 'forest',\n",
       " 'go',\n",
       " 'not',\n",
       " 'aware',\n",
       " 'find',\n",
       " 'day',\n",
       " 'tree',\n",
       " 'climb',\n",
       " 'high',\n",
       " 'marshy',\n",
       " 'swamp',\n",
       " 'try',\n",
       " 'not',\n",
       " 'get',\n",
       " 'stuck',\n",
       " 'come',\n",
       " 'stop',\n",
       " 'saw',\n",
       " 'unicorn',\n",
       " 'strolling',\n",
       " 'past',\n",
       " 'run',\n",
       " 'paw',\n",
       " 'furry',\n",
       " 'coat',\n",
       " 'unicorn',\n",
       " 'unicorn',\n",
       " 'have',\n",
       " 'trust',\n",
       " 'unicorn',\n",
       " 'have',\n",
       " 'trust',\n",
       " 'cat',\n",
       " 'come',\n",
       " 'wander',\n",
       " 'day',\n",
       " 'wander',\n",
       " 'be',\n",
       " 'into',\n",
       " 'forest',\n",
       " 'day',\n",
       " 'cat',\n",
       " 'unicorn',\n",
       " 'be',\n",
       " 'still',\n",
       " 'friend',\n",
       " 'friend',\n",
       " 'be',\n",
       " 'day',\n",
       " 'hi',\n",
       " 'mark',\n",
       " 'be',\n",
       " 'site',\n",
       " 'people',\n",
       " 'send',\n",
       " 'poem',\n",
       " 'not',\n",
       " 'advertising',\n",
       " 'happy',\n",
       " 'see',\n",
       " 'poem',\n",
       " 'comment',\n",
       " 'mention',\n",
       " 'site',\n",
       " 'have',\n",
       " 'stop',\n",
       " 'sorry…',\n",
       " 'cat',\n",
       " 'meow',\n",
       " 'be',\n",
       " 'strum',\n",
       " 'guitar',\n",
       " 'dog',\n",
       " 'woof',\n",
       " 'be',\n",
       " 'beat',\n",
       " 'drum',\n",
       " 'bird',\n",
       " 'song',\n",
       " 'be',\n",
       " 'tin',\n",
       " 'whistle',\n",
       " 'tune',\n",
       " 'pig',\n",
       " 'grunt',\n",
       " 'be',\n",
       " 'toot',\n",
       " 'kazoo',\n",
       " 'flute',\n",
       " 'melody',\n",
       " 'be',\n",
       " 'graceful',\n",
       " 'swan',\n",
       " 'shake',\n",
       " 'maraca',\n",
       " 'be',\n",
       " 'rattle',\n",
       " 'snake',\n",
       " 'jive',\n",
       " 'day',\n",
       " 'fairyland',\n",
       " 'clock',\n",
       " 'strike',\n",
       " 'lunch',\n",
       " 'time',\n",
       " 'meant',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'have',\n",
       " 'money',\n",
       " 'be',\n",
       " 'dust',\n",
       " 'fairy',\n",
       " 'dust',\n",
       " 'twinkle',\n",
       " 'shone',\n",
       " 'golden',\n",
       " 'twas',\n",
       " 'dust',\n",
       " 'precious',\n",
       " 'gold',\n",
       " 'break',\n",
       " 'plate',\n",
       " 'blame',\n",
       " 'make',\n",
       " 'late',\n",
       " 'blame',\n",
       " 'snap',\n",
       " 'key',\n",
       " 'blame',\n",
       " 'hurt',\n",
       " 'blame',\n",
       " 'be',\n",
       " 'blue',\n",
       " 'mean',\n",
       " 'have',\n",
       " 'flu',\n",
       " 'look',\n",
       " 'have',\n",
       " 'right',\n",
       " 'now',\n",
       " 'everyday',\n",
       " 'school',\n",
       " 'friend',\n",
       " 'play',\n",
       " 'rrrrreeeeaaalllyyyy',\n",
       " 'kind',\n",
       " 'get',\n",
       " 'annoy',\n",
       " 'people',\n",
       " 'shout',\n",
       " 'school',\n",
       " 'time',\n",
       " 'be',\n",
       " 'really',\n",
       " 'annoying',\n",
       " 'mean',\n",
       " 'work',\n",
       " 'aaaaahhhhhh',\n",
       " 'wolf',\n",
       " 'find',\n",
       " 'den',\n",
       " 'provide',\n",
       " 'shelter',\n",
       " 'warmth',\n",
       " 'cub',\n",
       " 'soon',\n",
       " 'be',\n",
       " 'go',\n",
       " 'be',\n",
       " 'bear',\n",
       " 'chicken',\n",
       " 'be',\n",
       " 'fun',\n",
       " 'play',\n",
       " 'fee',\n",
       " 'give',\n",
       " 'drink',\n",
       " 'my',\n",
       " 'name',\n",
       " 'be',\n",
       " 'ciennah',\n",
       " 'age',\n",
       " 'be',\n",
       " 'turn',\n",
       " 'ten',\n",
       " 'see',\n",
       " 'home',\n",
       " 'time',\n",
       " 'go',\n",
       " 'enjoy',\n",
       " 'fun']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_children[200:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('be', 3338), ('have', 593), ('go', 445), ('love', 411), ('do', 381), ('not', 316), ('friend', 282), ('get', 282), ('so', 280), ('day', 268), ('see', 256), ('make', 241), ('know', 215), ('say', 202), ('don', 196), ('come', 193), ('look', 180), ('time', 175), ('play', 169), ('just', 162)]\n",
      "\n",
      "[('be', 2051), ('do', 667), ('have', 478), ('so', 432), (\"n't\", 419), ('know', 417), ('love', 395), ('just', 373), ('not', 365), ('never', 266), ('see', 261), ('go', 257), ('feel', 235), ('i', 225), ('say', 220), ('now', 210), ('day', 201), ('want', 198), ('make', 197), ('heart', 195)]\n",
      "\n",
      "[('be', 1820), ('have', 488), ('not', 328), ('do', 252), ('go', 192), ('say', 190), ('come', 184), ('now', 168), ('make', 162), ('know', 161), ('love', 151), ('so', 151), ('night', 147), ('eye', 126), ('then', 125), ('see', 122), ('old', 118), ('time', 114), ('man', 110), ('still', 110)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freq_children = Counter(tokens_children)\n",
    "frequency_children = freq_children.most_common(20)\n",
    "print(frequency_children)\n",
    "print()\n",
    "freq_lay = Counter(tokens_lay)\n",
    "frequency_lay = freq_lay.most_common(20)\n",
    "print(frequency_lay)\n",
    "print()\n",
    "freq_prof = Counter(tokens_prof)\n",
    "frequency_prof = freq_prof.most_common(20)\n",
    "print(frequency_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Lay_frequency.tsv', 'w') as f:\n",
    "    f.write('Word\\tFrequency\\n')\n",
    "    for word,fr in frequency_lay:\n",
    "            f.write(word + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Children_frequency.tsv', 'w') as f:\n",
    "    f.write('Word\\tFrequency\\n')\n",
    "    for word,fr in frequency_children:\n",
    "            f.write(word + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Prof_frequency.tsv', 'w') as f:\n",
    "    f.write('Word\\tFrequency\\n')\n",
    "    for word,fr in frequency_prof:\n",
    "            f.write(word + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams_children = []\n",
    "    \n",
    "for i in range(len(tokens_children)-n+1):\n",
    "    bigrams_children.append(tokens_children[i:i+n])\n",
    "    \n",
    "ngrams_children = [' '.join(tokens_children[i:i+n]) for i in range(len(tokens_children)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams_lay = []\n",
    "    \n",
    "for i in range(len(tokens_lay)-n+1):\n",
    "    bigrams_lay.append(tokens_lay[i:i+n])\n",
    "    \n",
    "ngrams_lay = [' '.join(tokens_lay[i:i+n]) for i in range(len(tokens_lay)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams_prof = []\n",
    "    \n",
    "for i in range(len(tokens_prof)-n+1):\n",
    "    bigrams_prof.append(tokens_prof[i:i+n])\n",
    "    \n",
    "ngrams_prof = [' '.join(tokens_prof[i:i+n]) for i in range(len(tokens_prof)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('friend be', 94),\n",
       " ('be best', 83),\n",
       " ('be not', 74),\n",
       " ('do not', 59),\n",
       " ('be be', 56),\n",
       " ('be so', 56),\n",
       " ('be very', 55),\n",
       " ('best friend', 48),\n",
       " ('be great', 44),\n",
       " ('be fun', 42),\n",
       " ('be as', 40),\n",
       " ('love be', 40),\n",
       " ('want be', 39),\n",
       " ('be good', 38),\n",
       " ('be go', 36)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq = Counter(ngrams_children)\n",
    "freq_bigram_children = bigram_freq.most_common(15)\n",
    "freq_bigram_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"do n't\", 252),\n",
       " (\"n't know\", 88),\n",
       " ('be so', 65),\n",
       " ('so much', 57),\n",
       " ('be not', 56),\n",
       " ('love be', 50),\n",
       " ('know be', 47),\n",
       " ('life be', 44),\n",
       " ('be just', 42),\n",
       " ('be be', 42),\n",
       " ('have be', 38),\n",
       " ('know do', 36),\n",
       " ('be there', 35),\n",
       " (\"be n't\", 33),\n",
       " ('heart be', 33)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq_lay = Counter(ngrams_lay)\n",
    "freq_bigram_lay = bigram_freq_lay.most_common(15)\n",
    "freq_bigram_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be not', 59),\n",
       " (\"do n't\", 53),\n",
       " ('have be', 50),\n",
       " ('do not', 43),\n",
       " ('love be', 23),\n",
       " ('be be', 22),\n",
       " ('say be', 21),\n",
       " ('be go', 19),\n",
       " ('life be', 19),\n",
       " ('not be', 18),\n",
       " ('be so', 18),\n",
       " ('be make', 17),\n",
       " ('here be', 17),\n",
       " ('now be', 16),\n",
       " ('too much', 16)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq_prof = Counter(ngrams_prof)\n",
    "freq_bigram_prof = bigram_freq_prof.most_common(15)\n",
    "freq_bigram_prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 15181, 'JJ': 8368, 'VB': 5449, 'RB': 3848, 'VBP': 2834, 'VBN': 1254, 'NNS': 749, 'IN': 573, 'VBD': 515, 'VBG': 432, 'DT': 218, 'JJS': 194, 'PRP$': 167, 'JJR': 147, 'VBZ': 127, 'CD': 85, 'RBR': 66, 'RP': 62, 'WP': 60, 'MD': 52, 'PRP': 44, 'EX': 36, 'WDT': 30, 'RBS': 30, 'WRB': 28, 'CC': 25, 'NNP': 25, 'FW': 19, 'UH': 15, 'TO': 3, 'PDT': 2, 'WP$': 2})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CC': 0.0006151574803149606,\n",
       " 'CD': 0.0020915354330708663,\n",
       " 'DT': 0.005364173228346457,\n",
       " 'EX': 0.0008858267716535433,\n",
       " 'FW': 0.00046751968503937007,\n",
       " 'IN': 0.014099409448818897,\n",
       " 'JJ': 0.20590551181102362,\n",
       " 'JJR': 0.0036171259842519687,\n",
       " 'JJS': 0.004773622047244095,\n",
       " 'MD': 0.001279527559055118,\n",
       " 'NN': 0.3735482283464567,\n",
       " 'NNP': 0.0006151574803149606,\n",
       " 'NNS': 0.01843011811023622,\n",
       " 'PDT': 4.921259842519685e-05,\n",
       " 'PRP': 0.0010826771653543307,\n",
       " 'PRP$': 0.004109251968503937,\n",
       " 'RB': 0.09468503937007874,\n",
       " 'RBR': 0.001624015748031496,\n",
       " 'RBS': 0.0007381889763779527,\n",
       " 'RP': 0.0015255905511811023,\n",
       " 'TO': 7.381889763779527e-05,\n",
       " 'UH': 0.00036909448818897637,\n",
       " 'VB': 0.13407972440944882,\n",
       " 'VBD': 0.012672244094488189,\n",
       " 'VBG': 0.01062992125984252,\n",
       " 'VBN': 0.030856299212598425,\n",
       " 'VBP': 0.06973425196850394,\n",
       " 'VBZ': 0.003125,\n",
       " 'WDT': 0.0007381889763779527,\n",
       " 'WP': 0.0014763779527559055,\n",
       " 'WP$': 4.921259842519685e-05,\n",
       " 'WRB': 0.0006889763779527559}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_children = nltk.pos_tag(tokens_children)\n",
    "    \n",
    "from collections import Counter\n",
    "counts_children = Counter(tag for word,tag in pos_tags_children)\n",
    "print(counts_children)\n",
    "total = sum(counts_children.values())\n",
    "dict((word, float(n)/total) for word,n in counts_children.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 11388, 'JJ': 6311, 'RB': 4890, 'VB': 4779, 'VBP': 2926, 'VBN': 891, 'NNS': 457, 'VBD': 440, 'IN': 377, 'VBG': 349, 'PRP$': 201, 'DT': 162, 'JJR': 142, 'VBZ': 134, 'CD': 108, 'RBR': 99, 'RP': 79, 'JJS': 60, 'MD': 58, 'PRP': 57, 'CC': 36, 'EX': 32, 'WRB': 30, 'RBS': 24, 'WDT': 20, 'FW': 19, 'WP': 17, 'NNP': 14, 'TO': 9, 'UH': 3, 'PDT': 3, 'WP$': 2})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CC': 0.001055192426063253,\n",
       " 'CD': 0.0031655772781897587,\n",
       " 'DT': 0.004748365917284638,\n",
       " 'EX': 0.0009379488231673359,\n",
       " 'FW': 0.0005569071137556057,\n",
       " 'IN': 0.011050209572940176,\n",
       " 'JJ': 0.18498109446903305,\n",
       " 'JJR': 0.0041621479028050535,\n",
       " 'JJS': 0.0017586540434387549,\n",
       " 'MD': 0.0017000322419907963,\n",
       " 'NN': 0.3337925374446757,\n",
       " 'NNP': 0.0004103526101357095,\n",
       " 'NNS': 0.013395081630858516,\n",
       " 'PDT': 8.793270217193774e-05,\n",
       " 'PRP': 0.0016707213412668171,\n",
       " 'PRP$': 0.005891491045519829,\n",
       " 'RB': 0.14333030454025852,\n",
       " 'RBR': 0.0029017791716739456,\n",
       " 'RBS': 0.0007034616173755019,\n",
       " 'RP': 0.0023155611571943607,\n",
       " 'TO': 0.00026379810651581324,\n",
       " 'UH': 8.793270217193774e-05,\n",
       " 'VB': 0.14007679455989683,\n",
       " 'VBD': 0.012896796318550868,\n",
       " 'VBG': 0.010229504352668757,\n",
       " 'VBN': 0.02611601254506551,\n",
       " 'VBP': 0.08576369551836328,\n",
       " 'VBZ': 0.003927660697013219,\n",
       " 'WDT': 0.000586218014479585,\n",
       " 'WP': 0.0004982853123076472,\n",
       " 'WP$': 5.8621801447958495e-05,\n",
       " 'WRB': 0.0008793270217193774}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_lay = nltk.pos_tag(tokens_lay)\n",
    "    \n",
    "from collections import Counter\n",
    "counts_lay = Counter(tag for word,tag in pos_tags_lay)\n",
    "print(counts_lay)\n",
    "total = sum(counts_lay.values())\n",
    "dict((word, float(n)/total) for word,n in counts_lay.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 16786, 'JJ': 8822, 'RB': 3467, 'VB': 3270, 'VBP': 2448, 'VBN': 1090, 'VBD': 861, 'NNS': 668, 'IN': 497, 'VBG': 490, 'VBZ': 218, 'JJR': 132, 'RBR': 105, 'DT': 101, 'MD': 73, 'RP': 72, 'CD': 71, 'JJS': 66, 'PRP$': 63, 'CC': 48, 'WP': 36, 'WRB': 32, 'WDT': 29, 'EX': 29, 'NNP': 27, 'PRP': 24, 'FW': 18, 'RBS': 17, 'WP$': 12, 'PDT': 4, 'UH': 2})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CC': 0.0012127949871140534,\n",
       " 'CD': 0.0017939259184395371,\n",
       " 'DT': 0.0025519227853858205,\n",
       " 'EX': 0.0007327303047147405,\n",
       " 'FW': 0.00045479812016776996,\n",
       " 'IN': 0.012557481429076759,\n",
       " 'JJ': 0.22290161200667039,\n",
       " 'JJR': 0.0033351862145636463,\n",
       " 'JJS': 0.0016675931072818232,\n",
       " 'MD': 0.0018444590429026228,\n",
       " 'NN': 0.42412451361867703,\n",
       " 'NNP': 0.0006821971802516549,\n",
       " 'NNS': 0.016878063570670575,\n",
       " 'PDT': 0.00010106624892617111,\n",
       " 'PRP': 0.0006063974935570267,\n",
       " 'PRP$': 0.0015917934205871948,\n",
       " 'RB': 0.08759917125675881,\n",
       " 'RBR': 0.0026529890343119913,\n",
       " 'RBS': 0.0004295315579362272,\n",
       " 'RP': 0.0018191924806710798,\n",
       " 'UH': 5.0533124463085555e-05,\n",
       " 'VB': 0.08262165849714488,\n",
       " 'VBD': 0.02175451008135833,\n",
       " 'VBG': 0.012380615493455961,\n",
       " 'VBN': 0.027540552832381625,\n",
       " 'VBP': 0.061852544342816715,\n",
       " 'VBZ': 0.005508110566476325,\n",
       " 'WDT': 0.0007327303047147405,\n",
       " 'WP': 0.0009095962403355399,\n",
       " 'WP$': 0.00030319874677851334,\n",
       " 'WRB': 0.0008085299914093689}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_prof = nltk.pos_tag(tokens_prof)\n",
    "    \n",
    "from collections import Counter\n",
    "counts_prof = Counter(tag for word,tag in pos_tags_prof)\n",
    "print(counts_prof)\n",
    "total = sum(counts_prof.values())\n",
    "dict((word, float(n)/total) for word,n in counts_prof.items())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
