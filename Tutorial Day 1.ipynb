{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import text collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texts_amateur.txt\", 'r', encoding = \"utf-8-sig\") as f:\n",
    "    lay = f.read()\n",
    "with open(\"texts_children.txt\", 'r', encoding = \"utf-8-sig\") as f:\n",
    "    children = f.read()\n",
    "with open(\"texts_professional.txt\", 'r', encoding = \"utf-8-sig\") as f:\n",
    "    prof = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean from punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "children_clean = re.sub(\"[.,\\\"?!:;()\\[\\]{}\\-–—]\", \"\", children)\n",
    "lay_clean = re.sub(\"[.,\\\"?!:;()\\[\\]{}\\-–—]\", \"\", lay)\n",
    "prof_clean = re.sub(\"[.,\\\"?!:;()\\[\\]{}\\-–—]\", \"\", prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(poem):\n",
    "    tokens = word_tokenize(poem)\n",
    "    tokens_tags = nltk.pos_tag(tokens)\n",
    "    matching_tags = {'NN':'n', 'VB':'v', 'JJ':'a', 'RB':'r'}\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lemma = []\n",
    "    for (token, tag) in tokens_tags:\n",
    "\n",
    "        if not(token[0].isalpha()):\n",
    "            continue\n",
    "        token = token.lower()\n",
    "        if tag[:2] in matching_tags:\n",
    "            token = lemmatizer.lemmatize(token, pos=matching_tags[tag[:2]])\n",
    "            tokens_lemma.append(token)\n",
    "        \n",
    "    return ' '.join(tokens_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_child = lemmatizer(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_lay = lemmatizer(lay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_prof = lemmatizer(prof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_children = lemmas_child.split()\n",
    "tokens_lay = lemmas_lay.split()\n",
    "tokens_prof = lemmas_prof.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_words = {elem for elem in tokens_lay if len(elem) < 3} | {elem for elem in tokens_prof if len(elem) < 3} | {elem for elem in tokens_children if len(elem) < 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinies = ['a',\n",
    " 'ah',\n",
    " 'am',\n",
    " 'an',\n",
    " 'as',\n",
    " 'at',\n",
    " 'be',\n",
    " 'do',\n",
    " 'go',\n",
    " 'ha',\n",
    " 'he',\n",
    " 'hi',\n",
    " 'i',\n",
    " 'in',\n",
    " 'is',\n",
    " 'it',\n",
    " 'me',\n",
    " 'my',\n",
    " 'no',\n",
    " 'of',\n",
    " 'oh',\n",
    " 'ok',\n",
    " 'or',\n",
    " 'so',\n",
    " 'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tinies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arr in [tokens_lay, tokens_prof, tokens_children]:\n",
    "    for elem in arr:\n",
    "        if len(elem) < 3 and elem not in tinies:\n",
    "            arr.remove(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('be', 3325), ('have', 593), ('go', 445), ('love', 411), ('do', 380), ('not', 314), ('friend', 282), ('get', 282), ('so', 280), ('day', 267), ('see', 256), ('make', 241), ('know', 215), ('say', 202), ('don', 196), ('come', 193), ('look', 180), ('time', 173), ('play', 169), ('just', 162)]\n",
      "\n",
      "[('be', 2051), ('do', 667), ('have', 478), ('so', 432), (\"n't\", 419), ('know', 417), ('love', 395), ('just', 373), ('not', 365), ('never', 266), ('see', 261), ('go', 257), ('feel', 235), ('i', 225), ('say', 220), ('now', 210), ('day', 201), ('want', 198), ('make', 197), ('heart', 195)]\n",
      "\n",
      "[('be', 1820), ('have', 488), ('not', 328), ('do', 252), ('go', 192), ('say', 190), ('come', 184), ('now', 168), ('make', 162), ('know', 161), ('love', 151), ('so', 151), ('night', 147), ('eye', 126), ('then', 125), ('see', 122), ('old', 118), ('time', 114), ('man', 110), ('still', 110)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freq_children = Counter(tokens_children)\n",
    "frequency_children = freq_children.most_common(20)\n",
    "print(frequency_children)\n",
    "print()\n",
    "freq_lay = Counter(tokens_lay)\n",
    "frequency_lay = freq_lay.most_common(20)\n",
    "print(frequency_lay)\n",
    "print()\n",
    "freq_prof = Counter(tokens_prof)\n",
    "frequency_prof = freq_prof.most_common(20)\n",
    "print(frequency_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Lay_frequency.tsv', 'w') as f:\n",
    "    f.write('Word\\tFrequency\\n')\n",
    "    for word,fr in frequency_lay:\n",
    "            f.write(word + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Children_frequency.tsv', 'w') as f:\n",
    "    f.write('Word\\tFrequency\\n')\n",
    "    for word,fr in frequency_children:\n",
    "            f.write(word + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Prof_frequency.tsv', 'w') as f:\n",
    "    f.write('Word\\tFrequency\\n')\n",
    "    for word,fr in frequency_prof:\n",
    "            f.write(word + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most frequent bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams_children = []\n",
    "    \n",
    "for i in range(len(tokens_children)-n+1):\n",
    "    bigrams_children.append(tokens_children[i:i+n])\n",
    "    \n",
    "ngrams_children = [' '.join(tokens_children[i:i+n]) for i in range(len(tokens_children)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams_lay = []\n",
    "    \n",
    "for i in range(len(tokens_lay)-n+1):\n",
    "    bigrams_lay.append(tokens_lay[i:i+n])\n",
    "    \n",
    "ngrams_lay = [' '.join(tokens_lay[i:i+n]) for i in range(len(tokens_lay)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "bigrams_prof = []\n",
    "    \n",
    "for i in range(len(tokens_prof)-n+1):\n",
    "    bigrams_prof.append(tokens_prof[i:i+n])\n",
    "    \n",
    "ngrams_prof = [' '.join(tokens_prof[i:i+n]) for i in range(len(tokens_prof)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('friend be', 94),\n",
       " ('be best', 83),\n",
       " ('be not', 73),\n",
       " ('do not', 58),\n",
       " ('be be', 56),\n",
       " ('be so', 56),\n",
       " ('be very', 55),\n",
       " ('best friend', 48),\n",
       " ('be great', 44),\n",
       " ('be fun', 42),\n",
       " ('be as', 40),\n",
       " ('love be', 40),\n",
       " ('want be', 39),\n",
       " ('be good', 38),\n",
       " ('be go', 36)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq = Counter(ngrams_children)\n",
    "freq_bigram_children = bigram_freq.most_common(15)\n",
    "freq_bigram_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Bigram_frequency_child.tsv', 'w') as f:\n",
    "    f.write('Bigram\\tFrequency\\n')\n",
    "    for bigr,fr in freq_bigram_children:\n",
    "            f.write(bigr + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"do n't\", 252),\n",
       " (\"n't know\", 88),\n",
       " ('be so', 65),\n",
       " ('so much', 57),\n",
       " ('be not', 56),\n",
       " ('love be', 50),\n",
       " ('know be', 47),\n",
       " ('life be', 44),\n",
       " ('be just', 42),\n",
       " ('be be', 42),\n",
       " ('have be', 38),\n",
       " ('know do', 36),\n",
       " ('be there', 35),\n",
       " (\"be n't\", 33),\n",
       " ('heart be', 33)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq_lay = Counter(ngrams_lay)\n",
    "freq_bigram_lay = bigram_freq_lay.most_common(15)\n",
    "freq_bigram_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Bigram_frequency_lay.tsv', 'w') as f:\n",
    "    f.write('Bigram\\tFrequency\\n')\n",
    "    for bigr,fr in freq_bigram_lay:\n",
    "            f.write(bigr + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be not', 59),\n",
       " (\"do n't\", 53),\n",
       " ('have be', 50),\n",
       " ('do not', 43),\n",
       " ('love be', 23),\n",
       " ('be be', 22),\n",
       " ('say be', 21),\n",
       " ('be go', 19),\n",
       " ('life be', 19),\n",
       " ('not be', 18),\n",
       " ('be so', 18),\n",
       " ('be make', 17),\n",
       " ('here be', 17),\n",
       " ('now be', 16),\n",
       " ('too much', 16)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_freq_prof = Counter(ngrams_prof)\n",
    "freq_bigram_prof = bigram_freq_prof.most_common(15)\n",
    "freq_bigram_prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Bigram_frequency_prof.tsv', 'w') as f:\n",
    "    f.write('Bigram\\tFrequency\\n')\n",
    "    for bigr,fr in freq_bigram_prof:\n",
    "            f.write(bigr + '\\t' + str(fr) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS (part of speech) - tagging. \n",
    "Tag the words in each corpus, count POS tags to compare the corpora with regard to part of speech frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove stopwords using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Avvrik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_children_clean = [word for word in tokens_children if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lay_clean = [word for word in tokens_lay if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_prof_clean = [word for word in tokens_prof if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tag the words with NLTK and count the tags, then calculate percentage of each part of speech in the text collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 16507, 'JJ': 7420, 'VBP': 2679, 'RB': 2187, 'VB': 1456, 'NNS': 618, 'VBD': 597, 'IN': 478, 'VBG': 441, 'VBN': 245, 'JJS': 177, 'VBZ': 114, 'CD': 90, 'JJR': 80, 'NNP': 40, 'MD': 36, 'RP': 35, 'RBS': 29, 'RBR': 29, 'DT': 26, 'WP': 25, 'FW': 23, 'UH': 18, 'CC': 11, 'WRB': 9, 'PRP': 5, 'WDT': 2, 'WP$': 2, 'PRP$': 2})\n",
      "33381\n"
     ]
    }
   ],
   "source": [
    "pos_tags_children = nltk.pos_tag(tokens_children_clean)\n",
    "    \n",
    "from collections import Counter\n",
    "counts_children = Counter(tag for word,tag in pos_tags_children)\n",
    "print(counts_children)\n",
    "total = sum(counts_children.values())\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 12073, 'JJ': 5709, 'RB': 2745, 'VBP': 2630, 'VB': 1972, 'VBD': 558, 'NNS': 391, 'VBG': 361, 'IN': 303, 'VBN': 263, 'CD': 116, 'VBZ': 90, 'JJR': 72, 'JJS': 54, 'RP': 40, 'RBR': 38, 'MD': 32, 'DT': 23, 'FW': 22, 'NNP': 14, 'RBS': 8, 'CC': 8, 'WP': 6, 'PRP': 6, 'WDT': 4, 'UH': 2, 'WRB': 2, 'WP$': 2})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CC': 0.0002904443799012489,\n",
       " 'CD': 0.004211443508568109,\n",
       " 'DT': 0.0008350275922160906,\n",
       " 'FW': 0.0007987220447284345,\n",
       " 'IN': 0.011000580888759803,\n",
       " 'JJ': 0.20726837060702874,\n",
       " 'JJR': 0.00261399941911124,\n",
       " 'JJS': 0.0019604995643334303,\n",
       " 'MD': 0.0011617775196049957,\n",
       " 'NN': 0.43831687481847226,\n",
       " 'NNP': 0.0005082776648271856,\n",
       " 'NNS': 0.014195469067673541,\n",
       " 'PRP': 0.00021783328492593667,\n",
       " 'RB': 0.09965872785361603,\n",
       " 'RBR': 0.0013796108045309324,\n",
       " 'RBS': 0.0002904443799012489,\n",
       " 'RP': 0.0014522218995062445,\n",
       " 'UH': 7.261109497531223e-05,\n",
       " 'VB': 0.07159453964565786,\n",
       " 'VBD': 0.020258495498112112,\n",
       " 'VBG': 0.013106302643043856,\n",
       " 'VBN': 0.009548358989253557,\n",
       " 'VBP': 0.09548358989253558,\n",
       " 'VBZ': 0.0032674992738890504,\n",
       " 'WDT': 0.00014522218995062446,\n",
       " 'WP': 0.00021783328492593667,\n",
       " 'WP$': 7.261109497531223e-05,\n",
       " 'WRB': 7.261109497531223e-05}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_lay = nltk.pos_tag(tokens_lay_clean)\n",
    "    \n",
    "from collections import Counter\n",
    "counts_lay = Counter(tag for word,tag in pos_tags_lay)\n",
    "print(counts_lay)\n",
    "total = sum(counts_lay.values())\n",
    "dict((word, float(n)/total) for word,n in counts_lay.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 17796, 'JJ': 8160, 'VBP': 2205, 'RB': 2031, 'VB': 1064, 'VBD': 979, 'NNS': 601, 'VBG': 500, 'IN': 434, 'VBN': 348, 'VBZ': 169, 'JJR': 97, 'CD': 69, 'JJS': 67, 'RBR': 58, 'MD': 48, 'NNP': 30, 'RP': 29, 'FW': 26, 'WP': 18, 'CC': 18, 'WP$': 12, 'DT': 11, 'PRP': 6, 'RBS': 5, 'WDT': 4, 'UH': 3, 'WRB': 3, 'PDT': 2, 'PRP$': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CC': 0.0005173305742369374,\n",
       " 'CD': 0.0019831005345749267,\n",
       " 'DT': 0.000316146462033684,\n",
       " 'FW': 0.0007472552738977985,\n",
       " 'IN': 0.012473414956601713,\n",
       " 'JJ': 0.23452319365407828,\n",
       " 'JJR': 0.0027878369833879407,\n",
       " 'JJS': 0.0019256193596597115,\n",
       " 'MD': 0.0013795481979651664,\n",
       " 'NN': 0.5114674943955855,\n",
       " 'NNP': 0.000862217623728229,\n",
       " 'NNS': 0.01727309306202219,\n",
       " 'PDT': 5.748117491521527e-05,\n",
       " 'PRP': 0.0001724435247456458,\n",
       " 'PRP$': 2.8740587457607634e-05,\n",
       " 'RB': 0.058372133126401105,\n",
       " 'RBR': 0.0016669540725412428,\n",
       " 'RBS': 0.00014370293728803817,\n",
       " 'RP': 0.0008334770362706214,\n",
       " 'UH': 8.62217623728229e-05,\n",
       " 'VB': 0.030579985054894523,\n",
       " 'VBD': 0.028137035120997873,\n",
       " 'VBG': 0.014370293728803817,\n",
       " 'VBN': 0.010001724435247456,\n",
       " 'VBP': 0.06337299534402484,\n",
       " 'VBZ': 0.00485715928033569,\n",
       " 'WDT': 0.00011496234983043054,\n",
       " 'WP': 0.0005173305742369374,\n",
       " 'WP$': 0.0003448870494912916,\n",
       " 'WRB': 8.62217623728229e-05}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags_prof = nltk.pos_tag(tokens_prof_clean)\n",
    "    \n",
    "from collections import Counter\n",
    "counts_prof = Counter(tag for word,tag in pos_tags_prof)\n",
    "print(counts_prof)\n",
    "total = sum(counts_prof.values())\n",
    "dict((word, float(n)/total) for word,n in counts_prof.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf of each corpus with regard to other two corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_texts = [lemmas_child, lemmas_lay, lemmas_prof]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 12740)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(min_df=1, max_df=3, stop_words = 'english')\n",
    "tf = count.fit_transform(three_texts).toarray()\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 12740)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df=1, max_df=3, stop_words = 'english')\n",
    "tfidf = vect.fit_transform(three_texts).toarray()\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'friend': 4386,\n",
       " 'hang': 5017,\n",
       " 'shop': 9831,\n",
       " 'ice': 5490,\n",
       " 'cream': 2459,\n",
       " 'oreo': 7676,\n",
       " 'money': 7090,\n",
       " 'happy': 5031,\n",
       " 'mother': 7151,\n",
       " 'day': 2718,\n",
       " 'mum': 7225,\n",
       " 'love': 6553,\n",
       " 'miss': 7028,\n",
       " 'new': 7378,\n",
       " 'zealand': 12714,\n",
       " 'great': 4822,\n",
       " 'really': 8844,\n",
       " 'good': 4720,\n",
       " 'place': 8191,\n",
       " 'country': 2396,\n",
       " 've': 12061,\n",
       " 'house': 5402,\n",
       " 'lot': 6541,\n",
       " 'poisonous': 8292,\n",
       " 'spider': 10371,\n",
       " 'want': 12243,\n",
       " 'bite': 1030,\n",
       " 'rat': 8809,\n",
       " 'amaze': 313,\n",
       " 'easy': 3409,\n",
       " 'chase': 1773,\n",
       " 'bee': 885,\n",
       " 'aspire': 567,\n",
       " 'believe': 921,\n",
       " 'confidence': 2221,\n",
       " 'dream': 3262,\n",
       " 'enjoy': 3602,\n",
       " 'family': 3900,\n",
       " 'greatness': 4824,\n",
       " 'inspire': 5742,\n",
       " 'joy': 5983,\n",
       " 'kindness': 6083,\n",
       " 'mystery': 7269,\n",
       " 'precious': 8434,\n",
       " 'question': 8698,\n",
       " 'rely': 8987,\n",
       " 'special': 10342,\n",
       " 'try': 11619,\n",
       " 'unstoppable': 11938,\n",
       " 'vacate': 12018,\n",
       " 'worship': 12593,\n",
       " 'ray': 8822,\n",
       " 'yolo': 12687,\n",
       " 'zone': 12733,\n",
       " 'drink': 3287,\n",
       " 'sucker': 10766,\n",
       " 'enviromental': 3641,\n",
       " 'dislike': 3094,\n",
       " 'pointless': 8288,\n",
       " 'plastic': 8210,\n",
       " 'ocean': 7579,\n",
       " 'destroyer': 2931,\n",
       " 'hi': 5234,\n",
       " 'piece': 8128,\n",
       " 'paper': 7845,\n",
       " 'write': 12628,\n",
       " 'draw': 3254,\n",
       " 'turn': 11654,\n",
       " 'aeroplane': 168,\n",
       " 'old': 7612,\n",
       " 'play': 8218,\n",
       " 'dress': 3277,\n",
       " 'teddy': 11100,\n",
       " 'magic': 6651,\n",
       " 'fairy': 3879,\n",
       " 'like': 6392,\n",
       " 'unicorn': 11855,\n",
       " 'rainbow': 8760,\n",
       " 'discourage': 3062,\n",
       " 'small': 10113,\n",
       " 'leave': 6288,\n",
       " 'fingerprint': 4060,\n",
       " 'furniture': 4457,\n",
       " 'wall': 12225,\n",
       " 'chair': 1737,\n",
       " 'sit': 9970,\n",
       " 'rest': 9075,\n",
       " 'compare': 2148,\n",
       " 'couch': 2382,\n",
       " 'best': 961,\n",
       " 'beep': 889,\n",
       " 'sheep': 9780,\n",
       " 'dad': 2650,\n",
       " 'nice': 7390,\n",
       " 'helpful': 5197,\n",
       " 'help': 5195,\n",
       " 'hurt': 5459,\n",
       " 'don': 3176,\n",
       " 'listen': 6445,\n",
       " 'little': 6457,\n",
       " 'bit': 1028,\n",
       " 'cheeky': 1793,\n",
       " 'let': 6333,\n",
       " 'silly': 9926,\n",
       " 'just': 6016,\n",
       " 'chill': 1841,\n",
       " 'eat': 3410,\n",
       " 'healthy': 5123,\n",
       " 'stuff': 10722,\n",
       " 'dads': 2652,\n",
       " 'sister': 9969,\n",
       " 'annoy': 390,\n",
       " 'bossy': 1221,\n",
       " 'attitude': 622,\n",
       " 'naughty': 7317,\n",
       " 'hurtful': 5460,\n",
       " 'argumentative': 506,\n",
       " 'school': 9535,\n",
       " 'death': 2741,\n",
       " 'end': 3571,\n",
       " 'road': 9223,\n",
       " 'life': 6367,\n",
       " 'traveler': 11525,\n",
       " 'soul': 10290,\n",
       " 'guide': 4914,\n",
       " 'solid': 10236,\n",
       " 'mountain': 7168,\n",
       " 'emerge': 3534,\n",
       " 'view': 12115,\n",
       " 'way': 12311,\n",
       " 'black': 1041,\n",
       " 'white': 12416,\n",
       " 'picture': 8125,\n",
       " 'forever': 4275,\n",
       " 'real': 8838,\n",
       " 'secret': 9630,\n",
       " 'giggle': 4611,\n",
       " 'chat': 1779,\n",
       " 'feel': 3979,\n",
       " 'sad': 9388,\n",
       " 'trust': 11615,\n",
       " 'grow': 4885,\n",
       " 'cat': 1664,\n",
       " 'wander': 12239,\n",
       " 'forest': 4271,\n",
       " 'aware': 675,\n",
       " 'tree': 11540,\n",
       " 'climb': 1986,\n",
       " 'high': 5244,\n",
       " 'marshy': 6754,\n",
       " 'swamp': 10897,\n",
       " 'stuck': 10717,\n",
       " 'come': 2115,\n",
       " 'stop': 10634,\n",
       " 'saw': 9485,\n",
       " 'strolling': 10701,\n",
       " 'past': 7912,\n",
       " 'run': 9350,\n",
       " 'paw': 7947,\n",
       " 'furry': 4462,\n",
       " 'coat': 2045,\n",
       " 'mark': 6739,\n",
       " 'site': 9972,\n",
       " 'people': 8012,\n",
       " 'send': 9667,\n",
       " 'poem': 8275,\n",
       " 'advertising': 159,\n",
       " 'comment': 2134,\n",
       " 'mention': 6880,\n",
       " 'sorry': 10284,\n",
       " 'meow': 6882,\n",
       " 'strum': 10711,\n",
       " 'guitar': 4922,\n",
       " 'dog': 3155,\n",
       " 'woof': 12562,\n",
       " 'beat': 856,\n",
       " 'drum': 3316,\n",
       " 'bird': 1018,\n",
       " 'song': 10255,\n",
       " 'tin': 11345,\n",
       " 'whistle': 12414,\n",
       " 'tune': 11642,\n",
       " 'pig': 8138,\n",
       " 'grunt': 4902,\n",
       " 'toot': 11424,\n",
       " 'kazoo': 6046,\n",
       " 'flute': 4203,\n",
       " 'melody': 6854,\n",
       " 'graceful': 4755,\n",
       " 'swan': 10900,\n",
       " 'shake': 9738,\n",
       " 'maraca': 6722,\n",
       " 'rattle': 8812,\n",
       " 'snake': 10146,\n",
       " 'jive': 5949,\n",
       " 'fairyland': 3880,\n",
       " 'clock': 1996,\n",
       " 'strike': 10686,\n",
       " 'lunch': 6591,\n",
       " 'time': 11339,\n",
       " 'meant': 6825,\n",
       " 'dust': 3358,\n",
       " 'twinkle': 11689,\n",
       " 'shone': 9827,\n",
       " 'golden': 4710,\n",
       " 'twas': 11671,\n",
       " 'gold': 4709,\n",
       " 'break': 1289,\n",
       " 'plate': 8212,\n",
       " 'blame': 1052,\n",
       " 'make': 6673,\n",
       " 'late': 6230,\n",
       " 'snap': 10149,\n",
       " 'key': 6061,\n",
       " 'blue': 1129,\n",
       " 'mean': 6822,\n",
       " 'flu': 4190,\n",
       " 'look': 6518,\n",
       " 'right': 9189,\n",
       " 'everyday': 3722,\n",
       " 'rrrrreeeeaaalllyyyy': 9316,\n",
       " 'kind': 6078,\n",
       " 'shout': 9848,\n",
       " 'annoying': 392,\n",
       " 'work': 12572,\n",
       " 'aaaaahhhhhh': 3,\n",
       " 'wolf': 12542,\n",
       " 'den': 2866,\n",
       " 'provide': 8580,\n",
       " 'shelter': 9786,\n",
       " 'warmth': 12261,\n",
       " 'cub': 2569,\n",
       " 'soon': 10263,\n",
       " 'bear': 849,\n",
       " 'chicken': 1827,\n",
       " 'fun': 4443,\n",
       " 'fee': 3974,\n",
       " 'ciennah': 1905,\n",
       " 'age': 199,\n",
       " 'home': 5318,\n",
       " 'maybe': 6805,\n",
       " 'thing': 11222,\n",
       " 'fall': 3889,\n",
       " 'pick': 8121,\n",
       " 'able': 24,\n",
       " 'remember': 8993,\n",
       " 'say': 9488,\n",
       " 'tiger': 11318,\n",
       " 'loud': 6544,\n",
       " 'roar': 9228,\n",
       " 'fierce': 4027,\n",
       " 'proud': 8576,\n",
       " 'cloud': 2016,\n",
       " 'float': 4161,\n",
       " 'sky': 10018,\n",
       " 'steam': 10568,\n",
       " 'rise': 9211,\n",
       " 'cap': 1579,\n",
       " 'sea': 9604,\n",
       " 'shower': 9854,\n",
       " 'water': 12292,\n",
       " 'tap': 11042,\n",
       " 'drain': 3244,\n",
       " 'rain': 8759,\n",
       " 'collecting': 2084,\n",
       " 'pan': 7825,\n",
       " 'filter': 4049,\n",
       " 'soggy': 10217,\n",
       " 'sand': 9434,\n",
       " 'jealousy': 5907,\n",
       " 'green': 4831,\n",
       " 'taste': 11061,\n",
       " 'porridge': 8354,\n",
       " 'smell': 10121,\n",
       " 'jungle': 6009,\n",
       " 'berry': 953,\n",
       " 'pull': 8618,\n",
       " 'hair': 4974,\n",
       " 'aren': 499,\n",
       " 'arrive': 532,\n",
       " 'think': 11224,\n",
       " 'add': 110,\n",
       " 'emoticon': 3540,\n",
       " 'word': 12566,\n",
       " 'tell': 11118,\n",
       " 'share': 9757,\n",
       " 'message': 6910,\n",
       " 'thanks': 11182,\n",
       " 'roger': 9251,\n",
       " 'stevens': 10587,\n",
       " 'body': 1159,\n",
       " 'start': 10536,\n",
       " 'walk': 12222,\n",
       " 'torch': 11437,\n",
       " 'wouldn': 12599,\n",
       " 'worry': 12590,\n",
       " 'ghost': 4598,\n",
       " 'walking': 12224,\n",
       " 'door': 3190,\n",
       " 'oh': 7602,\n",
       " 'isn': 5852,\n",
       " 'guess': 4909,\n",
       " 'win': 12469,\n",
       " 'haunt': 5089,\n",
       " 'blood': 1108,\n",
       " 'drip': 3290,\n",
       " 'bye': 1485,\n",
       " 'orange': 7663,\n",
       " 'darker': 2695,\n",
       " 'true': 11606,\n",
       " 'yellow': 12675,\n",
       " 'light': 6382,\n",
       " 'shade': 9730,\n",
       " 'color': 2099,\n",
       " 'daylight': 2720,\n",
       " 'sun': 10806,\n",
       " 'act': 96,\n",
       " 'story': 10642,\n",
       " 'costume': 2377,\n",
       " 'heel': 5170,\n",
       " 'rose': 9283,\n",
       " 'red': 8901,\n",
       " 'violet': 12134,\n",
       " 'crayon': 2452,\n",
       " 'spike': 10375,\n",
       " 'prick': 8477,\n",
       " 'partner': 7895,\n",
       " 'different': 2995,\n",
       " 'size': 9979,\n",
       " 'big': 996,\n",
       " 'live': 6459,\n",
       " 'zoo': 12734,\n",
       " 'pet': 8077,\n",
       " 'monster': 7097,\n",
       " 'room': 9273,\n",
       " 'alive': 271,\n",
       " 'stand': 10517,\n",
       " 'tall': 11024,\n",
       " 'wind': 12470,\n",
       " 'branch': 1277,\n",
       " 'spooky': 10422,\n",
       " 'curled': 2606,\n",
       " 'sharp': 9762,\n",
       " 'creature': 2471,\n",
       " 'crawl': 2449,\n",
       " 'long': 6510,\n",
       " 'leg': 6300,\n",
       " 'beetle': 894,\n",
       " 'shiny': 9808,\n",
       " 'shell': 9785,\n",
       " 'dark': 2691,\n",
       " 'murky': 7234,\n",
       " 'ready': 8837,\n",
       " 'stormy': 10641,\n",
       " 'glint': 4658,\n",
       " 'thunder': 11292,\n",
       " 'lightning': 6389,\n",
       " 'blast': 1064,\n",
       " 'soak': 10194,\n",
       " 'wet': 12374,\n",
       " 'slowly': 10093,\n",
       " 'rot': 9288,\n",
       " 'away': 677,\n",
       " 'cow': 2415,\n",
       " 'shin': 9804,\n",
       " 'degree': 2826,\n",
       " 'barbecue': 772,\n",
       " 'lit': 6448,\n",
       " 'watch': 12287,\n",
       " 'glass': 4643,\n",
       " 'hat': 5073,\n",
       " 'sing': 9950,\n",
       " 'flower': 4181,\n",
       " 'blossom': 1119,\n",
       " 'sunday': 10810,\n",
       " 'toy': 11476,\n",
       " 'jacob': 5878,\n",
       " 'rooster': 9278,\n",
       " 'crow': 2527,\n",
       " 'owl': 7765,\n",
       " 'hoot': 5354,\n",
       " 'pigeon': 8139,\n",
       " 'coo': 2323,\n",
       " 'dove': 3220,\n",
       " 'robin': 9236,\n",
       " 'chirp': 1852,\n",
       " 'squark': 10465,\n",
       " 'duck': 3324,\n",
       " 'quack': 8675,\n",
       " 'swallow': 10895,\n",
       " 'talk': 11021,\n",
       " 'wood': 12557,\n",
       " 'dance': 2672,\n",
       " 'whirl': 12404,\n",
       " 'blow': 1122,\n",
       " 'leaf': 6268,\n",
       " 'ground': 4880,\n",
       " 'step': 10583,\n",
       " 'hear': 5126,\n",
       " 'crunch': 2551,\n",
       " 'beneath': 942,\n",
       " 'foot': 4241,\n",
       " 'fly': 4206,\n",
       " 'head': 5108,\n",
       " 'rabbit': 8728,\n",
       " 'hop': 5358,\n",
       " 'middle': 6944,\n",
       " 'listening': 6446,\n",
       " 'nature': 7314,\n",
       " 'sound': 10294,\n",
       " 'peace': 7957,\n",
       " 'bed': 870,\n",
       " 'sol': 10223,\n",
       " 'shoe': 9825,\n",
       " 'stab': 10487,\n",
       " 'grain': 4769,\n",
       " 'population': 8348,\n",
       " 'china': 1849,\n",
       " 'computer': 2180,\n",
       " 'cool': 2333,\n",
       " 'wake': 12218,\n",
       " 'morning': 7128,\n",
       " 'stomach': 10622,\n",
       " 'ache': 78,\n",
       " 'goodness': 4722,\n",
       " 'sake': 9409,\n",
       " 'night': 7401,\n",
       " 'chilli': 1842,\n",
       " 'kick': 6068,\n",
       " 'didn': 2984,\n",
       " 'spicy': 10370,\n",
       " 'deserve': 2909,\n",
       " 'ipad': 5832,\n",
       " 'phone': 8106,\n",
       " 'loan': 6480,\n",
       " 'mom': 7081,\n",
       " 'uncle': 11762,\n",
       " 'dill': 3007,\n",
       " 'instead': 5750,\n",
       " 'electronic': 3485,\n",
       " 'eye': 3825,\n",
       " 'pad': 7783,\n",
       " 'cut': 2625,\n",
       " 'india': 5642,\n",
       " 'chutney': 1901,\n",
       " 'food': 4234,\n",
       " 'seafood': 9608,\n",
       " 'pakistan': 7804,\n",
       " 'culture': 2585,\n",
       " 'relatives': 8970,\n",
       " 'vulture': 12193,\n",
       " 'holiday': 5309,\n",
       " 'friends': 4388,\n",
       " 'title': 11370,\n",
       " 'choose': 1869,\n",
       " 'fight': 4036,\n",
       " 'borrow': 1217,\n",
       " 'tomorrow': 11407,\n",
       " 'trade': 11485,\n",
       " 'privacy': 8501,\n",
       " 'invade': 5815,\n",
       " 'hug': 5421,\n",
       " 'smile': 10125,\n",
       " 'tear': 11091,\n",
       " 'shed': 9778,\n",
       " 'spread': 10441,\n",
       " 'know': 6126,\n",
       " 'care': 1610,\n",
       " 'laugh': 6239,\n",
       " 'wish': 12513,\n",
       " 'answer': 398,\n",
       " 'fade': 3864,\n",
       " 'promise': 8538,\n",
       " 'lose': 6536,\n",
       " 'heart': 5132,\n",
       " 'apart': 425,\n",
       " 'use': 12000,\n",
       " 'important': 5589,\n",
       " 'respect': 9065,\n",
       " 'lovingly': 6562,\n",
       " 'cute': 2626,\n",
       " 'birthday': 1025,\n",
       " 'dragon': 3242,\n",
       " 'hill': 5255,\n",
       " 'abc': 12,\n",
       " 'short': 9837,\n",
       " 'def': 2798,\n",
       " 'hij': 5253,\n",
       " 'hike': 5254,\n",
       " 'finally': 4053,\n",
       " 'lolly': 6503,\n",
       " 'sweet': 10917,\n",
       " 'relax': 8971,\n",
       " 'flow': 4180,\n",
       " 'happen': 5024,\n",
       " 'forward': 4313,\n",
       " 'minecraft': 6986,\n",
       " 'brainy': 1273,\n",
       " 'fast': 3934,\n",
       " 'unfortunately': 11838,\n",
       " 'bad': 717,\n",
       " 'paradise': 7853,\n",
       " 'split': 10409,\n",
       " 'yum': 12704,\n",
       " 'ok': 7605,\n",
       " 'quiet': 8710,\n",
       " 'sight': 9907,\n",
       " 'wave': 12302,\n",
       " 'wonderful': 12548,\n",
       " 'spirit': 10391,\n",
       " 'build': 1408,\n",
       " 'shelf': 9784,\n",
       " 'creation': 2466,\n",
       " 'craft': 2434,\n",
       " 'mind': 6983,\n",
       " 'tasty': 11065,\n",
       " 'temptation': 11128,\n",
       " 'careful': 1614,\n",
       " 'wise': 12511,\n",
       " 'fortnite': 4307,\n",
       " 'fortnight': 4305,\n",
       " 'pleasure': 8232,\n",
       " 'mate': 6784,\n",
       " 'melissa': 6848,\n",
       " 'drop': 3304,\n",
       " 'hide': 5240,\n",
       " 'seek': 9642,\n",
       " 'tidy': 11311,\n",
       " 'messy': 6916,\n",
       " 'bully': 1420,\n",
       " 'sigh': 9905,\n",
       " 'norman': 7470,\n",
       " 'quite': 8719,\n",
       " 'brilliant': 1337,\n",
       " 'idea': 5498,\n",
       " 'scale': 9494,\n",
       " 'purple': 8650,\n",
       " 'shine': 9805,\n",
       " 'amazing': 314,\n",
       " 'yeah': 12665,\n",
       " 'yes': 12677,\n",
       " 'breeze': 1306,\n",
       " 'sway': 10908,\n",
       " 'hooting': 5357,\n",
       " 'beak': 844,\n",
       " 'wing': 12488,\n",
       " 'gleam': 4647,\n",
       " 'moon': 7106,\n",
       " 'silhouette': 9920,\n",
       " 'swoop': 10956,\n",
       " 'search': 9614,\n",
       " 'prey': 8474,\n",
       " 'fro': 4402,\n",
       " 'spot': 10430,\n",
       " 'goes': 4704,\n",
       " 'read': 8831,\n",
       " 'far': 3915,\n",
       " 'clearly': 1972,\n",
       " 'sleep': 10048,\n",
       " 'early': 3387,\n",
       " 'happily': 5029,\n",
       " 'clean': 1964,\n",
       " 'properly': 8551,\n",
       " 'strange': 10653,\n",
       " 'fear': 3960,\n",
       " 'butterfly': 1475,\n",
       " 'mouse': 7179,\n",
       " 'die': 2986,\n",
       " 'man': 6688,\n",
       " 'cousin': 2408,\n",
       " 'happiness': 5030,\n",
       " 'cuddle': 2572,\n",
       " 'await': 667,\n",
       " 'sugar': 10782,\n",
       " 'moist': 7072,\n",
       " 'yummy': 12705,\n",
       " 'mixture': 7046,\n",
       " 'inside': 5731,\n",
       " 'melt': 6856,\n",
       " 'mouth': 7187,\n",
       " 'elsie': 3508,\n",
       " 'paint': 7797,\n",
       " 'hate': 5080,\n",
       " 'thistle': 11234,\n",
       " 'educate': 3439,\n",
       " 'poetry': 8281,\n",
       " 'footpath': 4245,\n",
       " 'world': 12581,\n",
       " 'chocolate': 1861,\n",
       " 'haven': 5092,\n",
       " 'rhyme': 9147,\n",
       " 'll': 6470,\n",
       " 'goodbye': 4721,\n",
       " 'mix': 7042,\n",
       " 'colour': 2104,\n",
       " 'orchard': 7668,\n",
       " 'juice': 5998,\n",
       " 'seed': 9641,\n",
       " 'cure': 2597,\n",
       " 'gramps': 4774,\n",
       " 'grig': 4850,\n",
       " 'field': 4024,\n",
       " 'clover': 2024,\n",
       " 'piggy': 8140,\n",
       " 'need': 7345,\n",
       " 'wait': 12214,\n",
       " 'tonight': 11416,\n",
       " 'superbowl': 10826,\n",
       " 'cheer': 1794,\n",
       " 'lovely': 6557,\n",
       " 'friday': 4383,\n",
       " 'tv': 11669,\n",
       " 'butter': 1474,\n",
       " 'toast': 11377,\n",
       " 'face': 3851,\n",
       " 'sure': 10851,\n",
       " 'monday': 7088,\n",
       " 'lobby': 6482,\n",
       " 'swim': 10936,\n",
       " 'pine': 8155,\n",
       " 'delicious': 2838,\n",
       " 'marshmallows': 6753,\n",
       " 'beautiful': 863,\n",
       " 'star': 10523,\n",
       " 'haiku': 4971,\n",
       " 'sunset': 10821,\n",
       " 'summer': 10800,\n",
       " 'flame': 4105,\n",
       " 'burn': 1446,\n",
       " 'bonfire': 1187,\n",
       " 'glow': 4678,\n",
       " 'bright': 1328,\n",
       " 'sweetness': 10926,\n",
       " 'freshly': 4378,\n",
       " 'grass': 4799,\n",
       " 'sadness': 9394,\n",
       " 'emotion': 3541,\n",
       " 'indigo': 5651,\n",
       " 'carrot': 1636,\n",
       " 'parrot': 7880,\n",
       " 'ride': 9173,\n",
       " 'chariot': 1761,\n",
       " 'tow': 11467,\n",
       " 'rotten': 9292,\n",
       " 'thrown': 11283,\n",
       " 'left': 6298,\n",
       " 'outside': 7725,\n",
       " 'peck': 7972,\n",
       " 'boy': 1252,\n",
       " 'pitch': 8177,\n",
       " 'beauty': 865,\n",
       " 'earth': 3392,\n",
       " 'ha': 4954,\n",
       " 'teach': 11083,\n",
       " 'english': 3592,\n",
       " 'spanish': 10319,\n",
       " 'hindi': 5260,\n",
       " 'portuguese': 8362,\n",
       " 'splash': 10399,\n",
       " 'tsunami': 11624,\n",
       " 'storm': 10640,\n",
       " 'soil': 10219,\n",
       " 'form': 4294,\n",
       " 'race': 8731,\n",
       " 'bunch': 1429,\n",
       " 'rude': 9328,\n",
       " 'sport': 10427,\n",
       " 'pump': 8627,\n",
       " 'forget': 4281,\n",
       " 'utility': 12010,\n",
       " 'pole': 8297,\n",
       " 'west': 12370,\n",
       " 'hot': 5394,\n",
       " 'cover': 2411,\n",
       " 'filbert': 4041,\n",
       " 'fox': 4325,\n",
       " 'gon': 4714,\n",
       " 'master': 6775,\n",
       " 'table': 10983,\n",
       " 'howl': 5411,\n",
       " 'vicious': 12104,\n",
       " 'moonlight': 7110,\n",
       " 'game': 4506,\n",
       " 'catch': 1668,\n",
       " 'cave': 1690,\n",
       " 'stay': 10558,\n",
       " 'slobber': 10083,\n",
       " 'drool': 3299,\n",
       " 'teeth': 11109,\n",
       " 'mace': 6629,\n",
       " 'bruise': 1377,\n",
       " 'especially': 3687,\n",
       " 'bat': 816,\n",
       " 'log': 6496,\n",
       " 'air': 231,\n",
       " 'conditioner': 2209,\n",
       " 'toilet': 11391,\n",
       " 'google': 4725,\n",
       " 'classroom': 1958,\n",
       " 'prefect': 8443,\n",
       " 'honest': 5334,\n",
       " 'fair': 3875,\n",
       " 'video': 12111,\n",
       " 'youtube': 12700,\n",
       " 'youtuber': 12701,\n",
       " 'parent': 7868,\n",
       " 'ask': 561,\n",
       " 'permission': 8041,\n",
       " 'post': 8376,\n",
       " 'excitedly': 3751,\n",
       " 'role': 9256,\n",
       " 'lo': 6476,\n",
       " 'lov': 6551,\n",
       " 'fo': 4210,\n",
       " 'fort': 4301,\n",
       " 'fortn': 4303,\n",
       " 'fortni': 4304,\n",
       " 'fortnit': 4306,\n",
       " 'pu': 8605,\n",
       " 'pub': 8606,\n",
       " 'pubg': 8607,\n",
       " 'curtain': 2614,\n",
       " 'brewing': 1313,\n",
       " 'chime': 1845,\n",
       " 'harry': 5059,\n",
       " 'prefer': 8444,\n",
       " 'perry': 8050,\n",
       " 'fell': 3988,\n",
       " 'hole': 5306,\n",
       " 'yeet': 12672,\n",
       " 'breakfast': 1291,\n",
       " 'realize': 8843,\n",
       " 'meat': 6829,\n",
       " 'fine': 4057,\n",
       " 'mirror': 7011,\n",
       " 'brush': 1382,\n",
       " 'pack': 7778,\n",
       " 'bag': 721,\n",
       " 'math': 6788,\n",
       " 'test': 11167,\n",
       " 'gag': 4483,\n",
       " 'img': 5546,\n",
       " 'alt': 301,\n",
       " 'class': 1955,\n",
       " 'wp': 12607,\n",
       " 'smiley': 10126,\n",
       " 'src': 10481,\n",
       " 'http': 5415,\n",
       " 'style': 10734,\n",
       " 'height': 5177,\n",
       " 'max': 6804,\n",
       " 'yell': 12673,\n",
       " 'stair': 10501,\n",
       " 'bob': 1155,\n",
       " 'mule': 7218,\n",
       " 'later': 6232,\n",
       " 'wonder': 12547,\n",
       " 'pie': 8127,\n",
       " 'yay': 12662,\n",
       " 'river': 9218,\n",
       " 'beast': 853,\n",
       " 'shiver': 9817,\n",
       " 'usa': 11998,\n",
       " 'veggie': 12064,\n",
       " 'teacher': 11084,\n",
       " 'book': 1196,\n",
       " 'brother': 1369,\n",
       " 'hershey': 5227,\n",
       " 'bar': 767,\n",
       " 'rush': 9360,\n",
       " 'minute': 7005,\n",
       " 'energy': 3585,\n",
       " 'car': 1597,\n",
       " 'homework': 5327,\n",
       " 'jump': 6002,\n",
       " 'alarm': 243,\n",
       " 'set': 9707,\n",
       " 'cold': 2076,\n",
       " 'dread': 3260,\n",
       " 'downstairs': 3227,\n",
       " 'munch': 7228,\n",
       " 'dare': 2688,\n",
       " 'hungry': 5447,\n",
       " 'mommy': 7085,\n",
       " 'daddy': 2651,\n",
       " 'ate': 599,\n",
       " 'bus': 1463,\n",
       " 'leap': 6275,\n",
       " 'motto': 7160,\n",
       " 'lotto': 6543,\n",
       " 'inspirational': 5741,\n",
       " 'feature': 3971,\n",
       " 'learning': 6281,\n",
       " 'pride': 8480,\n",
       " 'powerful': 8410,\n",
       " 'stride': 10683,\n",
       " 'jewel': 5935,\n",
       " 'balloon': 745,\n",
       " 'sibling': 9884,\n",
       " 'treat': 11537,\n",
       " 'mister': 7037,\n",
       " 'perfect': 8023,\n",
       " 'pumpkin': 8628,\n",
       " 'baby': 699,\n",
       " 'fussy': 4470,\n",
       " 'feeling': 3980,\n",
       " 'sunshine': 10822,\n",
       " 'fence': 3994,\n",
       " 'intense': 5769,\n",
       " 'poor': 8333,\n",
       " 'straight': 10647,\n",
       " 'woooed': 12565,\n",
       " 'gymnastics': 4952,\n",
       " 'simply': 9942,\n",
       " 'flip': 4157,\n",
       " 'cartwheel': 1645,\n",
       " 'lead': 6265,\n",
       " 'deep': 2791,\n",
       " 'crystal': 2568,\n",
       " 'glimmer': 4655,\n",
       " 'coral': 2346,\n",
       " 'waterfall': 12293,\n",
       " 'rainy': 8766,\n",
       " 'umbrella': 11727,\n",
       " 'eating': 3412,\n",
       " 'mozzarella': 7203,\n",
       " 'ramble': 8774,\n",
       " 'hard': 5034,\n",
       " 'lawn': 6253,\n",
       " 'topic': 11431,\n",
       " 'quantity': 8686,\n",
       " 'wide': 12439,\n",
       " 'bike': 1001,\n",
       " 'rid': 9169,\n",
       " 'riding': 9181,\n",
       " 'backwards': 713,\n",
       " 'dinner': 3019,\n",
       " 'tower': 11470,\n",
       " 'terror': 11162,\n",
       " 'restaurant': 9076,\n",
       " 'enjoying': 3604,\n",
       " 'transparent': 11516,\n",
       " 'wear': 12320,\n",
       " 'sleeve': 10055,\n",
       " 'guffaw': 4912,\n",
       " 'laughter': 6240,\n",
       " 'prance': 8419,\n",
       " 'free': 4359,\n",
       " 'groove': 4872,\n",
       " 'stomp': 10624,\n",
       " 'romp': 9266,\n",
       " 'welcome': 12358,\n",
       " 'bin': 1013,\n",
       " 'solve': 10245,\n",
       " 'maze': 6809,\n",
       " 'bring': 1343,\n",
       " 'nigeria': 7398,\n",
       " 'mystical': 7271,\n",
       " 'sandy': 9439,\n",
       " 'floor': 4168,\n",
       " 'algae': 262,\n",
       " 'widow': 12442,\n",
       " 'sung': 10813,\n",
       " 'today': 11383,\n",
       " 'follow': 4229,\n",
       " 'gracefully': 4756,\n",
       " 'flowingly': 4188,\n",
       " 'harmonise': 5052,\n",
       " 'africa': 190,\n",
       " 'huge': 5422,\n",
       " 'continent': 2301,\n",
       " 'honey': 5337,\n",
       " 'hold': 5302,\n",
       " 'tricky': 11566,\n",
       " 'north': 7472,\n",
       " 'south': 10305,\n",
       " 'america': 330,\n",
       " 'europe': 3708,\n",
       " 'kangaroo': 6032,\n",
       " 'stralia': 10651,\n",
       " 'australia': 644,\n",
       " 'asia': 559,\n",
       " 'antarctica': 401,\n",
       " 'ring': 9196,\n",
       " 'ear': 3384,\n",
       " 'jolt': 5968,\n",
       " 'electricity': 3482,\n",
       " 'hurtle': 5462,\n",
       " 'lay': 6255,\n",
       " 'scared': 9509,\n",
       " 'noise': 7445,\n",
       " 'bang': 756,\n",
       " 'swing': 10943,\n",
       " 'window': 12478,\n",
       " 'crackle': 2431,\n",
       " 'angrily': 374,\n",
       " 'sparkle': 10325,\n",
       " 'flicker': 4146,\n",
       " 'existence': 3772,\n",
       " 'disappear': 3045,\n",
       " 'second': 9629,\n",
       " 'hypnotize': 5480,\n",
       " 'mesmerize': 6908,\n",
       " 'steel': 10571,\n",
       " 'stamen': 10511,\n",
       " 'unwavering': 11961,\n",
       " 'reveal': 9106,\n",
       " 'gaze': 4551,\n",
       " 'steely': 10572,\n",
       " 'expression': 3808,\n",
       " 'icy': 5495,\n",
       " 'pierce': 8132,\n",
       " 'anybody': 419,\n",
       " 'terrify': 11159,\n",
       " 'blaze': 1066,\n",
       " 'passion': 7908,\n",
       " 'infinite': 5671,\n",
       " 'fuel': 4434,\n",
       " 'ablaze': 23,\n",
       " 'soften': 10211,\n",
       " 'mask': 6765,\n",
       " 'perplex': 8048,\n",
       " 'year': 12666,\n",
       " 'warm': 12256,\n",
       " 'armor': 518,\n",
       " 'reflection': 8925,\n",
       " 'smooth': 10136,\n",
       " 'scent': 9523,\n",
       " 'jar': 5897,\n",
       " 'travel': 11524,\n",
       " 'brighten': 1329,\n",
       " 'rad': 8739,\n",
       " 'super': 10824,\n",
       " 'okay': 7606,\n",
       " 'morocco': 7129,\n",
       " 'niger': 7397,\n",
       " 'chad': 1734,\n",
       " 'swaziland': 10911,\n",
       " 'band': 752,\n",
       " 'brand': 1278,\n",
       " 'legs': 6309,\n",
       " 'shard': 9756,\n",
       " 'tiny': 11357,\n",
       " 'flap': 4108,\n",
       " 'plant': 8206,\n",
       " 'asleep': 564,\n",
       " 'brown': 1373,\n",
       " 'dig': 2999,\n",
       " 'speckle': 10346,\n",
       " 'nose': 7475,\n",
       " 'lick': 6358,\n",
       " 'pollution': 8307,\n",
       " 'awful': 681,\n",
       " 'crummy': 2548,\n",
       " 'killing': 6077,\n",
       " 'execute': 3758,\n",
       " 'saudi': 9472,\n",
       " 'arabia': 479,\n",
       " 'korea': 6141,\n",
       " 'greenland': 4833,\n",
       " 'healing': 5120,\n",
       " 'save': 9479,\n",
       " 'acceptable': 58,\n",
       " 'fresh': 4375,\n",
       " 'animal': 379,\n",
       " 'human': 5429,\n",
       " 'factory': 3861,\n",
       " 'victory': 12110,\n",
       " 'hello': 5191,\n",
       " 'habitat': 4959,\n",
       " 'dry': 3322,\n",
       " 'heat': 5146,\n",
       " 'burnt': 1451,\n",
       " 'buzz': 1482,\n",
       " 'brings': 1346,\n",
       " 'sunny': 10818,\n",
       " 'seat': 9623,\n",
       " 'roller': 9258,\n",
       " 'coaster': 2044,\n",
       " 'zoom': 12736,\n",
       " 'slope': 10086,\n",
       " 'whoosh': 12429,\n",
       " 'steep': 10573,\n",
       " 'bump': 1425,\n",
       " 'corner': 2352,\n",
       " 'fling': 4154,\n",
       " 'force': 4253,\n",
       " 'scrap': 9567,\n",
       " 'spring': 10445,\n",
       " 'sudden': 10770,\n",
       " 'faster': 3936,\n",
       " 'round': 9300,\n",
       " 'flash': 4112,\n",
       " 'scream': 9577,\n",
       " 'push': 8662,\n",
       " 'thriller': 11270,\n",
       " 'gaping': 4517,\n",
       " 'memory': 6864,\n",
       " 'painful': 7793,\n",
       " 'joyous': 5985,\n",
       " ...}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['95',\n",
       " 'aaaaaaaaaaaaaccccccccccoooooooo',\n",
       " 'aaaaaaaaaaahhhhhhhhhhhhhhhh',\n",
       " 'aaaaahhhhhh',\n",
       " 'aarhus',\n",
       " 'abacus',\n",
       " 'abandon',\n",
       " 'abated',\n",
       " 'abattoir',\n",
       " 'abbey',\n",
       " 'abbie',\n",
       " 'abbot',\n",
       " 'abc',\n",
       " 'abdicate',\n",
       " 'abdomen',\n",
       " 'abduct',\n",
       " 'abe',\n",
       " 'abed',\n",
       " 'abel',\n",
       " 'aberdeen',\n",
       " 'abide',\n",
       " 'ability',\n",
       " 'abject',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'abominable',\n",
       " 'abort',\n",
       " 'abound',\n",
       " 'abracadabra',\n",
       " 'abraham',\n",
       " 'abroad',\n",
       " 'abrumptly',\n",
       " 'abrupt',\n",
       " 'absconded',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutes',\n",
       " 'absolution',\n",
       " 'absolves',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'abstract',\n",
       " 'abstracted',\n",
       " 'absurd',\n",
       " 'absurdities',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abusive',\n",
       " 'abyss',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'acclaim',\n",
       " 'accompaniment',\n",
       " 'accomplish',\n",
       " 'accomplishment',\n",
       " 'accord',\n",
       " 'accost',\n",
       " 'account',\n",
       " 'accounting',\n",
       " 'accrue',\n",
       " 'accumulate',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accusation',\n",
       " 'accuser',\n",
       " 'ace',\n",
       " 'ach',\n",
       " 'achaia',\n",
       " 'ache',\n",
       " 'achelous',\n",
       " 'achenor',\n",
       " 'acheron',\n",
       " 'achieve',\n",
       " 'achievement',\n",
       " 'achilles',\n",
       " 'aching',\n",
       " 'acid',\n",
       " 'acknowledge',\n",
       " 'acknowledging',\n",
       " 'acorn',\n",
       " 'acorned',\n",
       " 'acquire',\n",
       " 'acquires',\n",
       " 'acre',\n",
       " 'acrid',\n",
       " 'acrobat',\n",
       " 'act',\n",
       " 'actian',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acutally',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'adder',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addictive',\n",
       " 'addition',\n",
       " 'addle',\n",
       " 'address',\n",
       " 'adelade',\n",
       " 'adelaide',\n",
       " 'adele',\n",
       " 'adept',\n",
       " 'adequate',\n",
       " 'adieu',\n",
       " 'adjective',\n",
       " 'adjunct',\n",
       " 'adjust',\n",
       " 'adjusted',\n",
       " 'adjustment',\n",
       " 'admirable',\n",
       " 'admire',\n",
       " 'admirer',\n",
       " 'admit',\n",
       " 'ado',\n",
       " 'adolescence',\n",
       " 'adonis',\n",
       " 'adopt',\n",
       " 'adorable',\n",
       " 'adoration',\n",
       " 'adore',\n",
       " 'adorn',\n",
       " 'adornment',\n",
       " 'adoze',\n",
       " 'adrastus',\n",
       " 'adrenaline',\n",
       " 'adrift',\n",
       " 'adroit',\n",
       " 'adult',\n",
       " 'adulterer',\n",
       " 'adulterous',\n",
       " 'adultery',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'advent',\n",
       " 'adventist',\n",
       " 'adventure',\n",
       " 'adventurer',\n",
       " 'adventurous',\n",
       " 'adversity',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'aegis',\n",
       " 'aemilia',\n",
       " 'aeneas',\n",
       " 'aeonium',\n",
       " 'aerie',\n",
       " 'aero',\n",
       " 'aeroplane',\n",
       " 'aeros',\n",
       " 'aeschylus',\n",
       " 'aesop',\n",
       " 'aether',\n",
       " 'aetna',\n",
       " 'afar',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affectation',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affliction',\n",
       " 'afford',\n",
       " 'afield',\n",
       " 'aflame',\n",
       " 'afloat',\n",
       " 'aflutter',\n",
       " 'afold',\n",
       " 'afoot',\n",
       " 'aforetime',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'aft',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'aftershave',\n",
       " 'afterward',\n",
       " 'agains',\n",
       " 'agape',\n",
       " 'agathas',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agee',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'aggregate',\n",
       " 'aggression',\n",
       " 'agh',\n",
       " 'agile',\n",
       " 'agility',\n",
       " 'agitate',\n",
       " 'ago',\n",
       " 'agon',\n",
       " 'agonizing',\n",
       " 'agony',\n",
       " 'agree',\n",
       " 'agrees',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhhhhhhhhhhhhhhhhhhhhhhh',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aide',\n",
       " 'aiken',\n",
       " 'ail',\n",
       " 'ails',\n",
       " 'aim',\n",
       " 'aimless',\n",
       " 'aimlessly',\n",
       " 'ain',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airless',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'airstations',\n",
       " 'airy',\n",
       " 'aisle',\n",
       " 'ajar',\n",
       " 'akashic',\n",
       " 'ala',\n",
       " 'alabaster',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alba',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'albion',\n",
       " 'albir',\n",
       " 'album',\n",
       " 'alchemize',\n",
       " 'alcoflunce',\n",
       " 'alcohol',\n",
       " 'alder',\n",
       " 'ale',\n",
       " 'aleph',\n",
       " 'alert',\n",
       " 'alesha',\n",
       " 'alex',\n",
       " 'alexis',\n",
       " 'alfie',\n",
       " 'algae',\n",
       " 'algebra',\n",
       " 'alice',\n",
       " 'alien',\n",
       " 'aliens',\n",
       " 'alight',\n",
       " 'alignment',\n",
       " 'alike',\n",
       " 'alistair',\n",
       " 'alive',\n",
       " 'alivei',\n",
       " 'allegedly',\n",
       " 'allen',\n",
       " 'alley',\n",
       " 'allied',\n",
       " 'alligator',\n",
       " 'allight',\n",
       " 'alliteration',\n",
       " 'allllllllllllllllllllllllllllllllll',\n",
       " 'allness',\n",
       " 'allot',\n",
       " 'allotments',\n",
       " 'allow',\n",
       " 'allowance',\n",
       " 'allure',\n",
       " 'allurement',\n",
       " 'alluvial',\n",
       " 'ally',\n",
       " 'alma',\n",
       " 'almighty',\n",
       " 'almond',\n",
       " 'aloft',\n",
       " 'alonethis',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'alp',\n",
       " 'alphabet',\n",
       " 'alright',\n",
       " 'alt',\n",
       " 'altar',\n",
       " 'alter',\n",
       " 'alternately',\n",
       " 'alters',\n",
       " 'altitude',\n",
       " 'altogether',\n",
       " 'alway',\n",
       " 'amanda',\n",
       " 'amaryllis',\n",
       " 'amateur',\n",
       " 'amateurs',\n",
       " 'amaze',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'ambassador',\n",
       " 'amber',\n",
       " 'ambered',\n",
       " 'ambience',\n",
       " 'ambient',\n",
       " 'ambition',\n",
       " 'ambitious',\n",
       " 'amble',\n",
       " 'ambrosial',\n",
       " 'ambulance',\n",
       " 'ambush',\n",
       " 'ambushed',\n",
       " 'amen',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amethyst',\n",
       " 'amiable',\n",
       " 'amiably',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'amiss',\n",
       " 'ammoth',\n",
       " 'amor',\n",
       " 'amp',\n",
       " 'amphetamine',\n",
       " 'ample',\n",
       " 'amputate',\n",
       " 'amuse',\n",
       " 'ana',\n",
       " 'anacca',\n",
       " 'anahorish',\n",
       " 'analize',\n",
       " 'analyst',\n",
       " 'analytically',\n",
       " 'ancestral',\n",
       " 'anchor',\n",
       " 'anchored',\n",
       " 'ancient',\n",
       " 'andra',\n",
       " 'andrew',\n",
       " 'andromeda',\n",
       " 'andy',\n",
       " 'aneas',\n",
       " 'anemone',\n",
       " 'anesthetic',\n",
       " 'anew',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angeles',\n",
       " 'angelic',\n",
       " 'angelico',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angkor',\n",
       " 'angle',\n",
       " 'angler',\n",
       " 'anglo',\n",
       " 'angrily',\n",
       " 'angry',\n",
       " 'angst',\n",
       " 'angstrom',\n",
       " 'anguish',\n",
       " 'animal',\n",
       " 'anjou',\n",
       " 'ankle',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'annalist',\n",
       " 'annals',\n",
       " 'annihilation',\n",
       " 'anniversary',\n",
       " 'annodomini',\n",
       " 'announce',\n",
       " 'annoy',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'anoints',\n",
       " 'anonymity',\n",
       " 'anonymous',\n",
       " 'anorexic',\n",
       " 'anothers',\n",
       " 'answer',\n",
       " 'answering',\n",
       " 'ant',\n",
       " 'antarctica',\n",
       " 'antenna',\n",
       " 'anther',\n",
       " 'anthropology',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'anticipating',\n",
       " 'antidote',\n",
       " 'antimachus',\n",
       " 'antipode',\n",
       " 'antiquated',\n",
       " 'antique',\n",
       " 'antler',\n",
       " 'antlered',\n",
       " 'antony',\n",
       " 'anvil',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyte',\n",
       " 'anytiime',\n",
       " 'anytime',\n",
       " 'apan',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apathetic',\n",
       " 'ape',\n",
       " 'aphid',\n",
       " 'aphrodite',\n",
       " 'apollo',\n",
       " 'apologetic',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'apostrophe',\n",
       " 'appal',\n",
       " 'appall',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'apparition',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appears',\n",
       " 'appendicitis',\n",
       " 'apperhension',\n",
       " 'appertif',\n",
       " 'appetite',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'applesauce',\n",
       " 'appletree',\n",
       " 'appletrees',\n",
       " 'apply',\n",
       " 'appointed',\n",
       " 'appointment',\n",
       " 'appraise',\n",
       " 'appreciate',\n",
       " 'apprehension',\n",
       " 'apprentice',\n",
       " 'apprenticeship',\n",
       " 'appries',\n",
       " 'approach',\n",
       " 'approbation',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'april',\n",
       " 'apron',\n",
       " 'aqeducts',\n",
       " 'aqua',\n",
       " 'aquarium',\n",
       " 'aquarius',\n",
       " 'aquatic',\n",
       " 'aquinas',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabesque',\n",
       " 'arabia',\n",
       " 'arabian',\n",
       " 'arabic',\n",
       " 'aran',\n",
       " 'arbor',\n",
       " 'arc',\n",
       " 'arcadian',\n",
       " 'arcane',\n",
       " 'arch',\n",
       " 'archeaologist',\n",
       " 'archer',\n",
       " 'archerfield',\n",
       " 'archery',\n",
       " 'archetype',\n",
       " 'arctic',\n",
       " 'ardestan',\n",
       " 'ardor',\n",
       " 'ardour',\n",
       " 'arduously',\n",
       " 'area',\n",
       " 'aren',\n",
       " 'arena',\n",
       " 'aretha',\n",
       " 'arghhhhhh',\n",
       " 'argo',\n",
       " 'argue',\n",
       " 'argument',\n",
       " 'argumentative',\n",
       " 'arianna',\n",
       " 'ariel',\n",
       " 'aries',\n",
       " 'arise',\n",
       " 'arissa',\n",
       " 'aristocrat',\n",
       " 'aristotle',\n",
       " 'ark',\n",
       " 'arm',\n",
       " 'armaan',\n",
       " 'armchair',\n",
       " 'armor',\n",
       " 'armour',\n",
       " 'armoured',\n",
       " 'army',\n",
       " 'aroma',\n",
       " 'aromatic',\n",
       " 'arose',\n",
       " 'arouse',\n",
       " 'arousing',\n",
       " 'arraign',\n",
       " 'array',\n",
       " 'arrest',\n",
       " 'arrgh',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrogance',\n",
       " 'arrow',\n",
       " 'arrows',\n",
       " 'art',\n",
       " 'artery',\n",
       " 'artful',\n",
       " 'artifice',\n",
       " 'artificial',\n",
       " 'artist',\n",
       " 'artistic',\n",
       " 'artpiece',\n",
       " 'artwork',\n",
       " 'ascend',\n",
       " 'ascends',\n",
       " 'ascent',\n",
       " 'ascraeus',\n",
       " 'ascription',\n",
       " 'ash',\n",
       " 'ashamed',\n",
       " 'ashen',\n",
       " 'ashore',\n",
       " 'ashplants',\n",
       " 'ashtray',\n",
       " 'ashur',\n",
       " 'asia',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'askew',\n",
       " 'aslant',\n",
       " 'asleep',\n",
       " 'asp',\n",
       " 'aspect',\n",
       " 'aspire',\n",
       " 'assail',\n",
       " 'assailant',\n",
       " 'assailing',\n",
       " 'assassinate',\n",
       " 'assault',\n",
       " 'assemble',\n",
       " 'assembled',\n",
       " 'assert',\n",
       " 'asses',\n",
       " 'asshole',\n",
       " 'assiduous',\n",
       " 'assimilate',\n",
       " 'assistant',\n",
       " 'associate',\n",
       " 'assume',\n",
       " 'assumes',\n",
       " 'assurance',\n",
       " 'assure',\n",
       " 'assuredly',\n",
       " 'asterisk',\n",
       " 'asteroid',\n",
       " 'astonish',\n",
       " 'astonishment',\n",
       " 'astound',\n",
       " 'astounded',\n",
       " 'astray',\n",
       " 'astrology',\n",
       " 'astronaut',\n",
       " 'astronomy',\n",
       " 'asway',\n",
       " 'atalic',\n",
       " 'ate',\n",
       " 'ated',\n",
       " 'atheist',\n",
       " 'athlete',\n",
       " 'athletic',\n",
       " 'atlantic',\n",
       " 'atlas',\n",
       " 'atmosphere',\n",
       " 'atmospheric',\n",
       " 'atom',\n",
       " 'atop',\n",
       " 'atrocity',\n",
       " 'atropos',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attacker',\n",
       " 'attempt',\n",
       " 'attend',\n",
       " 'attendance',\n",
       " 'attendant',\n",
       " 'attention',\n",
       " 'attentive',\n",
       " 'attic',\n",
       " 'attitude',\n",
       " 'attract',\n",
       " 'attractive',\n",
       " 'attrition',\n",
       " 'au',\n",
       " 'auburn',\n",
       " 'auction',\n",
       " 'audi',\n",
       " 'audible',\n",
       " 'audience',\n",
       " 'audition',\n",
       " 'aughty',\n",
       " 'august',\n",
       " 'augustus',\n",
       " 'aum',\n",
       " 'aunt',\n",
       " 'auntie',\n",
       " 'aura',\n",
       " 'aureate',\n",
       " 'aureole',\n",
       " 'auspicious',\n",
       " 'austere',\n",
       " 'australia',\n",
       " 'author',\n",
       " 'authority',\n",
       " 'autism',\n",
       " 'auto',\n",
       " 'automatically',\n",
       " 'automobile',\n",
       " 'autonomy',\n",
       " 'autopsy',\n",
       " 'autumn',\n",
       " 'avail',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'avernus',\n",
       " 'averse',\n",
       " 'avert',\n",
       " 'averted',\n",
       " 'avicii',\n",
       " 'avid',\n",
       " 'avila',\n",
       " 'avocado',\n",
       " 'avoid',\n",
       " 'avon',\n",
       " 'await',\n",
       " 'awaited',\n",
       " 'awake',\n",
       " 'awaken',\n",
       " 'awakeness',\n",
       " 'awakening',\n",
       " 'awakens',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'awaysay',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awkwardly',\n",
       " 'awooooo',\n",
       " 'awry',\n",
       " 'axe',\n",
       " 'ay',\n",
       " 'aye',\n",
       " 'azure',\n",
       " 'baaaaaaa',\n",
       " 'baaing',\n",
       " 'baba',\n",
       " 'babble',\n",
       " 'babbling',\n",
       " 'babe',\n",
       " 'babel',\n",
       " 'baby',\n",
       " 'babylon',\n",
       " 'babylonian',\n",
       " 'bacchanal',\n",
       " 'bacchus',\n",
       " 'bach',\n",
       " 'backache',\n",
       " 'backcloth',\n",
       " 'backed',\n",
       " 'background',\n",
       " 'backs',\n",
       " 'backstreet',\n",
       " 'backstroke',\n",
       " 'backward',\n",
       " 'backwards',\n",
       " 'backwash',\n",
       " 'backyard',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'bade',\n",
       " 'badger',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'baggage',\n",
       " 'bagpipe',\n",
       " 'bagpiper',\n",
       " 'bahamas',\n",
       " 'bail',\n",
       " 'bait',\n",
       " 'baize',\n",
       " 'bake',\n",
       " 'bakeboard',\n",
       " 'baker',\n",
       " 'bakery',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'balancing',\n",
       " 'balascio',\n",
       " 'bald',\n",
       " 'bale',\n",
       " 'balham',\n",
       " 'ball',\n",
       " 'ballad',\n",
       " 'ballerina',\n",
       " 'ballet',\n",
       " 'ballgame',\n",
       " 'balloon',\n",
       " 'ballooner',\n",
       " 'ballyshannon',\n",
       " 'balmyard',\n",
       " 'bamboo',\n",
       " 'ban',\n",
       " 'banana',\n",
       " 'band',\n",
       " 'bandage',\n",
       " 'bandstand',\n",
       " 'bane',\n",
       " 'bang',\n",
       " 'bangkok',\n",
       " 'banish',\n",
       " 'banister',\n",
       " 'bank',\n",
       " 'banked',\n",
       " 'bann',\n",
       " 'banner',\n",
       " 'banquet',\n",
       " 'banshee',\n",
       " 'banter',\n",
       " 'bar',\n",
       " 'barack',\n",
       " 'barb',\n",
       " 'barbarism',\n",
       " 'barbarous',\n",
       " 'barbecue',\n",
       " 'barbershop',\n",
       " 'barbitos',\n",
       " 'bard',\n",
       " 'bardic',\n",
       " 'bardot',\n",
       " 'bare',\n",
       " 'bared',\n",
       " 'barefoot',\n",
       " 'bareheaded',\n",
       " 'barely',\n",
       " 'barf',\n",
       " 'bark',\n",
       " 'barker',\n",
       " 'barley',\n",
       " 'barlow',\n",
       " 'barn',\n",
       " 'barnacle',\n",
       " 'barnyard',\n",
       " 'barometer',\n",
       " 'baron',\n",
       " 'barr',\n",
       " 'barrage',\n",
       " 'barrel',\n",
       " 'barreled',\n",
       " 'barren',\n",
       " 'barrier',\n",
       " 'barrow',\n",
       " 'barter',\n",
       " 'basalt',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'baseless',\n",
       " 'basement',\n",
       " 'bash',\n",
       " 'basher',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'basket',\n",
       " 'basketball',\n",
       " 'bass',\n",
       " 'bastard',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'bate',\n",
       " 'bath',\n",
       " 'bather',\n",
       " 'bathing',\n",
       " 'bathroom',\n",
       " 'bathysphere',\n",
       " 'batman',\n",
       " 'baton',\n",
       " 'batten',\n",
       " 'batter',\n",
       " 'battering',\n",
       " 'battery',\n",
       " 'battle',\n",
       " 'battlefield',\n",
       " 'battlement',\n",
       " 'baugh',\n",
       " 'bay',\n",
       " 'bazaar',\n",
       " 'bbqs',\n",
       " 'beach',\n",
       " 'beached',\n",
       " 'beacon',\n",
       " 'bead',\n",
       " 'beaded',\n",
       " 'beady',\n",
       " 'beagle',\n",
       " 'beak',\n",
       " 'beaker',\n",
       " 'beam',\n",
       " 'beaming',\n",
       " 'bean',\n",
       " 'bear',\n",
       " 'beard',\n",
       " 'bearded',\n",
       " 'bearing',\n",
       " 'beast',\n",
       " 'beastly',\n",
       " 'beasty',\n",
       " 'beat',\n",
       " 'beaten',\n",
       " 'beater',\n",
       " 'beatific',\n",
       " 'beatification',\n",
       " 'beating',\n",
       " 'beatles',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'becalm',\n",
       " 'becareful',\n",
       " 'beckett',\n",
       " 'beckon',\n",
       " 'bed',\n",
       " 'bedding',\n",
       " 'beddy',\n",
       " 'bede',\n",
       " 'bedfellow',\n",
       " 'bedl',\n",
       " 'bedlam',\n",
       " 'bedmate',\n",
       " 'bedraggle',\n",
       " 'bedreggled',\n",
       " 'bedroom',\n",
       " 'bedsheets',\n",
       " 'bedside',\n",
       " 'bedspread',\n",
       " 'bedtime',\n",
       " 'bee',\n",
       " 'beef',\n",
       " 'beefcake',\n",
       " 'beelzebub',\n",
       " 'beep',\n",
       " 'beer',\n",
       " 'beerhouse',\n",
       " 'beery',\n",
       " 'beethoven',\n",
       " 'beetle',\n",
       " 'befall',\n",
       " 'befor',\n",
       " 'befriend',\n",
       " 'beg',\n",
       " 'beggar',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'begining',\n",
       " 'beginn',\n",
       " 'beginning',\n",
       " 'begs',\n",
       " 'beguile',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behaviour',\n",
       " 'beheaded',\n",
       " 'behold',\n",
       " 'beholder',\n",
       " 'beiber',\n",
       " 'beige',\n",
       " 'belaud',\n",
       " 'belchin',\n",
       " 'beleafs',\n",
       " 'beleave',\n",
       " 'beleive',\n",
       " 'belief',\n",
       " 'believe',\n",
       " 'believing',\n",
       " 'bell',\n",
       " 'bellerophon',\n",
       " 'bellied',\n",
       " 'bellmetal',\n",
       " 'bellow',\n",
       " 'bellowing',\n",
       " 'belly',\n",
       " 'bellyache',\n",
       " 'belong',\n",
       " 'beloved',\n",
       " 'belovéd',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'benares',\n",
       " 'bench',\n",
       " 'benches',\n",
       " 'bend',\n",
       " 'bending',\n",
       " 'bendy',\n",
       " 'beneath',\n",
       " 'benediction',\n",
       " 'benefit',\n",
       " 'benevolence',\n",
       " 'benign',\n",
       " 'bennett',\n",
       " 'bent',\n",
       " 'bentless',\n",
       " 'bequeath',\n",
       " 'berate',\n",
       " 'bereft',\n",
       " 'berry',\n",
       " 'berth',\n",
       " 'bertold',\n",
       " 'beseech',\n",
       " 'beset',\n",
       " 'besmirch',\n",
       " 'besotted',\n",
       " 'bessie',\n",
       " 'best',\n",
       " 'bestfriend',\n",
       " 'bestow',\n",
       " 'bet',\n",
       " 'beth',\n",
       " 'bethany',\n",
       " 'bethel',\n",
       " 'bethlehem',\n",
       " 'betray',\n",
       " 'betrayal',\n",
       " 'better',\n",
       " 'bettie',\n",
       " 'betting',\n",
       " 'betty',\n",
       " 'betweenie',\n",
       " 'bevvy',\n",
       " 'beware',\n",
       " 'bewilder',\n",
       " 'bewildered',\n",
       " 'bewitch',\n",
       " 'bewitched',\n",
       " 'bex',\n",
       " 'bfas',\n",
       " 'bffs',\n",
       " 'bfg',\n",
       " 'bhasin',\n",
       " 'bias',\n",
       " 'bib',\n",
       " 'bicker',\n",
       " 'bicky',\n",
       " 'bid',\n",
       " 'bidding',\n",
       " 'bide',\n",
       " 'bier',\n",
       " 'bifocals',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggie',\n",
       " 'biggig',\n",
       " ...]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = vect.get_feature_names()\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rows represent corpora, columns represent feature words. Therefore, to find tf-idf of each corpus, we need to extract it from the rows one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12740,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_children = tfidf[0]\n",
    "row_children.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_indeces_child = np.argsort(row_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_indeces_child = list(sort_indeces_child)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fun',\n",
       " 'cat',\n",
       " 'night',\n",
       " 'want',\n",
       " 'sky',\n",
       " 'best',\n",
       " 'like',\n",
       " 'just',\n",
       " 'play',\n",
       " 'mum',\n",
       " 'time',\n",
       " 'look',\n",
       " 'come',\n",
       " 'don',\n",
       " 'say',\n",
       " 'know',\n",
       " 'make',\n",
       " 'day',\n",
       " 'friend',\n",
       " 'love']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_child = []\n",
    "for ind in top_indeces_child:\n",
    "    top_words_child.append(words[ind])\n",
    "top_words_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('children_tf_idf.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(str(top_words_child))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['try',\n",
       " 'tell',\n",
       " 'come',\n",
       " 'leave',\n",
       " 'look',\n",
       " 'thing',\n",
       " 'eye',\n",
       " 'way',\n",
       " 'time',\n",
       " 'think',\n",
       " 'life',\n",
       " 'heart',\n",
       " 'want',\n",
       " 'make',\n",
       " 'day',\n",
       " 'say',\n",
       " 'feel',\n",
       " 'just',\n",
       " 'love',\n",
       " 'know']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_lay = tfidf[1]\n",
    "row_lay.shape\n",
    "sort_indeces_lay = np.argsort(row_lay) \n",
    "\n",
    "top_indeces_lay = list(sort_indeces_lay)[-20:]\n",
    "  \n",
    "top_words_lay = []\n",
    "for ind in top_indeces_lay:\n",
    "    top_words_lay.append(words[ind])\n",
    "top_words_lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lay_tf_idf.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(str(top_words_lay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thing',\n",
       " 'life',\n",
       " 'face',\n",
       " 'think',\n",
       " 'leave',\n",
       " 'white',\n",
       " 'look',\n",
       " 'hand',\n",
       " 'day',\n",
       " 'light',\n",
       " 'man',\n",
       " 'time',\n",
       " 'old',\n",
       " 'eye',\n",
       " 'love',\n",
       " 'night',\n",
       " 'make',\n",
       " 'know',\n",
       " 'come',\n",
       " 'say']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_prof = tfidf[2]\n",
    "row_prof.shape\n",
    "sort_indeces_prof = np.argsort(row_prof) \n",
    "\n",
    "top_indeces_prof = list(sort_indeces_prof)[-20:]\n",
    "  \n",
    "top_words_prof = []\n",
    "for ind in top_indeces_prof:\n",
    "    top_words_prof.append(words[ind])\n",
    "top_words_prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prof_tf_idf.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(str(top_words_prof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split the strings into poems\n",
    "2. Vectorize them and find term frequency for each text collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "poems_children = [lemmas_child[i:i+n] for i in range(0, len(lemmas_child), n)]\n",
    "poems_lay = [lemmas_lay[i:i+n] for i in range(0, len(lemmas_lay), n)]\n",
    "poems_prof = [lemmas_prof[i:i+n] for i in range(0, len(lemmas_prof), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 5084)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_child = CountVectorizer(stop_words = 'english')\n",
    "tf_child = count_child.fit_transform(poems_children).toarray()\n",
    "tf_child.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 1051)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_lay = CountVectorizer(min_df=2, max_df=3, stop_words = 'english')\n",
    "tf_lay = count_lay.fit_transform(poems_lay).toarray()\n",
    "tf_lay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 1946)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_prof = CountVectorizer(min_df=2, max_df=3, stop_words = 'english')\n",
    "tf_prof = count_prof.fit_transform(poems_prof).toarray()\n",
    "tf_prof.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avvrik\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "come, make, blue, love, day, cat, friend\n",
      "topic 1\n",
      "ripper, fast, loud, eater, hater, runner, lover\n",
      "topic 2\n",
      "blue, night, time, day, love, sky, say\n",
      "topic 3\n",
      "just, night, christmas, say, love, day, box\n",
      "topic 4\n",
      "taste, sound, smell, feel, look, summer, beach\n",
      "topic 5\n",
      "don, say, know, make, day, friend, love\n",
      "topic 6\n",
      "say, want, day, come, know, friend, love\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "lda_child = LatentDirichletAllocation(n_components=7)\n",
    "lda_child.fit(tf_child)\n",
    "\n",
    "topic_words_child = lda_child.components_\n",
    "topic_words_child.shape\n",
    "count_words_child = count_child.get_feature_names()\n",
    "\n",
    "for topic_ind, topic in enumerate(topic_words_child):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_child = list(topic.argsort())[-7:]\n",
    "    lda_top_words_child = []\n",
    "    for ind in top_indeces_child:\n",
    "        lda_top_words_child.append(count_words_child[ind])\n",
    "    print(', '.join(lda_top_words_child))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avvrik\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "reflection, dollar, sex, muse, spell, misconstrue, pursue\n",
      "topic 1\n",
      "extra, distress, forevermore, machine, patter, fairytale, guitar\n",
      "topic 2\n",
      "existence, video, debut, pop, funny, misconstrue, spider\n",
      "topic 3\n",
      "poetess, horse, crystal, illusion, sneeze, robot, depression\n",
      "topic 4\n",
      "expression, discover, pie, feather, fuck, ive, mama\n",
      "topic 5\n",
      "design, thy, bother, affection, unrequited, feed, perfume\n",
      "topic 6\n",
      "painful, teacher, sigh, toast, risk, willow, fairy\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "lda_lay = LatentDirichletAllocation(n_components=7)\n",
    "lda_lay.fit(tf_lay)\n",
    "\n",
    "topic_words_lay = lda_lay.components_\n",
    "topic_words_lay.shape\n",
    "count_words_lay = count_lay.get_feature_names()\n",
    "\n",
    "for topic_ind, topic in enumerate(topic_words_lay):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_lay = list(topic.argsort())[-7:]\n",
    "    lda_top_words_lay = []\n",
    "    for ind in top_indeces_lay:\n",
    "        lda_top_words_lay.append(count_words_lay[ind])\n",
    "    print(', '.join(lda_top_words_lay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avvrik\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "limp, mountainside, perch, ain, ram, ma, valley, harlem\n",
      "topic 1\n",
      "strict, eh, halt, toddle, spiral, childhood, la, odor\n",
      "topic 2\n",
      "hardy, whitewash, byre, elm, lo, guitar, nibble, scum\n",
      "topic 3\n",
      "coldly, glist, tier, loneliness, aye, fiesole, warmth, ti\n",
      "topic 4\n",
      "upper, princess, depart, hippopotamus, castle, miner, radio, create\n",
      "topic 5\n",
      "apollo, decay, riding, ceiling, raft, nonsense, sabbath, frog\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "lda_prof = LatentDirichletAllocation(n_components=6)\n",
    "lda_prof.fit(tf_prof)\n",
    "\n",
    "topic_words_prof = lda_prof.components_\n",
    "topic_words_prof.shape\n",
    "count_words_prof = count_prof.get_feature_names()\n",
    "\n",
    "for topic_ind, topic in enumerate(topic_words_prof):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_prof = list(topic.argsort())[-8:]\n",
    "    lda_top_words_prof = []\n",
    "    for ind in top_indeces_prof:\n",
    "        lda_top_words_prof.append(count_words_prof[ind])\n",
    "    print(', '.join(lda_top_words_prof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling with NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "dog, fortnite, play, school, don, say, love\n",
      "topic 1\n",
      "taste, look, sound, feel, smell, summer, beach\n",
      "topic 2\n",
      "big, come, dog, sun, make, day, cat\n",
      "topic 3\n",
      "cat, play, look, love, best, dad, friend\n",
      "topic 4\n",
      "say, beautiful, day, blue, sky, love, wish\n",
      "topic 5\n",
      "think, good, like, play, make, want, know\n",
      "topic 6\n",
      "winter, sea, christmas, tree, night, day, box\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_child = NMF(n_components=7)\n",
    "nmf_child.fit(tf_child)\n",
    "nmf_topic_word_child = nmf_child.components_\n",
    "nmf_topic_word_child.shape\n",
    "for topic_ind, topic in enumerate(nmf_topic_word_child):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_child = list(topic.argsort())[-7:]\n",
    "    top_words_child = []\n",
    "    for ind in top_indeces_child:\n",
    "        top_words_child.append(count_words_child[ind])\n",
    "    print(', '.join(top_words_child))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "tan, scene, wreck, doesnt, hoping, horse, depression\n",
      "topic 1\n",
      "writer, amidst, spell, cat, expression, poetess, sex\n",
      "topic 2\n",
      "glory, wipe, bean, discover, feather, pie, mama\n",
      "topic 3\n",
      "road, rail, pouring, sore, patter, fairytale, guitar\n",
      "topic 4\n",
      "snowflake, sparkle, beam, spider, flake, crystal, sneeze\n",
      "topic 5\n",
      "case, untitled, grim, raven, remorse, careful, robot\n",
      "topic 6\n",
      "effort, bridge, masterpiece, allow, ancient, arrive, dime\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_lay = NMF(n_components=7)\n",
    "nmf_lay.fit(tf_lay)\n",
    "nmf_topic_word_lay = nmf_lay.components_\n",
    "nmf_topic_word_lay.shape\n",
    "for topic_ind, topic in enumerate(nmf_topic_word_lay):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_lay = list(topic.argsort())[-7:]\n",
    "    top_words_lay = []\n",
    "    for ind in top_indeces_lay:\n",
    "        top_words_lay.append(count_words_lay[ind])\n",
    "    print(', '.join(top_words_lay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "unsaid, frog, whitewash, intent, ram, create, perch\n",
      "topic 1\n",
      "glist, enamel, dragon, aye, fiesole, warmth, ti\n",
      "topic 2\n",
      "nude, thread, handful, eh, childhood, la, odor\n",
      "topic 3\n",
      "sphere, dante, telescope, ye, guitar, lo, scum\n",
      "topic 4\n",
      "gigantic, thistle, vale, upward, daisy, mountainside, valley\n",
      "topic 5\n",
      "elm, insurance, telescope, harlem, nonsense, sabbath, nibble\n",
      "topic 6\n",
      "sly, stern, toddle, halt, princess, miner, spiral\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf_prof = NMF(n_components=7)\n",
    "nmf_prof.fit(tf_prof)\n",
    "nmf_topic_word_prof = nmf_prof.components_\n",
    "nmf_topic_word_prof.shape\n",
    "for topic_ind, topic in enumerate(nmf_topic_word_prof):\n",
    "    print('topic', topic_ind)\n",
    "    top_indeces_prof = list(topic.argsort())[-7:]\n",
    "    top_words_prof = []\n",
    "    for ind in top_indeces_prof:\n",
    "        top_words_prof.append(count_words_prof[ind])\n",
    "    print(', '.join(top_words_prof))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
